[{"content":"五月份进行了两次面试，面试不同于闭门造车问的许多问题需要进行规范的回答类似于八股文之类的，虽然我很不喜欢八股文代表的那种死板的精神，但不得不承认在对于基础知识的掌握有许多不足，在面试过程中很多概念熟悉了解但是语言组织不够流畅，对面试中没能回答上来或者回答不流畅或者以后面试可能会遇到的额问题进行整理。\nk8s架构和基本组件及其功能 Kubernetes（k8s）是一个开源的容器编排平台，用于自动化部署、扩展和管理容器化应用。其架构采用主从（Master-Node）模型，核心组件分为控制平面（Control Plane）和工作节点（Worker Node）两部分。以下是详细架构和组件功能说明：\n一、控制平面（Master Components） 控制平面负责集群的全局决策和调度，通常运行在独立的Master节点上。\nAPI Server（kube-apiserver）\n功能：集群的唯一入口，提供RESTful API，处理所有管理请求（如创建/删除Pod、部署应用等）。\n特点：无状态设计，可水平扩展；通过认证、授权、准入控制保证安全性。\nScheduler（kube-scheduler）\n功能：监听未调度的Pod，根据资源需求、节点负载、亲和性等策略，将Pod绑定到合适的工作节点。\n策略：支持自定义调度算法（如优先选择低负载节点）。\nController Manager（kube-controller-manager）\n功能：运行一系列控制器，确保集群状态与预期一致。核心控制器包括：\nNode Controller：监控节点状态（如宕机时重新调度Pod）。\nDeployment Controller：管理副本数，实现滚动更新/回滚。\nService Controller：维护Service与Pod的映射关系。\netcd\n功能：分布式键值存储数据库，保存集群所有配置数据和状态（如Pod、Service、Namespace等）。\n特点：高可用性要求高，通常需部署奇数个节点（如3、5个）。\n二、工作节点（Node Components） 每个工作节点负责运行容器化应用，包含以下组件：\nkubelet\n功能：节点上的“代理”，与API Server通信，管理本节点Pod的生命周期（创建/删除容器、监控资源等）。\n关键职责：确保Pod中容器健康（通过探针检查），上报节点状态。\nkube-proxy\n功能：维护节点上的网络规则（如iptables/IPVS），实现Service的负载均衡和流量转发。\n示例：将访问Service VIP的请求转发到后端Pod。\n容器运行时（Container Runtime）\n功能：负责运行容器（如Docker、containerd、CRI-O），支持OCI标准。\n三、附加组件（Addons） 这些组件非必需，但为集群提供增强功能：\nCNI插件（如Calico、Flannel）\n功能：实现Pod间网络通信和跨节点网络策略。\nCoreDNS\n功能：为集群提供DNS解析服务（如Service名称到IP的映射）。\nIngress Controller（如Nginx Ingress）\n功能：管理外部访问集群的HTTP/HTTPS路由规则。\nMetrics Server\n功能：收集资源使用指标（CPU/内存），供HPA（自动扩缩容）使用。\nDashboard\n功能：提供Web UI界面管理集群。\njekins如何实现扩容 jenkins 的扩容主要通过 水平扩展（Horizontal Scaling） 和 垂直扩展（Vertical Scaling） 两种方式实现，具体取决于你的 Jenkins 架构（单机版或分布式）。以下是详细方案：\n一、单机 Jenkins 扩容 适用于单节点 Jenkins，提升单个 Master 节点的处理能力。\n垂直扩展（Vertical Scaling） 方式：增加 Jenkins Master 节点的 CPU、内存、磁盘 等资源。 适用场景：任务量较小，但单个任务资源需求高（如大型构建任务）。\n实现方法：\n物理机：升级硬件配置。\n虚拟机/云主机：调整实例规格（如 AWS EC2 从 t2.medium 升级到 t2.large）。\n容器化部署：调整 Docker/Kubernetes 资源限制（resources.requests/limits）。\n优化 Jenkins 配置 调整 JVM 参数：修改 JENKINS_JAVA_OPTS，增加堆内存（如 -Xmx4g）。 bash 在 Jenkins 启动脚本或 systemd 配置中设置 JAVA_OPTS=\u0026quot;-Xmx4g -Xms2g\u0026quot; 清理旧数据：定期清理构建历史、日志、无用插件，减少磁盘占用。\n二、分布式 Jenkins 扩容（水平扩展） 适用于高并发构建场景，通过 Master + Agent 架构 分散负载。\n添加 Jenkins Agent（工作节点） 原理：Master 负责调度任务，Agent 执行具体构建任务。 实现方式：\n静态 Agent：手动或通过脚本添加固定节点（物理机/虚拟机）。\n动态 Agent（弹性伸缩）：\nKubernetes Plugin：在 K8s 集群中自动创建/销毁 Pod 作为 Agent。\nDocker Plugin：按需启动 Docker 容器作为 Agent。\n云平台插件（AWS EC2、Azure VM）：根据负载自动扩缩容云实例。\nKubernetes 动态扩缩容（推荐方案） 步骤： 安装 Kubernetes Plugin： Jenkins → 插件管理 → 安装 Kubernetes Plugin。\n配置 Kubernetes Cloud： Jenkins → 系统管理 → 节点管理 → 配置 Kubernetes 集群信息（API 地址、Namespace、认证等）。\n定义 Pod 模板： 指定 Agent 的容器镜像、资源限制、卷挂载等（示例配置）：\n1 2 3 4 5 6 7 containers: - name: jnlp image: jenkins/inbound-agent:latest resources: limits: cpu: \u0026#34;1\u0026#34; memory: \u0026#34;2Gi\u0026#34; 设置自动伸缩规则：\n通过 Jenkins Pipeline 或任务配置指定 label（如 kubernetes）。\nKubernetes 根据任务队列长度自动创建/删除 Agent Pod。\n效果：\n无任务时：Agent Pod 数为 0，节省资源。\n高并发时：自动创建多个 Agent Pod 并行执行任务。\n三、高可用（HA）方案 若需进一步提升可用性，可结合以下策略：\nMaster 节点高可用：\n使用 Jenkins 集群（多个 Master + 共享存储，如 NFS 或 S3）。\n容器化部署时，通过 Kubernetes StatefulSet + PVC 持久化数据。\n负载均衡：\n通过 Nginx/HAProxy 代理多个 Jenkins Master。\n备份恢复：\n定期备份 JENKINS_HOME 目录（含配置和任务数据）。\n蓝绿发布，金丝雀发布，灰度发布 一、金丝雀发布（Canary Release） 核心原理 名称来源：类比矿工用金丝雀检测矿井瓦斯，先让小部分用户试用新版本，观察无问题后再逐步扩大范围。 实现方式：\n将新版本（Canary）与旧版本同时在线，按比例（如 5%→20%→100%）将流量逐步切换到新版本。\n通过监控（错误率、延迟等）判断新版本稳定性，发现问题立即回滚。\n典型场景 需要验证新功能在真实环境的表现。 避免全量发布导致的全局故障。\n实现工具 Kubernetes：通过 Ingress（如 Nginx Ingress 的 canary 注解）或 Service Mesh（如 Istio 的流量拆分）。 示例（Istio 配置）：\nyaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: my-app spec: hosts: - my-app.example.com http: - route: - destination: host: my-app subset: v1 # 旧版本 weight: 90 # 90%流量 - destination: host: my-app subset: v2 # 新版本（金丝雀） weight: 10 # 10%流量\n二、蓝绿发布（Blue-Green Deployment） 核心原理 颜色标识： Blue：当前生产环境（旧版本）。\nGreen：新版本环境（完全独立部署）。\n切换方式：在测试通过后，一次性将全部流量从 Blue 切换到 Green（通过负载均衡器或 DNS 切换）。\n典型场景 需要零停机时间的发布。 快速回滚（直接切回 Blue 环境）。\n优缺点 优点：发布和回滚极快，避免版本共存问题。 缺点：需双倍资源成本，数据库兼容性需提前处理。\n实现工具 Kubernetes：通过创建两套完整的 Deployment + Service，切换 selector 或使用 Ingress 路由。 云平台：AWS Elastic Beanstalk、Azure Deployment Slots。\n三、灰度发布（Gray Release） 核心原理 广义概念：涵盖所有渐进式发布策略（包括金丝雀发布）。 狭义实现：按用户特征（如地理位置、用户ID、设备类型）逐步开放新版本，而非单纯按流量比例。\n典型场景 定向测试：仅对内部员工或特定用户群开放新功能。 A/B 测试：对比不同版本的用户行为数据。\n实现工具 前端：通过 Feature Flag（如 LaunchDarkly）控制功能可见性。 后端：Nginx/Apache 规则、Service Mesh（如 Istio 的基于 Header 的路由）。\n","date":"2025-06-07T14:51:28+08:00","permalink":"https://Logic0528.github.io/p/%E9%9D%A2%E8%AF%95/","title":"面试"},{"content":"项目目录结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 k8s-deploy-platform/ ├── helm-charts/ # Helm Chart模板 │ └── app-template/ │ ├── Chart.yaml # Chart基本信息 │ ├── values.yaml # 默认配置值 │ └── templates/ # K8s资源模板 │ ├── deployment.yaml # 部署配置 │ ├── service.yaml # 服务配置 │ └── ingress.yaml # 入口配置 │ ├── manifests/ # K8s资源清单 │ ├── applications/ # 应用相关配置 │ │ └── app-template.yaml │ ├── logging/ # 日志系统配置 │ │ └── efk-values.yaml │ └── monitoring/ # 监控系统配置 │ ├── prometheus-rules.yaml # 监控规则 │ ├── prometheus-values.yaml # Prometheus配置 │ ├── pv.yaml # 持久化卷 │ └── storageclass.yaml # 存储类 │ └── scripts/ # 自动化脚本 ├── install.sh # 安装脚本 └── deploy.sh # 部署脚本 helm-charts/app-template Chart.yaml 1 2 3 4 5 apiVersion: v2 name: app-template description: 一个基础的应用部署模板 version: 0.1.0 type: application values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # 应用配置 name: myapp replicas: 2 image: nginx:latest # 资源配置 resources: requests: memory: \u0026#34;64Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; # 健康检查配置 healthCheck: enabled: true path: /health port: 8080 initialDelay: 30 period: 10 # 服务配置 service: type: ClusterIP port: 80 targetPort: 8080 # Ingress配置 ingress: enabled: false className: nginx host: myapp.example.com path: / pathType: Prefix value.yaml中service类型为ClusterIP,clusterip仅在集群内部可以访问，与之对应的是nodeport，nodeport在所有节点上开放指定端口（30000-32767）\nclusterIP让外部访问的方式：\n可以通过配置ingress\n1 2 3 4 5 graph LR A[外部用户] --\u0026gt; B[Ingress] B --\u0026gt; C[ClusterIP Service] C --\u0026gt; D[Pod1] C --\u0026gt; E[Pod2] 也可以使用这条命令临时端口转发\n1 kubectl port-forward -n monitoring svc/monitoring-grafana 3000:80 1 2 3 4 graph LR A[外部用户] --\u0026gt; B[临时端口转发] B --\u0026gt; C[ClusterIP Service] C --\u0026gt; D[Pod1] templates/deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion: apps/v1 kind: Deployment metadata: name: {{ .Values.name }} spec: replicas: {{ .Values.replicas }} selector: matchLabels: app: {{ .Values.name }} template: metadata: labels: app: {{ .Values.name }} spec: containers: - name: {{ .Values.name }} image: {{ .Values.image }} resources: {{ toYaml .Values.resources | nindent 12 }} {{- if .Values.healthCheck.enabled }} livenessProbe: httpGet: path: {{ .Values.healthCheck.path }} port: {{ .Values.healthCheck.port }} initialDelaySeconds: {{ .Values.healthCheck.initialDelay }} periodSeconds: {{ .Values.healthCheck.period }} {{- end }} deployment.yaml这样配置可以根据values.yaml中的配置动态生成deployment资源，相当于values.yaml定义变量，方便修改\ntemplates/service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: Service metadata: name: {{ .Values.name }} spec: type: {{ .Values.service.type }} ports: - port: {{ .Values.service.port }} targetPort: {{ .Values.service.targetPort }} protocol: TCP selector: app: {{ .Values.name }} templates/ingress.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 {{- if .Values.ingress.enabled }} apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: {{ .Values.name }} spec: ingressClassName: {{ .Values.ingress.className }} rules: - host: {{ .Values.ingress.host }} http: paths: - path: {{ .Values.ingress.path }} pathType: {{ .Values.ingress.pathType }} backend: service: name: {{ .Values.name }} port: number: {{ .Values.service.port }} {{- end }} manifests applications/app-template.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion: apps/v1 kind: Deployment metadata: name: {{ .Values.name }} spec: replicas: {{ .Values.replicas }} selector: matchLabels: app: {{ .Values.name }} template: metadata: labels: app: {{ .Values.name }} spec: containers: - name: {{ .Values.name }} image: {{ .Values.image }} resources: {{ toYaml .Values.resources | nindent 12 }} {{- if .Values.healthCheck.enabled }} livenessProbe: httpGet: path: {{ .Values.healthCheck.path }} port: {{ .Values.healthCheck.port }} initialDelaySeconds: {{ .Values.healthCheck.initialDelay }} periodSeconds: {{ .Values.healthCheck.period }} {{- end }} logging/efk-values.yaml 暂时还不会\nmonitoring/prometheus-rules.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: app-alerts namespace: monitoring spec: groups: - name: app rules: - alert: HighCPUUsage expr: container_cpu_usage_seconds_total \u0026gt; 80 for: 5m labels: severity: warning annotations: description: \u0026#34;容器 CPU 使用率超过 80%\u0026#34; - alert: HighMemoryUsage expr: container_memory_usage_bytes \u0026gt; 80 for: 5m labels: severity: warning annotations: description: \u0026#34;容器内存使用率超过 80%\u0026#34; monitoring/prometheus-values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 prometheus: enabled: true serverFiles: prometheus.yml: global: scrape_interval: 15s scrape_configs: - job_name: \u0026#39;kubernetes-pods\u0026#39; kubernetes_sd_configs: - role: pod - job_name: \u0026#39;kubernetes-nodes\u0026#39; kubernetes_sd_configs: - role: node persistence: enabled: true size: 10Gi grafana: enabled: true adminPassword: admin persistence: enabled: true size: 5Gi type: pvc storageClassName: local-storage grafana实现持久化存储时，需要配置storageClassName，这个storageClassName需要在k8s集群中已经存在，这里使用的是local-storage\nmonitoring/storageclass.yaml 1 2 3 4 5 6 7 class.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: local-storage provisioner: kubernetes.io/no-provisioner volumeBindingMode: Immediate monitoring/pv.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: v1 kind: PersistentVolume metadata: name: grafana-pv spec: capacity: storage: 5Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: local-storage local: path: /mnt/data nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: #- master # 替换为你的节点名称 - work-node1 注意：\n这里的path需要创建好，否则会报错 nodeAffinity是为了保证grafana的数据不会被其他节点的pod所使用，就一个工作节点所以没写，master节点上一般会有污点，最好调度到工作节点上 scripts install.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # filepath: scripts/setup-cluster.sh #!/bin/bash # 启动本地集群 start_cluster() { echo \u0026#34;Starting minikube cluster...\u0026#34; minikube start --driver=docker \\ --memory=4096 \\ --cpus=2 \\ --kubernetes-version=v1.24.0 } # 安装必要的组件 install_components() { # 添加 Helm 仓库 helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo add elastic https://helm.elastic.co helm repo update } # 创建必要的命名空间 create_namespaces() { kubectl create namespace monitoring kubectl create namespace logging kubectl create namespace applications } main() { start_cluster install_components create_namespaces } main \u0026#34;$@\u0026#34; deploy.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 #!/bin/bash # 设置变量 APP_NAME=$1 IMAGE=$2 NAMESPACE=${3:-applications} REPLICAS=${4:-1} # 验证输入 if [ -z \u0026#34;$APP_NAME\u0026#34; ] || [ -z \u0026#34;$IMAGE\u0026#34; ]; then echo \u0026#34;Usage: $0 \u0026lt;app-name\u0026gt; \u0026lt;image\u0026gt; [namespace] [replicas]\u0026#34; exit 1 fi # 部署应用 deploy_application() { echo \u0026#34;Deploying $APP_NAME...\u0026#34; helm upgrade --install $APP_NAME ./helm-charts/app-template \\ --namespace $NAMESPACE \\ --set name=$APP_NAME \\ --set image=$IMAGE \\ --set replicas=$REPLICAS } main() { deploy_application } main \u0026#34;$@\u0026#34; 项目暂时完成，grafana全是英文暂时不熟练，后续再完善\n","date":"2025-05-20T10:38:28+08:00","permalink":"https://Logic0528.github.io/p/%E5%9F%BA%E4%BA%8Ek8s%E7%9A%84%E5%BA%94%E7%94%A8%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2%E5%B9%B3%E5%8F%B0/","title":"基于K8s的应用自动化部署平台"},{"content":"污点（Taint） effect: NoSchedule：不允许调度，已经存在的Pod不受影响\nPreferNoSchedule：尽量不调度，已经存在的Pod不受影响\nNoExecute：不允许调度，并且驱逐已经存在的Pod\n容忍（Toleration） operator: Equal：需要key和value完全匹配\nExists：只需要key匹配，不需要value匹配\nvalue: 污点的值\n示例代码：\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: Pod metadata: name: pod-with-tolerations spec: tolerations: - key: \u0026#34;key1\u0026#34; # 污点的键 operator: \u0026#34;Equal\u0026#34; # 操作符：Equal或Exists value: \u0026#34;value1\u0026#34; # 污点的值 effect: \u0026#34;NoSchedule\u0026#34; # 效果 tolerationSeconds: 3600 # 容忍时间（仅用于NoExecute） 亲和力（Affinity） NodeAffinity 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: v1 kind: Pod metadata: name: with-node-affinity spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: # 硬性要求 nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux preferredDuringSchedulingIgnoredDuringExecution: # 软性偏好 - weight: 1 preference: matchExpressions: - key: disk-type operator: In values: - ssd PodAffinity 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: Pod metadata: name: with-pod-affinity spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - web-store topologyKey: kubernetes.io/hostname PodAntiAffinity(节点反亲和力) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: v1 kind: Pod metadata: name: with-pod-antiaffinity spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - cache topologyKey: kubernetes.io/hostname 没什么好说的，类似于标签，慢工出细活~\n","date":"2025-05-15T15:52:46+08:00","permalink":"https://Logic0528.github.io/p/k8s-%E6%B1%A1%E7%82%B9%E4%B8%8E%E5%AE%B9%E5%BF%8D/","title":"K8s-污点与容忍"},{"content":"PV和PVC PV和PVC是Kubernetes中用于管理持久化存储的两个重要概念。\nPV（Persistent Volume） 是集群中的存储资源 由管理员事先创建和配置 独立与pod的生命周期 可以对接各种存储后端，如nfs、ceph、云存储等 定义了存储的容量、访问模式、存储类等信息 PVC（Persistent Volume Claim） 是pod对存储资源的请求 由用户/开发者创建 类似于pod消耗Node资源，PVC消耗PV资源 声明需要的存储容量、访问模式、存储类等信息 不需要关心存储的具体实现细节 工作流程 管理员创建PV，定义存储资源的属性 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: v1 kind: PersistentVolume #描述资源类型为PV类型 metadata: name: pv0001 spec: capacity: #容量配置 storage: 5Gi volumeMode: Filesystem #存储类型为文件系统 accessModes: #访问模式：ReadWriteOnce WeadWriteMany ReadOnlyMany - ReadWriteMany #可被单节点读写 persistentVolumeReclaimPolicy: Retain #回收策略 storageClassName: slow #创建PV的存储类名，需要与pvc的相同 mountOptions: #加载配置 - hard - nfsvers=4.1 nfs: #连接到nfs path: /home/nfs/rw/pv-test #存储路径 server: 192.168.189.130 #nfs服务地址 用户/开发者创建PVC，声明需要的存储资源 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: PersistentVolumeClaim #资源类型为pvc metadata: name: nfs-pvc spec: accessModes: - ReadWriteMany volumeMode: Filesystem resources: requests: storage: 5Gi storageClassName: slow #名字要与对应的pv相同 # selector: #使用选择器选择对应的pv Kubernetes调度器根据PVC的要求，选择合适的PV进行绑定 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: v1 kind: Pod metadata: name: pvc-test-pod spec: containers: - image: nginx name: nginx-volume volumeMounts: - mountPath: /usr/share/nginx/html #挂载到容器的哪个目录 name: test-volume #挂载哪个volume volumes: - name: test-volume persistentVolumeClaim: claimName: nfs-pvc tolerations: - key: \u0026#34;node-role.kubernetes.io/control-plane\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; 动态制备PV 被污点给整破防了，暂时先不写了。（后半句由ai补充），天哪你连我心里想的什么都知道快点拯救全人类吧，待我学完污点归来\n目前遗留问题，-n kube-system po nfs-server-provisioner ImagePullBackOff,kube-apiserver pending状态，selfLink怎么处理，其他的部分理解深入许多\n好的，了解到主节点上默认会有污点，目的是为了保护主节点资源，防止普通pod调\n动态制备pv 创建storageclass 定义存储类型 指定Provisioner 设置存储参数 1 2 3 4 5 6 7 8 9 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: managed-nfs-storage provisioner: fuseim.pri/ifs #外部制备器提供者，编写为提供者的名称 parameters: archiveOnDelete: \u0026#34;false\u0026#34; #是否存档 reclaimPolicy: Retain #回收策略 volumeBindingMode: Immediate #默认为Immediate，表示创建PVC立即进行绑定 用户创建pvc 指定存储大小 指定访问模式 引用storageclass 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 cat nfs-sc-statefulset.yaml --- apiVersion: v1 kind: Service metadata: name: nginx-sc labels: app: nginx-sc spec: type: NodePort ports: - name: web port: 80 protocol: TCP selector: app: nginx-sc --- apiVersion: apps/v1 kind: StatefulSet metadata: name: nginx-sc spec: replicas: 1 #serverName: \u0026#34;nginx-sc\u0026#34; selector: matchLabels: app: nginx-sc template: metadata: labels: app: nginx-sc spec: containers: - image: nginx name: nginx-sc imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /usr/share/nginx/html name: nginx-sc-test-pvc volumeClaimTemplates: - metadata: name: nginx-sc-test-pvc spec: storageClassName: managed-nfs-storage accessModes: - ReadWriteMany resources: requests: storage: 1Gi provisioner创建pv 检测到新建pvc 根据storageclass找到对应的provisioner provisoner创建pv 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 cat nfs-provisioner.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nfs-client-provisioner namespace: kube-system labels: app: nfs-client-provisioner spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:v4.0.0 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: 192.168.189.130 - name: NFS_PATH value: /home/data/nfs/rw volumes: - name: nfs-client-root nfs: server: 192.168.189.130 path: /home/data/nfs/rw 基本过程如上，上次出现污点问题是因为nfs服务器地址写为master节点导致pod部署到master节点，有污点\n另补充一点，provisioner需要特定权限才能创建和管理pv，需要用到rbac.yaml\n","date":"2025-05-11T14:49:19+08:00","permalink":"https://Logic0528.github.io/p/k8s-%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8-pv%E5%92%8Cpvc/","title":"K8s-持久化存储-PV和PVC"},{"content":"超级帅哥悄悄路过\n","date":"2025-05-08T23:05:44+08:00","permalink":"https://Logic0528.github.io/p/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/","title":"第一篇博客"}]