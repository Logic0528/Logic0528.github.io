[{"content":"“最后一次来你家店吃饭了，离开北京了”\n这个男人快步走进这家面馆，人还没到柜台时我听到了这句话，今天是我第一次来北京。\n他应该是这家店的常客了，老板停顿了一下\n“去哪？”\n“回老家，西安” 深呼吸，吐气\n几秒的沉默，吃面的我也放慢了速度，我意识到这或许是第一段难忘的记忆\n“……” “回去干啥？”\n“一个月三四千，服务员什么的……”\n“还行，在那边三四千比这边六千都舒服，吃点啥”\n“来份小炒肉盖饭吧”\n“再来瓶啤酒”\n每天都有人疲惫地离开，每天都有人迷茫地涌进来，一座城市像一个巨大的有机生物体，细胞们通过一条条血管般的轨道运输到各处，这个生物体里有最令人作呕的腐烂，也有最生机勃勃的希望\n今天坐出租车时在下雨，雨刷器规律的摆动着，有雨水时就刷刷两下，没有雨时就安静的躺在那里，我想如果此时雨刷器在玻璃上跳起拉丁舞，那些一切杂乱的动作组合排列在一起，或许就是生命的意义\n我在心中暗自对自己说，等我离开北京那一天，我的最后一餐要是这家店的小炒肉盖饭，或许可以再来瓶啤酒\n","date":"2025-06-19T22:16:51+08:00","permalink":"https://Logic0528.github.io/p/%E5%8C%97%E6%BC%82%E6%97%A5%E8%AE%B001/","title":"北漂日记01"},{"content":"五月份进行了两次面试，面试不同于闭门造车,问的许多问题需要进行规范的回答类似于八股文之类的，虽然我很不喜欢八股文代表的那种死板的精神，但不得不承认在对于基础知识的掌握有许多不足，在面试过程中很多概念熟悉了解但是语言组织不够流畅，对面试中没能回答上来或者回答不流畅或者以后面试可能会遇到的额问题进行整理。 屡战屡败，屡败屡战，面试中不会的问题记下来解决也是一种收获\n1.k8s架构和基本组件 k8s是一个开源的容器编排平台，用于自动化部署，扩展和管理容器化应用。其架构采用主从架构（master-node），核心组件分为控制平面control plane和工作节点worker node。\n控制平面组件：\nkube-apiserver:API网关和集群控制中心，处理所有管理请求如创建/删除pod，部署应用等 etcd: 分布式键值存储，保存集群配置,如pod,service,namespace\nkube-scheduler: 负责pod调度到合适的节点\nkube-controller-manager: 运行控制器进程，包括node controller,deployment controller,service controller\n节点组件：\nkubelet: 管理节点上的pod\nkube-proxy: 实现网络规则和负载均衡，维护节点上的网络配置\ncontainer runtime: 负责运行容器（如docker，containerd）\n附加组件： coreDNS：集群DNS服务，提供域名解析功能，将服务名解析为IP地址。 IngressContorller：负责外部流量访问，支持负载均衡和路由规则。HTTP路由 NetworkPluginL: calico，flannel等\n2.pod,deployment,service pod： 是k8s中最小的调度单位，封装一个或多个容器，共享网络和存储\ndeployment： 基于ReplicaSet的更高层抽象，声明式管理Pod的期望状态，如副本数，镜像版本，更新策略 作用： 确保pod副本数始终等于期望值，自动重启/创建失败的pod，支持滚动更新，回滚版本\nservice：抽象的网络层，为一组pod提供固定访问入口 ip+端口 作用： 内部服务发现：同意集群内其他pod可通过service域名访问 外部暴露： 通过nodeport等类型将服务暴露到集群外 负载均衡： 将流量分发到后端多个pod副本\ndeployment是管理pod的控制器，service是抽象网络层，三者通过标签匹配。\n3.nginx与负载均衡 nginx是一款高性能的web服务器，反向代理服务器，负载均衡器 web服务器： 处理静态资源的请求，直接返回文件内容，性能远超传统服务器\n反向代理服务器： 将客户端请求转发到后端多个应用服务器如tomcat，隐藏真实服务器地址\n负载均衡器： 分发客户端请求到多个后端服务器，避免单点过载\n负载均衡： 将网络流量均发到多个计算资源如服务器，容器，避免单一节点过载，提高系统的可用性，性能和容错能力\nnginx负载均衡方式： 轮询；默认策略，按顺序分发请求 weight: 根据权重分配 ip_hash： 基于客户端IP哈希 等\u0026hellip;.深入了解后补充\n负载均衡的优点\njekins如何实现扩容 jenkins 的扩容主要通过 水平扩展（Horizontal Scaling） 和 垂直扩展（Vertical Scaling） 两种方式实现，具体取决于你的 Jenkins 架构（单机版或分布式）。以下是详细方案：\n一、单机 Jenkins 扩容 适用于单节点 Jenkins，提升单个 Master 节点的处理能力。\n垂直扩展（Vertical Scaling） 方式：增加 Jenkins Master 节点的 CPU、内存、磁盘 等资源。 适用场景：任务量较小，但单个任务资源需求高（如大型构建任务）。 实现方法： 物理机：升级硬件配置。 虚拟机/云主机：调整实例规格（如 AWS EC2 从 t2.medium 升级到 t2.large）。 容器化部署：调整 Docker/Kubernetes 资源限制（resources.requests/limits）。\n优化 Jenkins 配置 调整 JVM 参数：修改 JENKINS_JAVA_OPTS，增加堆内存（如 -Xmx4g）。 bash 在 Jenkins 启动脚本或 systemd 配置中设置 JAVA_OPTS=\u0026quot;-Xmx4g -Xms2g\u0026quot; 清理旧数据：定期清理构建历史、日志、无用插件，减少磁盘占用。 二、分布式 Jenkins 扩容（水平扩展） 适用于高并发构建场景，通过 Master + Agent 架构 分散负载。\n添加 Jenkins Agent（工作节点） 原理：Master 负责调度任务，Agent 执行具体构建任务。 实现方式： 静态 Agent：手动或通过脚本添加固定节点（物理机/虚拟机）。 动态 Agent（弹性伸缩）： Kubernetes Plugin：在 K8s 集群中自动创建/销毁 Pod 作为 Agent。 Docker Plugin：按需启动 Docker 容器作为 Agent。 云平台插件（AWS EC2、Azure VM）：根据负载自动扩缩容云实例。\nKubernetes 动态扩缩容（推荐方案） 步骤： 安装 Kubernetes Plugin： Jenkins → 插件管理 → 安装 Kubernetes Plugin。 配置 Kubernetes Cloud： Jenkins → 系统管理 → 节点管理 → 配置 Kubernetes 集群信息（API 地址、Namespace、认证等）。 定义 Pod 模板： 指定 Agent 的容器镜像、资源限制、卷挂载等（示例配置）：\n1 2 3 4 5 6 7 containers: - name: jnlp image: jenkins/inbound-agent:latest resources: limits: cpu: \u0026#34;1\u0026#34; memory: \u0026#34;2Gi\u0026#34; 设置自动伸缩规则： 通过 Jenkins Pipeline 或任务配置指定 label（如 kubernetes）。 Kubernetes 根据任务队列长度自动创建/删除 Agent Pod。 效果： 无任务时：Agent Pod 数为 0，节省资源。 高并发时：自动创建多个 Agent Pod 并行执行任务。\n三、高可用（HA）方案 若需进一步提升可用性，可结合以下策略： Master 节点高可用： 使用 Jenkins 集群（多个 Master + 共享存储，如 NFS 或 S3）。 容器化部署时，通过 Kubernetes StatefulSet + PVC 持久化数据。 负载均衡： 通过 Nginx/HAProxy 代理多个 Jenkins Master。 备份恢复： 定期备份 JENKINS_HOME 目录（含配置和任务数据）。\n蓝绿发布，金丝雀发布，灰度发布 一、金丝雀发布（Canary Release） 核心原理 名称来源：类比矿工用金丝雀检测矿井瓦斯，先让小部分用户试用新版本，观察无问题后再逐步扩大范围。 实现方式： 将新版本（Canary）与旧版本同时在线，按比例（如 5%→20%→100%）将流量逐步切换到新版本。 通过监控（错误率、延迟等）判断新版本稳定性，发现问题立即回滚。\n典型场景 需要验证新功能在真实环境的表现。 避免全量发布导致的全局故障。\n实现工具 Kubernetes：通过 Ingress（如 Nginx Ingress 的 canary 注解）或 Service Mesh（如 Istio 的流量拆分）。 示例（Istio 配置）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: my-app spec: hosts: - my-app.example.com http: - route: - destination: host: my-app subset: v1 # 旧版本 weight: 90 # 90%流量 - destination: host: my-app subset: v2 # 新版本（金丝雀） weight: 10 # 10%流量 ","date":"2025-06-07T14:51:28+08:00","permalink":"https://Logic0528.github.io/p/%E9%9D%A2%E8%AF%95/","title":"面试"},{"content":"项目目录结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 k8s-deploy-platform/ ├── helm-charts/ # Helm Chart模板 │ └── app-template/ │ ├── Chart.yaml # Chart基本信息 │ ├── values.yaml # 默认配置值 │ └── templates/ # K8s资源模板 │ ├── deployment.yaml # 部署配置 │ ├── service.yaml # 服务配置 │ └── ingress.yaml # 入口配置 │ ├── manifests/ # K8s资源清单 │ ├── applications/ # 应用相关配置 │ │ └── app-template.yaml │ ├── logging/ # 日志系统配置 │ │ └── efk-values.yaml │ └── monitoring/ # 监控系统配置 │ ├── prometheus-rules.yaml # 监控规则 │ ├── prometheus-values.yaml # Prometheus配置 │ ├── pv.yaml # 持久化卷 │ └── storageclass.yaml # 存储类 │ └── scripts/ # 自动化脚本 ├── install.sh # 安装脚本 └── deploy.sh # 部署脚本 helm-charts/app-template Chart.yaml 1 2 3 4 5 apiVersion: v2 name: app-template description: 一个基础的应用部署模板 version: 0.1.0 type: application values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # 应用配置 name: myapp replicas: 2 image: nginx:latest # 资源配置 resources: requests: memory: \u0026#34;64Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; # 健康检查配置 healthCheck: enabled: true path: /health port: 8080 initialDelay: 30 period: 10 # 服务配置 service: type: ClusterIP port: 80 targetPort: 8080 # Ingress配置 ingress: enabled: false className: nginx host: myapp.example.com path: / pathType: Prefix value.yaml中service类型为ClusterIP,clusterip仅在集群内部可以访问，与之对应的是nodeport，nodeport在所有节点上开放指定端口（30000-32767）\nclusterIP让外部访问的方式：\n可以通过配置ingress\n1 2 3 4 5 graph LR A[外部用户] --\u0026gt; B[Ingress] B --\u0026gt; C[ClusterIP Service] C --\u0026gt; D[Pod1] C --\u0026gt; E[Pod2] 也可以使用这条命令临时端口转发\n1 kubectl port-forward -n monitoring svc/monitoring-grafana 3000:80 1 2 3 4 graph LR A[外部用户] --\u0026gt; B[临时端口转发] B --\u0026gt; C[ClusterIP Service] C --\u0026gt; D[Pod1] templates/deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion: apps/v1 kind: Deployment metadata: name: {{ .Values.name }} spec: replicas: {{ .Values.replicas }} selector: matchLabels: app: {{ .Values.name }} template: metadata: labels: app: {{ .Values.name }} spec: containers: - name: {{ .Values.name }} image: {{ .Values.image }} resources: {{ toYaml .Values.resources | nindent 12 }} {{- if .Values.healthCheck.enabled }} livenessProbe: httpGet: path: {{ .Values.healthCheck.path }} port: {{ .Values.healthCheck.port }} initialDelaySeconds: {{ .Values.healthCheck.initialDelay }} periodSeconds: {{ .Values.healthCheck.period }} {{- end }} deployment.yaml这样配置可以根据values.yaml中的配置动态生成deployment资源，相当于values.yaml定义变量，方便修改\ntemplates/service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: Service metadata: name: {{ .Values.name }} spec: type: {{ .Values.service.type }} ports: - port: {{ .Values.service.port }} targetPort: {{ .Values.service.targetPort }} protocol: TCP selector: app: {{ .Values.name }} templates/ingress.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 {{- if .Values.ingress.enabled }} apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: {{ .Values.name }} spec: ingressClassName: {{ .Values.ingress.className }} rules: - host: {{ .Values.ingress.host }} http: paths: - path: {{ .Values.ingress.path }} pathType: {{ .Values.ingress.pathType }} backend: service: name: {{ .Values.name }} port: number: {{ .Values.service.port }} {{- end }} manifests applications/app-template.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion: apps/v1 kind: Deployment metadata: name: {{ .Values.name }} spec: replicas: {{ .Values.replicas }} selector: matchLabels: app: {{ .Values.name }} template: metadata: labels: app: {{ .Values.name }} spec: containers: - name: {{ .Values.name }} image: {{ .Values.image }} resources: {{ toYaml .Values.resources | nindent 12 }} {{- if .Values.healthCheck.enabled }} livenessProbe: httpGet: path: {{ .Values.healthCheck.path }} port: {{ .Values.healthCheck.port }} initialDelaySeconds: {{ .Values.healthCheck.initialDelay }} periodSeconds: {{ .Values.healthCheck.period }} {{- end }} logging/efk-values.yaml 暂时还不会\nmonitoring/prometheus-rules.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: app-alerts namespace: monitoring spec: groups: - name: app rules: - alert: HighCPUUsage expr: container_cpu_usage_seconds_total \u0026gt; 80 for: 5m labels: severity: warning annotations: description: \u0026#34;容器 CPU 使用率超过 80%\u0026#34; - alert: HighMemoryUsage expr: container_memory_usage_bytes \u0026gt; 80 for: 5m labels: severity: warning annotations: description: \u0026#34;容器内存使用率超过 80%\u0026#34; monitoring/prometheus-values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 prometheus: enabled: true serverFiles: prometheus.yml: global: scrape_interval: 15s scrape_configs: - job_name: \u0026#39;kubernetes-pods\u0026#39; kubernetes_sd_configs: - role: pod - job_name: \u0026#39;kubernetes-nodes\u0026#39; kubernetes_sd_configs: - role: node persistence: enabled: true size: 10Gi grafana: enabled: true adminPassword: admin persistence: enabled: true size: 5Gi type: pvc storageClassName: local-storage grafana实现持久化存储时，需要配置storageClassName，这个storageClassName需要在k8s集群中已经存在，这里使用的是local-storage\nmonitoring/storageclass.yaml 1 2 3 4 5 6 7 class.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: local-storage provisioner: kubernetes.io/no-provisioner volumeBindingMode: Immediate monitoring/pv.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: v1 kind: PersistentVolume metadata: name: grafana-pv spec: capacity: storage: 5Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: local-storage local: path: /mnt/data nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: #- master # 替换为你的节点名称 - work-node1 注意：\n这里的path需要创建好，否则会报错 nodeAffinity是为了保证grafana的数据不会被其他节点的pod所使用，就一个工作节点所以没写，master节点上一般会有污点，最好调度到工作节点上 scripts install.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # filepath: scripts/setup-cluster.sh #!/bin/bash # 启动本地集群 start_cluster() { echo \u0026#34;Starting minikube cluster...\u0026#34; minikube start --driver=docker \\ --memory=4096 \\ --cpus=2 \\ --kubernetes-version=v1.24.0 } # 安装必要的组件 install_components() { # 添加 Helm 仓库 helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo add elastic https://helm.elastic.co helm repo update } # 创建必要的命名空间 create_namespaces() { kubectl create namespace monitoring kubectl create namespace logging kubectl create namespace applications } main() { start_cluster install_components create_namespaces } main \u0026#34;$@\u0026#34; deploy.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 #!/bin/bash # 设置变量 APP_NAME=$1 IMAGE=$2 NAMESPACE=${3:-applications} REPLICAS=${4:-1} # 验证输入 if [ -z \u0026#34;$APP_NAME\u0026#34; ] || [ -z \u0026#34;$IMAGE\u0026#34; ]; then echo \u0026#34;Usage: $0 \u0026lt;app-name\u0026gt; \u0026lt;image\u0026gt; [namespace] [replicas]\u0026#34; exit 1 fi # 部署应用 deploy_application() { echo \u0026#34;Deploying $APP_NAME...\u0026#34; helm upgrade --install $APP_NAME ./helm-charts/app-template \\ --namespace $NAMESPACE \\ --set name=$APP_NAME \\ --set image=$IMAGE \\ --set replicas=$REPLICAS } main() { deploy_application } main \u0026#34;$@\u0026#34; 项目暂时完成，grafana全是英文暂时不熟练，后续再完善\n","date":"2025-05-20T10:38:28+08:00","permalink":"https://Logic0528.github.io/p/%E5%9F%BA%E4%BA%8Ek8s%E7%9A%84%E5%BA%94%E7%94%A8%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2%E5%B9%B3%E5%8F%B0/","title":"基于K8s的应用自动化部署平台"},{"content":"污点（Taint） effect: NoSchedule：不允许调度，已经存在的Pod不受影响\nPreferNoSchedule：尽量不调度，已经存在的Pod不受影响\nNoExecute：不允许调度，并且驱逐已经存在的Pod\n容忍（Toleration） operator: Equal：需要key和value完全匹配\nExists：只需要key匹配，不需要value匹配\nvalue: 污点的值\n示例代码：\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: Pod metadata: name: pod-with-tolerations spec: tolerations: - key: \u0026#34;key1\u0026#34; # 污点的键 operator: \u0026#34;Equal\u0026#34; # 操作符：Equal或Exists value: \u0026#34;value1\u0026#34; # 污点的值 effect: \u0026#34;NoSchedule\u0026#34; # 效果 tolerationSeconds: 3600 # 容忍时间（仅用于NoExecute） 亲和力（Affinity） NodeAffinity 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: v1 kind: Pod metadata: name: with-node-affinity spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: # 硬性要求 nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux preferredDuringSchedulingIgnoredDuringExecution: # 软性偏好 - weight: 1 preference: matchExpressions: - key: disk-type operator: In values: - ssd PodAffinity 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: Pod metadata: name: with-pod-affinity spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - web-store topologyKey: kubernetes.io/hostname PodAntiAffinity(节点反亲和力) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: v1 kind: Pod metadata: name: with-pod-antiaffinity spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - cache topologyKey: kubernetes.io/hostname 没什么好说的，类似于标签，慢工出细活~\n","date":"2025-05-15T15:52:46+08:00","permalink":"https://Logic0528.github.io/p/k8s-%E6%B1%A1%E7%82%B9%E4%B8%8E%E5%AE%B9%E5%BF%8D/","title":"K8s-污点与容忍"},{"content":"PV和PVC PV和PVC是Kubernetes中用于管理持久化存储的两个重要概念。\nPV（Persistent Volume） 是集群中的存储资源 由管理员事先创建和配置 独立与pod的生命周期 可以对接各种存储后端，如nfs、ceph、云存储等 定义了存储的容量、访问模式、存储类等信息 PVC（Persistent Volume Claim） 是pod对存储资源的请求 由用户/开发者创建 类似于pod消耗Node资源，PVC消耗PV资源 声明需要的存储容量、访问模式、存储类等信息 不需要关心存储的具体实现细节 工作流程 管理员创建PV，定义存储资源的属性 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: v1 kind: PersistentVolume #描述资源类型为PV类型 metadata: name: pv0001 spec: capacity: #容量配置 storage: 5Gi volumeMode: Filesystem #存储类型为文件系统 accessModes: #访问模式：ReadWriteOnce WeadWriteMany ReadOnlyMany - ReadWriteMany #可被单节点读写 persistentVolumeReclaimPolicy: Retain #回收策略 storageClassName: slow #创建PV的存储类名，需要与pvc的相同 mountOptions: #加载配置 - hard - nfsvers=4.1 nfs: #连接到nfs path: /home/nfs/rw/pv-test #存储路径 server: 192.168.189.130 #nfs服务地址 用户/开发者创建PVC，声明需要的存储资源 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: PersistentVolumeClaim #资源类型为pvc metadata: name: nfs-pvc spec: accessModes: - ReadWriteMany volumeMode: Filesystem resources: requests: storage: 5Gi storageClassName: slow #名字要与对应的pv相同 # selector: #使用选择器选择对应的pv Kubernetes调度器根据PVC的要求，选择合适的PV进行绑定 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: v1 kind: Pod metadata: name: pvc-test-pod spec: containers: - image: nginx name: nginx-volume volumeMounts: - mountPath: /usr/share/nginx/html #挂载到容器的哪个目录 name: test-volume #挂载哪个volume volumes: - name: test-volume persistentVolumeClaim: claimName: nfs-pvc tolerations: - key: \u0026#34;node-role.kubernetes.io/control-plane\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; 动态制备PV 被污点给整破防了，暂时先不写了。（后半句由ai补充），天哪你连我心里想的什么都知道快点拯救全人类吧，待我学完污点归来\n目前遗留问题，-n kube-system po nfs-server-provisioner ImagePullBackOff,kube-apiserver pending状态，selfLink怎么处理，其他的部分理解深入许多\n好的，了解到主节点上默认会有污点，目的是为了保护主节点资源，防止普通pod调\n动态制备pv 创建storageclass 定义存储类型 指定Provisioner 设置存储参数 1 2 3 4 5 6 7 8 9 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: managed-nfs-storage provisioner: fuseim.pri/ifs #外部制备器提供者，编写为提供者的名称 parameters: archiveOnDelete: \u0026#34;false\u0026#34; #是否存档 reclaimPolicy: Retain #回收策略 volumeBindingMode: Immediate #默认为Immediate，表示创建PVC立即进行绑定 用户创建pvc 指定存储大小 指定访问模式 引用storageclass 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 cat nfs-sc-statefulset.yaml --- apiVersion: v1 kind: Service metadata: name: nginx-sc labels: app: nginx-sc spec: type: NodePort ports: - name: web port: 80 protocol: TCP selector: app: nginx-sc --- apiVersion: apps/v1 kind: StatefulSet metadata: name: nginx-sc spec: replicas: 1 #serverName: \u0026#34;nginx-sc\u0026#34; selector: matchLabels: app: nginx-sc template: metadata: labels: app: nginx-sc spec: containers: - image: nginx name: nginx-sc imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /usr/share/nginx/html name: nginx-sc-test-pvc volumeClaimTemplates: - metadata: name: nginx-sc-test-pvc spec: storageClassName: managed-nfs-storage accessModes: - ReadWriteMany resources: requests: storage: 1Gi provisioner创建pv 检测到新建pvc 根据storageclass找到对应的provisioner provisoner创建pv 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 cat nfs-provisioner.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nfs-client-provisioner namespace: kube-system labels: app: nfs-client-provisioner spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:v4.0.0 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: 192.168.189.130 - name: NFS_PATH value: /home/data/nfs/rw volumes: - name: nfs-client-root nfs: server: 192.168.189.130 path: /home/data/nfs/rw 基本过程如上，上次出现污点问题是因为nfs服务器地址写为master节点导致pod部署到master节点，有污点\n另补充一点，provisioner需要特定权限才能创建和管理pv，需要用到rbac.yaml\n","date":"2025-05-11T14:49:19+08:00","permalink":"https://Logic0528.github.io/p/k8s-%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8-pv%E5%92%8Cpvc/","title":"K8s-持久化存储-PV和PVC"},{"content":"超级帅哥悄悄路过\n","date":"2025-05-08T23:05:44+08:00","permalink":"https://Logic0528.github.io/p/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/","title":"第一篇博客"}]