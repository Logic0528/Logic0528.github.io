[{"content":"自己动手，丰衣足食，经过多次的搭建k8s集群现在可以说是基本明白了，记录一下过程不用每次都再搜索别人的教程了\n准备工作 1 2 3 4 5 6 7 8 9 10 11 12 #关闭防火墙 systemctl stop firewalld systemctl disable firewalld #关闭swap分区 swapoff -a 永久关闭需修改/etc/fstab sudo vim /etc/fstab #关闭selinux setenforce 0 sed -i \u0026#39;s/SELINUX=enforcing/SELINUX=disabled/g\u0026#39; /etc/selinux/config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # 添加配置文件并加载ipvs模块（kube-proxy使用ipvs模式） cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/ipvs.conf ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh nf_conntrack EOF # 执行命令加载模块 systemctl restart systemd-modules-load.service modprobe -- ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh nf_conntrack # 添加内核参数配置文件，参数在重新启动后保持不变 cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 EOF # 执行命令使参数生效 sysctl --system 安装前检查 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 执行命令检查selinux状态，输出应为SELinux status：disabled sestatus # 检查swap是否已经关闭，输出应为空 cat /proc/swaps # 检查ipvs模块是否已经加载 lsmod |grep ip_vs # 检查防火墙是否已经关闭 systemctl status firewalld # 检查内核参数是否已经设置，值应该为 1 cat /proc/sys/net/ipv4/ip_forward 安装docker,容器运行时 Ubuntu系统：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # step 1: 安装必要的一些系统工具 sudo apt-get update sudo apt-get install ca-certificates curl gnupg # step 2: 信任 Docker 的 GPG 公钥 sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg # Step 3: 写入软件源信息 echo \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://mirrors.aliyun.com/docker-ce/linux/ubuntu \\ \u0026#34;$(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;)\u0026#34; stable\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null # Step 4: 安装Docker sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin # 安装指定版本的Docker-CE: # Step 1: 查找Docker-CE的版本: # apt-cache madison docker-ce # docker-ce | 17.03.1~ce-0~ubuntu-xenial | https://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages # docker-ce | 17.03.0~ce-0~ubuntu-xenial | https://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages # Step 2: 安装指定版本的Docker-CE: (VERSION例如上面的17.03.1~ce-0~ubuntu-xenial) # sudo apt-get -y install docker-ce=[VERSION] centos系统：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # step 1: 安装必要的一些系统工具 sudo yum install -y yum-utils # Step 2: 添加软件源信息 yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # Step 3: 安装Docker sudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin # Step 4: 开启Docker服务 sudo service docker start # 注意： # 官方软件源默认启用了最新的软件，您可以通过编辑软件源的方式获取各个版本的软件包。例如官方并没有将测试版本的软件源置为可用，您可以通过以下方式开启。同理可以开启各种测试版本等。 # vim /etc/yum.repos.d/docker-ce.repo # 将[docker-ce-test]下方的enabled=0修改为enabled=1 # # 安装指定版本的Docker-CE: # Step 1: 查找Docker-CE的版本: # yum list docker-ce.x86_64 --showduplicates | sort -r # Loading mirror speeds from cached hostfile # Loaded plugins: branch, fastestmirror, langpacks # docker-ce.x86_64 17.03.1.ce-1.el7.centos docker-ce-stable # docker-ce.x86_64 17.03.1.ce-1.el7.centos @docker-ce-stable # docker-ce.x86_64 17.03.0.ce-1.el7.centos docker-ce-stable # Available Packages # Step2: 安装指定版本的Docker-CE: (VERSION例如上面的17.03.0.ce.1-1.el7.centos) # sudo yum -y install docker-ce-[VERSION] 接下来这一点是之前几次失败的原因\n1 2 # 生成默认配置 containerd config default | sudo tee /etc/containerd/config.toml 配置文件\n1 2 3 4 5 6 7 8 # 修改/etc/containerd/config.toml配置 disabled_plugins = [] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] SystemdCgroup = true # 这里改为true [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;] # 这里修改镜像地址 sandbox_image = \u0026#34;crpi-u3f530xi0zgtvmy0.cn-beijing.personal.cr.aliyuncs.com/image-infra/pause:3.10\u0026#34; 注意plugins.\u0026ldquo;io.containerd.grpc.v1.cri\u0026quot;里面有个systemd_cgroup=true/false,之前出错就是因为有这个，和下面的SystemdCgroup冲突了，有的话注释掉\n1 2 3 4 5 6 7 8 9 # 设置开机启动 systemctl enable containerd # 启动containerd systemctl start containerd # 检查状态 systemctl status containerd 安装kubelet,kubeadm,kubectl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 配置kubernetes 源 cat \u0026lt;\u0026lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/ enabled=1 gpgcheck=1 gpgkey=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/repodata/repomd.xml.key exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni EOF # 安装kubelet，kubeadm，kubectl yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes # 启动kubelet，启动完成查看kubelet的状态可能是异常的，不用惊慌 systemctl enable --now kubelet 集群初始化 1 2 3 4 5 sudo kubeadm init \\ --pod-network-cidr=10.244.0.0/16 \\ --apiserver-advertise-address=192.168.189.100 \\ --control-plane-endpoint=192.168.189.100 \\ --upload-certs 安装flannel网络插件 1 wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml ","date":"2025-06-30T14:59:31+08:00","permalink":"https://Logic0528.github.io/p/k8s%E9%9B%86%E7%BE%A4%E5%88%9D%E5%A7%8B%E5%8C%96/","title":"K8s集群初始化"},{"content":"","date":"2025-06-30T14:59:09+08:00","permalink":"https://Logic0528.github.io/p/%E5%8C%97%E6%BC%82%E6%97%A5%E8%AE%B004/","title":"北漂日记04"},{"content":"实习第一天，报道领电脑，感觉还不错，没人给我上压力，领导还挺和气的中午带我去吃饭，下班时找我聊聊天问候两句，今天像上了一天自习，虽然没学太多东西只是把电脑装点软件但是感觉动力十足，晚上回来后把linux基础作业完成了，配置了桥接模式，需要和主机一个网段，下载了nginx，看了访问日志，主机需要防火墙放行80端口才能访问，现在已经九点半了，接下来洗个澡就十点了，但是今天还没有学新东西，看看阿里的acp什么的，如果去打英雄联盟呢好像一天没休息了也还行，但是会不会又是明日复明日明日何其多呢，感觉早睡也睡不着，今天一百八到手的还挺轻松，算了不打了，到点来局大乱斗睡觉吧，把明天的docker作业看一下，看会acp，一把大乱斗然后睡觉，嗯，完美\n","date":"2025-06-23T21:35:42+08:00","permalink":"https://Logic0528.github.io/p/%E5%8C%97%E6%BC%82%E6%97%A5%E8%AE%B003/","title":"北漂日记03"},{"content":"来到北京的第二天，上午从民宿起床后吃了饭往公寓搬东西入住，两天从看房到入住感觉有些太快了，处处谨慎的问东问西透露出我是个刚出社会的新兵蛋子，房主显然有些不耐烦了，下午坐地铁来的时候突然想听听“北京欢迎你”，也不知道北京欢不欢迎我\n坐地铁时有个空位，我和另一个大叔看上了这个座位但双方都很谦让，他说你做吧，本来我是想坐的毕竟我行李一大堆，但是我看到大叔地中海的发型快要顶不住秃掉觉得有些可怜就说你做吧我还有几站就下车了，哈哈\n来北京两天我已经有好几个兄弟了，他们一半是房产中介，一半是销售\n","date":"2025-06-20T18:27:47+08:00","permalink":"https://Logic0528.github.io/p/%E5%8C%97%E6%BC%82%E6%97%A5%E8%AE%B002/","title":"北漂日记02"},{"content":"“最后一次来你家店吃饭了，离开北京了”\n这个男人快步走进这家面馆，人还没到柜台时我听到了这句话，今天是我第一次来北京。\n他应该是这家店的常客了，老板停顿了一下\n“去哪？”\n“回老家，西安” 深呼吸，吐气\n几秒的沉默，吃面的我也放慢了速度，我意识到这或许是第一段难忘的记忆\n“……” “回去干啥？”\n“一个月三四千，服务员什么的……”\n“还行，在那边三四千比这边六千都舒服，吃点啥”\n“来份小炒肉盖饭吧”\n“再来瓶啤酒”\n每天都有人疲惫地离开，每天都有人迷茫地涌进来，一座城市像一个巨大的有机生物体，细胞们通过一条条血管般的轨道运输到各处，这个生物体里有最令人作呕的腐烂，也有最生机勃勃的希望\n今天坐出租车时在下雨，雨刷器规律的摆动着，有雨水时就刷刷两下，没有雨时就安静的躺在那里，我想如果此时雨刷器在玻璃上跳起拉丁舞，那些一切杂乱的动作组合排列在一起，或许就是生命的意义\n我在心中暗自对自己说，等我离开北京那一天，我的最后一餐要是这家店的小炒肉盖饭，或许可以再来瓶啤酒\n","date":"2025-06-19T22:16:51+08:00","permalink":"https://Logic0528.github.io/p/%E5%8C%97%E6%BC%82%E6%97%A5%E8%AE%B001/","title":"北漂日记01"},{"content":"五月份进行了两次面试，面试不同于闭门造车,问的许多问题需要进行规范的回答类似于八股文之类的，虽然我很不喜欢八股文代表的那种死板的精神，但不得不承认在对于基础知识的掌握有许多不足，在面试过程中很多概念熟悉了解但是语言组织不够流畅，对面试中没能回答上来或者回答不流畅或者以后面试可能会遇到的额问题进行整理。 屡战屡败，屡败屡战，面试中不会的问题记下来解决也是一种收获\n1.k8s架构和基本组件 k8s是一个开源的容器编排平台，用于自动化部署，扩展和管理容器化应用。其架构采用主从架构（master-node），核心组件分为控制平面control plane和工作节点worker node。\n控制平面组件：\nkube-apiserver:API网关和集群控制中心，处理所有管理请求如创建/删除pod，部署应用等 etcd: 分布式键值存储，保存集群配置,如pod,service,namespace\nkube-scheduler: 负责pod调度到合适的节点\nkube-controller-manager: 运行控制器进程，包括node controller,deployment controller,service controller\n节点组件：\nkubelet: 管理节点上的pod\nkube-proxy: 实现网络规则和负载均衡，维护节点上的网络配置\ncontainer runtime: 负责运行容器（如docker，containerd）\n附加组件： coreDNS：集群DNS服务，提供域名解析功能，将服务名解析为IP地址。 IngressContorller：负责外部流量访问，支持负载均衡和路由规则。HTTP路由 NetworkPluginL: calico，flannel等\n2.pod,deployment,service pod： 是k8s中最小的调度单位，封装一个或多个容器，共享网络和存储\ndeployment： 基于ReplicaSet的更高层抽象，声明式管理Pod的期望状态，如副本数，镜像版本，更新策略 作用： 确保pod副本数始终等于期望值，自动重启/创建失败的pod，支持滚动更新，回滚版本\nservice：抽象的网络层，为一组pod提供固定访问入口 ip+端口 作用： 内部服务发现：同意集群内其他pod可通过service域名访问 外部暴露： 通过nodeport等类型将服务暴露到集群外 负载均衡： 将流量分发到后端多个pod副本\ndeployment是管理pod的控制器，service是抽象网络层，三者通过标签匹配。\n3.nginx与负载均衡 nginx是一款高性能的web服务器，反向代理服务器，负载均衡器 web服务器： 处理静态资源的请求，直接返回文件内容，性能远超传统服务器\n反向代理服务器： 将客户端请求转发到后端多个应用服务器如tomcat，隐藏真实服务器地址\n负载均衡器： 分发客户端请求到多个后端服务器，避免单点过载\n负载均衡： 将网络流量均发到多个计算资源如服务器，容器，避免单一节点过载，提高系统的可用性，性能和容错能力\nnginx负载均衡方式： 轮询；默认策略，按顺序分发请求 weight: 根据权重分配 ip_hash： 基于客户端IP哈希 等\u0026hellip;.深入了解后补充\nnginx反向代理的作用与好处：\n作用：将用户请求转发到后端服务器，隐藏后端服务器 好处： 1.节省公网IP，域名是解析到反向代理服务器，后端服务器不需要配置ip 2.提高安全性，反向代理对用户不可见，降低直接攻击后端服务器的风险 3.统一访问入口，反向代理可以代理多个服务器，也就是多个项目，通过域名，将不同的项目转化到不同的后台服务器 4.提高访问速度，反向代理服务器是在后端的前面，所以他可以将一些静态内容缓存到本地，下一次再有请求的时候直接从本地读取，不会像后端发起请求，加快了响应速度\n4.k8s中如何将一个服务暴露到外部，有什么区别 1.nodeport: 在节点上开放一个固定端口，外部流量通过任一节点的该端口转发到该服务，适用于测试环境，内部系统临时暴露 缺点：端口管理复杂，需要避开暴露的端口，仅支持4层负载均衡，安全性差 2.ingress： 基于ingress controller实现7层负载均衡 作用： 提供基于域名的路由规则，实现七层负载均衡，支持路径规则，TLS 加密，基于域名的虚拟主机等功能\n区别： 1.层级不同：nodeport在集群每个节点上暴露端口，ingress在集群外部 2.负载均衡方式：nodeport基于IPVS实现四层负载均衡，ingress基于Nginx实现七层负载均衡 3.功能复杂性：nodeport仅支持基本的负载均衡，ingress支持更丰富的功能，如路由规则，TLS 加密，基于域名的虚拟主机等\n5.dockerfile常用变量，基于nginx镜像做个性化配置 1 2 3 4 5 6 7 8 9 10 11 12 FROM 镜像 USER 设置运行后续命令的用户 WORKDIR 设置工作目录 ENV 设置环境变量 RUN 执行命令，安装软件或配置环境 COPY 将本地文件复制到镜像中 EXPOSE 暴露端口 ENV 定义环境变量，可在后续命令中使用 VOLUME 定义匿名卷，用于数据持久化 CMD 设置容器启动时执行的命令 ENDPOINT 配置容器启动时执行的命令 基于nginx镜像做个性化设置： 在同目录下创建nginx.conf 修改nginx配置，docker build -t mynginx:v1\n6.linux常用命令 1 2 3 4 5 6 7 8 9 10 11 12 13 14 whoami 输出当前用户名 id echo $USER uptime 查看系统负载和运行时间 free 查看内存使用情况 df -h 查看磁盘空间使用情况 ps aux 查看进程信息 netstat -tlnp 查看网络连接情况 lsblk 查看磁盘总容量及使用情况 dd 测试读写速率 iostat 磁盘整体读写速率 lscpu 查看有多少个cpu top 实时监控内存及进程占用 ","date":"2025-06-07T14:51:28+08:00","permalink":"https://Logic0528.github.io/p/%E9%9D%A2%E8%AF%95/","title":"面试"},{"content":"项目目录结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 k8s-deploy-platform/ ├── helm-charts/ # Helm Chart模板 │ └── app-template/ │ ├── Chart.yaml # Chart基本信息 │ ├── values.yaml # 默认配置值 │ └── templates/ # K8s资源模板 │ ├── deployment.yaml # 部署配置 │ ├── service.yaml # 服务配置 │ └── ingress.yaml # 入口配置 │ ├── manifests/ # K8s资源清单 │ ├── applications/ # 应用相关配置 │ │ └── app-template.yaml │ ├── logging/ # 日志系统配置 │ │ └── efk-values.yaml │ └── monitoring/ # 监控系统配置 │ ├── prometheus-rules.yaml # 监控规则 │ ├── prometheus-values.yaml # Prometheus配置 │ ├── pv.yaml # 持久化卷 │ └── storageclass.yaml # 存储类 │ └── scripts/ # 自动化脚本 ├── install.sh # 安装脚本 └── deploy.sh # 部署脚本 helm-charts/app-template Chart.yaml 1 2 3 4 5 apiVersion: v2 name: app-template description: 一个基础的应用部署模板 version: 0.1.0 type: application values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # 应用配置 name: myapp replicas: 2 image: nginx:latest # 资源配置 resources: requests: memory: \u0026#34;64Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; # 健康检查配置 healthCheck: enabled: true path: /health port: 8080 initialDelay: 30 period: 10 # 服务配置 service: type: ClusterIP port: 80 targetPort: 8080 # Ingress配置 ingress: enabled: false className: nginx host: myapp.example.com path: / pathType: Prefix value.yaml中service类型为ClusterIP,clusterip仅在集群内部可以访问，与之对应的是nodeport，nodeport在所有节点上开放指定端口（30000-32767）\nclusterIP让外部访问的方式：\n可以通过配置ingress\n1 2 3 4 5 graph LR A[外部用户] --\u0026gt; B[Ingress] B --\u0026gt; C[ClusterIP Service] C --\u0026gt; D[Pod1] C --\u0026gt; E[Pod2] 也可以使用这条命令临时端口转发\n1 kubectl port-forward -n monitoring svc/monitoring-grafana 3000:80 1 2 3 4 graph LR A[外部用户] --\u0026gt; B[临时端口转发] B --\u0026gt; C[ClusterIP Service] C --\u0026gt; D[Pod1] templates/deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion: apps/v1 kind: Deployment metadata: name: {{ .Values.name }} spec: replicas: {{ .Values.replicas }} selector: matchLabels: app: {{ .Values.name }} template: metadata: labels: app: {{ .Values.name }} spec: containers: - name: {{ .Values.name }} image: {{ .Values.image }} resources: {{ toYaml .Values.resources | nindent 12 }} {{- if .Values.healthCheck.enabled }} livenessProbe: httpGet: path: {{ .Values.healthCheck.path }} port: {{ .Values.healthCheck.port }} initialDelaySeconds: {{ .Values.healthCheck.initialDelay }} periodSeconds: {{ .Values.healthCheck.period }} {{- end }} deployment.yaml这样配置可以根据values.yaml中的配置动态生成deployment资源，相当于values.yaml定义变量，方便修改\ntemplates/service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: Service metadata: name: {{ .Values.name }} spec: type: {{ .Values.service.type }} ports: - port: {{ .Values.service.port }} targetPort: {{ .Values.service.targetPort }} protocol: TCP selector: app: {{ .Values.name }} templates/ingress.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 {{- if .Values.ingress.enabled }} apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: {{ .Values.name }} spec: ingressClassName: {{ .Values.ingress.className }} rules: - host: {{ .Values.ingress.host }} http: paths: - path: {{ .Values.ingress.path }} pathType: {{ .Values.ingress.pathType }} backend: service: name: {{ .Values.name }} port: number: {{ .Values.service.port }} {{- end }} manifests applications/app-template.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion: apps/v1 kind: Deployment metadata: name: {{ .Values.name }} spec: replicas: {{ .Values.replicas }} selector: matchLabels: app: {{ .Values.name }} template: metadata: labels: app: {{ .Values.name }} spec: containers: - name: {{ .Values.name }} image: {{ .Values.image }} resources: {{ toYaml .Values.resources | nindent 12 }} {{- if .Values.healthCheck.enabled }} livenessProbe: httpGet: path: {{ .Values.healthCheck.path }} port: {{ .Values.healthCheck.port }} initialDelaySeconds: {{ .Values.healthCheck.initialDelay }} periodSeconds: {{ .Values.healthCheck.period }} {{- end }} logging/efk-values.yaml 暂时还不会\nmonitoring/prometheus-rules.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: app-alerts namespace: monitoring spec: groups: - name: app rules: - alert: HighCPUUsage expr: container_cpu_usage_seconds_total \u0026gt; 80 for: 5m labels: severity: warning annotations: description: \u0026#34;容器 CPU 使用率超过 80%\u0026#34; - alert: HighMemoryUsage expr: container_memory_usage_bytes \u0026gt; 80 for: 5m labels: severity: warning annotations: description: \u0026#34;容器内存使用率超过 80%\u0026#34; monitoring/prometheus-values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 prometheus: enabled: true serverFiles: prometheus.yml: global: scrape_interval: 15s scrape_configs: - job_name: \u0026#39;kubernetes-pods\u0026#39; kubernetes_sd_configs: - role: pod - job_name: \u0026#39;kubernetes-nodes\u0026#39; kubernetes_sd_configs: - role: node persistence: enabled: true size: 10Gi grafana: enabled: true adminPassword: admin persistence: enabled: true size: 5Gi type: pvc storageClassName: local-storage grafana实现持久化存储时，需要配置storageClassName，这个storageClassName需要在k8s集群中已经存在，这里使用的是local-storage\nmonitoring/storageclass.yaml 1 2 3 4 5 6 7 class.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: local-storage provisioner: kubernetes.io/no-provisioner volumeBindingMode: Immediate monitoring/pv.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: v1 kind: PersistentVolume metadata: name: grafana-pv spec: capacity: storage: 5Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: local-storage local: path: /mnt/data nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: #- master # 替换为你的节点名称 - work-node1 注意：\n这里的path需要创建好，否则会报错 nodeAffinity是为了保证grafana的数据不会被其他节点的pod所使用，就一个工作节点所以没写，master节点上一般会有污点，最好调度到工作节点上 scripts install.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # filepath: scripts/setup-cluster.sh #!/bin/bash # 启动本地集群 start_cluster() { echo \u0026#34;Starting minikube cluster...\u0026#34; minikube start --driver=docker \\ --memory=4096 \\ --cpus=2 \\ --kubernetes-version=v1.24.0 } # 安装必要的组件 install_components() { # 添加 Helm 仓库 helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo add elastic https://helm.elastic.co helm repo update } # 创建必要的命名空间 create_namespaces() { kubectl create namespace monitoring kubectl create namespace logging kubectl create namespace applications } main() { start_cluster install_components create_namespaces } main \u0026#34;$@\u0026#34; deploy.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 #!/bin/bash # 设置变量 APP_NAME=$1 IMAGE=$2 NAMESPACE=${3:-applications} REPLICAS=${4:-1} # 验证输入 if [ -z \u0026#34;$APP_NAME\u0026#34; ] || [ -z \u0026#34;$IMAGE\u0026#34; ]; then echo \u0026#34;Usage: $0 \u0026lt;app-name\u0026gt; \u0026lt;image\u0026gt; [namespace] [replicas]\u0026#34; exit 1 fi # 部署应用 deploy_application() { echo \u0026#34;Deploying $APP_NAME...\u0026#34; helm upgrade --install $APP_NAME ./helm-charts/app-template \\ --namespace $NAMESPACE \\ --set name=$APP_NAME \\ --set image=$IMAGE \\ --set replicas=$REPLICAS } main() { deploy_application } main \u0026#34;$@\u0026#34; 项目暂时完成，grafana全是英文暂时不熟练，后续再完善\n","date":"2025-05-20T10:38:28+08:00","permalink":"https://Logic0528.github.io/p/%E5%9F%BA%E4%BA%8Ek8s%E7%9A%84%E5%BA%94%E7%94%A8%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2%E5%B9%B3%E5%8F%B0/","title":"基于K8s的应用自动化部署平台"},{"content":"污点（Taint） effect: NoSchedule：不允许调度，已经存在的Pod不受影响\nPreferNoSchedule：尽量不调度，已经存在的Pod不受影响\nNoExecute：不允许调度，并且驱逐已经存在的Pod\n容忍（Toleration） operator: Equal：需要key和value完全匹配\nExists：只需要key匹配，不需要value匹配\nvalue: 污点的值\n示例代码：\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: Pod metadata: name: pod-with-tolerations spec: tolerations: - key: \u0026#34;key1\u0026#34; # 污点的键 operator: \u0026#34;Equal\u0026#34; # 操作符：Equal或Exists value: \u0026#34;value1\u0026#34; # 污点的值 effect: \u0026#34;NoSchedule\u0026#34; # 效果 tolerationSeconds: 3600 # 容忍时间（仅用于NoExecute） 亲和力（Affinity） NodeAffinity 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: v1 kind: Pod metadata: name: with-node-affinity spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: # 硬性要求 nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux preferredDuringSchedulingIgnoredDuringExecution: # 软性偏好 - weight: 1 preference: matchExpressions: - key: disk-type operator: In values: - ssd PodAffinity 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: Pod metadata: name: with-pod-affinity spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - web-store topologyKey: kubernetes.io/hostname PodAntiAffinity(节点反亲和力) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: v1 kind: Pod metadata: name: with-pod-antiaffinity spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - cache topologyKey: kubernetes.io/hostname 没什么好说的，类似于标签，慢工出细活~\n","date":"2025-05-15T15:52:46+08:00","permalink":"https://Logic0528.github.io/p/k8s-%E6%B1%A1%E7%82%B9%E4%B8%8E%E5%AE%B9%E5%BF%8D/","title":"K8s-污点与容忍"},{"content":"PV和PVC PV和PVC是Kubernetes中用于管理持久化存储的两个重要概念。\nPV（Persistent Volume） 是集群中的存储资源 由管理员事先创建和配置 独立与pod的生命周期 可以对接各种存储后端，如nfs、ceph、云存储等 定义了存储的容量、访问模式、存储类等信息 PVC（Persistent Volume Claim） 是pod对存储资源的请求 由用户/开发者创建 类似于pod消耗Node资源，PVC消耗PV资源 声明需要的存储容量、访问模式、存储类等信息 不需要关心存储的具体实现细节 工作流程 管理员创建PV，定义存储资源的属性 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: v1 kind: PersistentVolume #描述资源类型为PV类型 metadata: name: pv0001 spec: capacity: #容量配置 storage: 5Gi volumeMode: Filesystem #存储类型为文件系统 accessModes: #访问模式：ReadWriteOnce WeadWriteMany ReadOnlyMany - ReadWriteMany #可被单节点读写 persistentVolumeReclaimPolicy: Retain #回收策略 storageClassName: slow #创建PV的存储类名，需要与pvc的相同 mountOptions: #加载配置 - hard - nfsvers=4.1 nfs: #连接到nfs path: /home/nfs/rw/pv-test #存储路径 server: 192.168.189.130 #nfs服务地址 用户/开发者创建PVC，声明需要的存储资源 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: PersistentVolumeClaim #资源类型为pvc metadata: name: nfs-pvc spec: accessModes: - ReadWriteMany volumeMode: Filesystem resources: requests: storage: 5Gi storageClassName: slow #名字要与对应的pv相同 # selector: #使用选择器选择对应的pv Kubernetes调度器根据PVC的要求，选择合适的PV进行绑定 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: v1 kind: Pod metadata: name: pvc-test-pod spec: containers: - image: nginx name: nginx-volume volumeMounts: - mountPath: /usr/share/nginx/html #挂载到容器的哪个目录 name: test-volume #挂载哪个volume volumes: - name: test-volume persistentVolumeClaim: claimName: nfs-pvc tolerations: - key: \u0026#34;node-role.kubernetes.io/control-plane\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; 动态制备PV 被污点给整破防了，暂时先不写了。（后半句由ai补充），天哪你连我心里想的什么都知道快点拯救全人类吧，待我学完污点归来\n目前遗留问题，-n kube-system po nfs-server-provisioner ImagePullBackOff,kube-apiserver pending状态，selfLink怎么处理，其他的部分理解深入许多\n好的，了解到主节点上默认会有污点，目的是为了保护主节点资源，防止普通pod调\n动态制备pv 创建storageclass 定义存储类型 指定Provisioner 设置存储参数 1 2 3 4 5 6 7 8 9 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: managed-nfs-storage provisioner: fuseim.pri/ifs #外部制备器提供者，编写为提供者的名称 parameters: archiveOnDelete: \u0026#34;false\u0026#34; #是否存档 reclaimPolicy: Retain #回收策略 volumeBindingMode: Immediate #默认为Immediate，表示创建PVC立即进行绑定 用户创建pvc 指定存储大小 指定访问模式 引用storageclass 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 cat nfs-sc-statefulset.yaml --- apiVersion: v1 kind: Service metadata: name: nginx-sc labels: app: nginx-sc spec: type: NodePort ports: - name: web port: 80 protocol: TCP selector: app: nginx-sc --- apiVersion: apps/v1 kind: StatefulSet metadata: name: nginx-sc spec: replicas: 1 #serverName: \u0026#34;nginx-sc\u0026#34; selector: matchLabels: app: nginx-sc template: metadata: labels: app: nginx-sc spec: containers: - image: nginx name: nginx-sc imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /usr/share/nginx/html name: nginx-sc-test-pvc volumeClaimTemplates: - metadata: name: nginx-sc-test-pvc spec: storageClassName: managed-nfs-storage accessModes: - ReadWriteMany resources: requests: storage: 1Gi provisioner创建pv 检测到新建pvc 根据storageclass找到对应的provisioner provisoner创建pv 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 cat nfs-provisioner.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nfs-client-provisioner namespace: kube-system labels: app: nfs-client-provisioner spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:v4.0.0 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: 192.168.189.130 - name: NFS_PATH value: /home/data/nfs/rw volumes: - name: nfs-client-root nfs: server: 192.168.189.130 path: /home/data/nfs/rw 基本过程如上，上次出现污点问题是因为nfs服务器地址写为master节点导致pod部署到master节点，有污点\n另补充一点，provisioner需要特定权限才能创建和管理pv，需要用到rbac.yaml\n","date":"2025-05-11T14:49:19+08:00","permalink":"https://Logic0528.github.io/p/k8s-%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8-pv%E5%92%8Cpvc/","title":"K8s-持久化存储-PV和PVC"},{"content":"超级帅哥悄悄路过\n","date":"2025-05-08T23:05:44+08:00","permalink":"https://Logic0528.github.io/p/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/","title":"第一篇博客"}]