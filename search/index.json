[{"content":"一.命令类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 yum install lrzsz rz #上传文件，500M以下 #远程文件同步 rsync -avz -e ssh /path/to/source/ user@remote:/path/to/destination/ # -a 归档模式，递归并保留文件属性 # -v 详细输出模式 # -z 传输时压缩文件数据 find /path -name \u0026#34;filename\u0026#34; #vim :1,100yank #复制第一到第一百行内容 :100 #跳转至第一百行 :%d #删除全部 :5,10d sed -i \u0026#39;s/old/new/g\u0026#39; file.txt kubectl -n spline expose deployment front-end --type=NodePort -port=80 --target-port=80 --name=front-end-svc # kubectl expose 和 kubectl create service都能创建service，expose专门用于 “为已有工作负载（如 Deployment、Pod、ReplicaSet 等）自动创建 Service” 二.问题类 1 2 3 /data/bkci_inst/script/ci.env #环境变量 /data/bkee/logs/ci/ #日志 1.mongodb未安装，导致脚本后续用户创建失败认证失败等一系列问题 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # 安装 MongoDB sudo rpm -ivh /data/myhttpd/data/cw_yum_3.0/mongodb-org-server-4.4.15-1.el8.x86_64.rpm sudo rpm -ivh /data/myhttpd/data/cw_yum_3.0/mongodb-org-shell-4.4.15-1.el8.x86_64.rpm sudo rpm -ivh /data/myhttpd/data/cw_yum_3.0/mongodb-database-tools-rhel70-x86_64-100.10.0.rpm # 编辑配置文件 sudo vim /etc/mongod.conf # bindIp: 0.0.0.0 # 找到并注释掉认证配置 # security: # authorization: enabled # 注释这两行，临时关闭认证 # 重启 MongoDB 服务 sudo systemctl start mongod # 查看变量里存储的密码，用户名一般就是root,下面进行手动创建用户 echo $BK_MONGODB_ADMIN_PASSWORD # 无密码连接 MongoDB（此时关闭了认证） mongo mongodb://10.11.27.174:27017/admin // 切换到 admin 数据库（管理员用户必须创建在 admin 库） use admin // 创建 root 用户，密码使用环境变量中的 JYMFo_iY9zBj db.createUser({ user: \u0026#34;root\u0026#34;, // 与 BK_MONGODB_ADMIN_USER 一致 pwd: \u0026#34;JYMFo_iY9zBj\u0026#34;, // 与 BK_MONGODB_ADMIN_PASSWORD 一致 roles: [{ role: \u0026#34;root\u0026#34;, db: \u0026#34;admin\u0026#34; }] // 授予最高权限（确保能创建其他用户） }) // 验证用户是否创建成功 db.getUser(\u0026#34;root\u0026#34;) // 应输出用户信息，确认密码哈希值存在 // 退出终端 exit 2.yum源相关问题 yum源安装失败 1 2 3 4 cat /etc/sysconfig/myhttpd.yaml #看端口是否是8070 baseurl= ip:8070///文件路径 #修改后重启myhttpd服务 非中控机无法通过网络访问中控机yum源 1 2 cat /etc/yum.repo/cwyum.repo # 查看是否IP地址后是否有8070端口，该路径下全部repo文件都需要改 缺少yum源，可能是缺少麒麟的yum源 1 2 3 4 5 6 cd /etc/yum.repo/ ls # 有个bak文件夹里面有麒麟的yum源，mv ky.repo ../ yum clean all yum makecache 3.pulsar脚本访问java路径不存在 路径不存在或是后面的mysql等服务没起来，原因可能是文件路径与脚本不匹配导致的\n1 2 3 4 5 6 7 8 9 10 11 # 正确设置 JAVA_HOME # 获取 JAVA_HOME 的正确值 readlink -f $(which java) | sed \u0026#34;s:bin/java::\u0026#34; # 更新环境变量 sudo vi /etc/profile.d/java.sh # 根据实际输出结果修改下面的路径 export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.261.b12-1.el7_7.x86_64/ export PATH=$JAVA_HOME/bin:$PATH # 刷新环境变量 source /etc/profile.d/java.sh 该问题后续会导致link_pulsar_to_mq租户不存在\n1 2 3 4 5 6 7 8 9 10 cd /data/apache-pulsar-2.10.6 bin/pulsar-admin tenants create cpack bin/pulsar-admin namespaces create cpack/bkrepo # 查看所有租户 bin/pulsar-admin tenants list # 查看 cpack 租户下的命名空间 bin/pulsar-admin namespaces list cpack 4.cousul无法访问 缺少client_addr,默认值为127.0.0.1，即仅允许本地访问8500端口，导致应用通过10.11.26.166：8500访问时被拒绝\n1 2 3 4 5 6 7 8 # 编辑配置文件 vi /etc/consul.d/consul.json { \u0026#34;client_addr\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, // 新增此行，允许所有接口访问 HTTP 接口 \u0026#34;bind_addr\u0026#34;: \u0026#34;10.11.26.166\u0026#34;, ... } 5.mongodb,redis,mysql,zookeeper等服务启动失败 很大概率是因为目录权限问题，或者是你在重启机器后发现原来正常的mysql启动不起来了 正确属组应该是mysql:mysql而不是blueking:blueking\n1 2 3 4 5 6 7 8 9 10 # 查看当前配置文件权限 ls -l /etc/redis/sentinel-default.conf # 设置权限：允许 redis 用户读写配置文件 sudo chown redis:redis /etc/redis/sentinel-default.conf sudo chmod 644 /etc/redis/sentinel-default.conf # 确保可读可写 # 确认权限已生效 ls -l /etc/redis/sentinel-default.conf # 输出应显示：-rw-r--r-- 1 redis redis ... 6.Mongodb从节点加入不了集群 1 2 3 4 5 6 7 8 Client view of cluster state is {type=REPLICA_SET, servers=[]} \u0026gt; rs.status0 “ok” : 0, \u0026#34;errmsg\u0026#34;:\u0026#34;not running with --replSet\u0026#34;, \u0026#34;code\u0026#34; : 76, \u0026#34;codeName\u0026#34; : \u0026#34;NoReplicationEnabled\u0026#34; \u0026gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # 修改配置文件(通常为/etc/mongod.conf)： replication: replSetName：\u0026#34;你的副本集名称\u0026#34; # 例如：rs0 # 用管理员账号登录MongoDB mongo --host 10.11.27.172 -u root -p JYMFo_iY9zBj -authenticationDatabase admin # 初始化副本集（单节点配置） rs.initiate({ _id：\u0026#34;你的副本集名称\u0026#34;， members:[ ｛_id: 0,host: \u0026#34;10.11.27.172:27017\u0026#34;} ] #修改应用配置，添加replicaSet参数: spring: data: mongodb: uri：mongodb://root:JYMFo_iY9zBj@10.11.27.172:27017/你的数据库名?replicaSet=你的副本集名称 7.安装es7失败 1 2 # 删除无效插件目录 sudo rm -rf /usr/share/elasticsearch/plugins/elasticsearch-analysis-ik- 8.vteam未注册consul服务 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 vim /data/bkee/ci/vteam/BOOT-INF/classes/application-bklatest.yml # 修改内容如下： cloud: consul: # host: 192.168.104.81 # Consul 服务器地址（填写实际运行 Consul 的节点 IP，如 10.11.27.175） host: 10.11.27.175 # Consul 服务端口（默认 8500） port: 53 discovery: # 服务名（必须为 vteam-devops，与 DNS 解析的服务名匹配） serviceName: vteam-devops # 服务标签（必须包含 devops，与 DNS 解析的 devops. 前缀匹配） tags: devops # 启用服务注册（必须设为 true） register: true # 注册时使用服务所在节点的实际 IP（10.11.27.171） preferIpAddress: true # 服务端口（若未指定，默认使用应用启动端口，如 8080，确保端口未被占用） port: 8080 systemctl restart bk-ci-vteam.service 9.mongod连接超时，副本集配置不匹配 1 2 3 4 5 6 7 8 9 10 11 12 # mongod配置的一个是副本集连接，但是主节点没有副本集 # 快速解决 vim /data/bkee/etc/ci/application-cteam.yml data: mongodb: uri: mongodb://ctest:ctest@10.11.27.174:27017/db_devops_ci_ctest?maxPoolSize=50\u0026amp;slaveOk=true\u0026amp;safe=true\u0026amp;w=1\u0026amp;wtimeoutMS=2000 # uri: mongodb://ctest:ctest@10.11.27.174:27017/db_devops_ci_ctest?maxPoolSize=50\u0026amp;replicaSet=rs0\u0026amp;slaveOk=true\u0026amp;safe=true\u0026amp;w=1\u0026amp;wtimeoutMS=2000 field-naming-strategy: org.springframework.data.mapping.model.SnakeCaseFieldNamingStrategy auto-index-creation: true # 删除replicaset=rs0或者是去主节点上初始化副本集rs0 10.批量启动和停止服务 1 2 3 4 5 6 7 8 9 [Unit] # 关键配置：将服务与目标关联 PartOf=bk-ci.target # 指定服务在目标启动时自动启动 WantedBy=bk-ci.target systemctl stop bk-ci.service # 由于partof属性，相关联的服务也都停止 systemctl start bk-ci.service # 由于wantedby属性，相关联的服务也都启动 ","date":"2025-07-31T14:49:19+08:00","permalink":"https://Logic0528.github.io/p/%E4%B8%83%E6%9C%88%E5%AE%9E%E4%B9%A0%E6%80%BB%E7%BB%93/","title":"七月实习总结"},{"content":"CKA认证 一.PVC 注意：解题前需连接到对应的主机。 题目： mariadb namespace 中的MariaDB Deployment 被误删除。请恢复该deployment并确保数据持久性。\n请按照以下步骤:\n如下规格在mariadb namespace 中创建名为 mariadb 的PVC： - 访问模式为ReadWriteOnce(依题目) - 存储为250Mi 集群中现有一个PersistentVolume,您必须使用现有的PV。 编辑位于~/mariadb-deployment.yaml 的 MariaDB Deployment 文件，以使用上一步中创建的PVC。 将更新的 Deploment 文件应用到集群 确保MariaDB Deployment正在运行且稳定 搭建模拟环境 正式考试中不需要搭建模拟环境，只需按照下面的解题步骤解题即可。在自己电脑上可以搭建个模拟环境模拟考试，同时起到练习的作用\n1.创建本地供应商\n​\t动态制备 PV ,我们需要一个provisioner,一个 storageclass,在local-path-storage命名空间下。\n​\tProvisioner 是一个核心组件，负责动态创建和管理持久化存储，Provisioner允许在Pod请求存储时自动创建所需的存储卷，无需管理员预先创建，这依赖于StorageClass资源的定义\n1 kubectl create ns local-path-storage local-path-provisioner.yaml\nlocal-path-storage.yaml\n1 2.创建namespace\n1 kubectl create ns mariadb 3.mariadb-pv.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: PersistentVolume metadata: name: mariadb-pv spec: capacity: storage:250Mi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClasName: local-path hostPath: path: /mnt/data/mariadb 1 2 kubectl apply -f mariadb-pv.yaml kubectl get pv 4.mariadb-deployment.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: apps/v1 kind: Deployment metadata: name: mariadb namespace: mariadb spec: replicas: 1 selector: matchLabels: app: mariadb template: metadata: labels: app: mariadb spec: containers: - name: mariadb image: mariadb:10.5 imagePullPolicy: IfNotPresent ports: - containerPort: 3306 env: - name: MYSQL_ROOT_PASSWORD value: \u0026#34;rootpassword\u0026#34; 解题 解题即是在考试环境下需要做的操作\n切换到指定节点\n检查pv,以及pv用到的storageclass\n1 2 kubectl get pv kubectl get sc 创建pvc\n1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mariadb ##按题目要求修改 namespace: mariadb ##按题目要求修改 spec: storageClassName: local-path ##第二步查到的sc accessModes: - ReadWriteOnce ##按题目要求修改 resources: requests: storage: 250Mi ##按题目要求修改 1 2 kubectl apply -f pvc.yaml kubectl get pvc -n mariadb 配置Deployment关联PVC\n1 vim ~/mariadb-deployment.yaml 增加volumes和volumeMounts部分\n1 2 3 4 5 6 7 volumes： - name:mariadb-data persistentVolumeClaim: claimName: \u0026#34;mariadb\u0026#34; volumesMounts: - name: mariadb-data mountPath: /var/lib/mysql ##按题目要求修改 完整的deployment.yaml文件如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion: apps/v1 kind: Deployment metadata: name: mariadb namespace: mariadb # 指定 namespace spec: replicas: 1 selector: matchLabels: app: mariadb template: metadata: labels: app: mariadb spec: containers: - name: mariadb image: mariadb:10.5 # 以考试为准，无需自己指定 imagePullPolicy: IfNotPresent ports: - containerPort: 3306 env: - name: MYSQL_ROOT_PASSWORD value: \u0026#34;rootpassword\u0026#34; # 可按题目或需求设置 volumeMounts: - name: mariadb-da mountPath: /var/lib/mysql # 数据挂载路径 volumes: - name: mariadb-data persistentVolumeClaim: claimName: \u0026#34;mariadb\u0026#34; 检查Deployment和Pod运行状态\n1 2 kubectl apply -f ~/mariadb-deployment.yaml kubectl get deployment -n mariadb Pod状态应为Running\nexit退出当前节点\n二.四层代理Service 题目：\n重新配置 spline namespace 中现有的 front-end Deployment, 以公开现有容器nginx的端口80/TCP。 创建一个名为 front-end-svc 的新Service，以公开容器端口 80/TCP 配置新的Service，以通过NodePort公开各个Pod 搭建模拟环境 创建namespace\n1 kubectl create ns spline front-end Deployment\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 vim 2-svc-deployment.yaml apiVersion: v1 kind: Deployment metadata: name：front-end namespace: spline spec: replicas: 1 selector: matchLabels: app: front-end template: metadata: labels: app: front-end spec: containers: - name: nginx image: nginx:1.25 imagePullPolicy: IfNotPresent 1 kubectl apply -f 2-svc-deploy.yaml 解题 连接指定节点\n修改 front-end deployment 配置，添加端口配置\n在name: nginx下方新增端口配置\n1 2 3 4 5 kubectl -n spline edit deployment front-end ports: - containerPort: 80 protocol: TCP 注意：\n正确缩进，确保ports在name: nginx同层级下 执行 kubectl -n spline edit deployment front-end 之后若提示如下，说明修改成功:deploymnet.apps/front-end edited 暴露 Deployment，创建 NodePort 类型的Service，执行命令：\n1 kubectl -n spline expose deployment front-end --type=NodePort -port=80 --target-port=80 --name=front-end-svc 看到service/front-end-svc exposed,表示创建service成功 参数说明：\n\u0026ndash;type=NodePort : 依题目要求 \u0026ndash;port=80 ：Service监听端口 \u0026ndash;target-port : 容器内应用端口 \u0026ndash;name=front-end-svc : Service名称 查看Service\n1 2 3 4 kubectl -n spline get svc front-end-svc -o wide #测试访问，用查到的cluster-ip curl 10.96.101.208:80 exit退出当前节点\n三.七层代理资源Ingress 题目：创建新的 Ingress 资源，具体如下： 名称： echo-hello Namespace: sound\n使用 Service 端口 8080 在 http://example.org/echo-hello 上公开 echoserver-service 的 Service\n可以使用以下命令检查 echoserver-service Service 的可用性，该命令应返回 hello world\n搭建模拟环境 安装Ingress-nginx-controller\n1 2 3 4 5 #ingress-deploy.yaml kubectl apply -f ingress-deploy.yaml 创建命名空间\n1 kubectl create ns sound 创建pod\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 vim 3-ingress-deploy-pod.yaml apiVersion: v1 kind: Deployment metadata: name: echoserver namespace: sound spec: repliacs: 1 selector: matchLabels: app: echoserver template: metadata: labels: app: echoserver spec: containers: - name: echoserver image: docker.io/library/tomcat:8.5.34-jre8-alpine imagePullPolicy: IfNotPresent ports: - containerPort:8080 1 kubectl apply -f 3-ingress-deploy-pod.yaml 创建service\n1 vim 3-ingress-nsvc.yaml 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: Service metadata: name: echoserver-service namespace: sound spec: selector: app: echoserver ports: - protocol: TCP port: 8080 targetPort: 8080 1 2 3 4 5 kubectl apply -f 3-ingress-svc.yaml kubectl get svc -n sound curl ip:8080 #返回网页内容，说明可以通过svc代理pod 解题 连接到指定节点\n查询 IngressClass ,确认 class 名字\n1 2 3 4 5 6 kubectl get ingressclasses.networking.k8s.io #kubectl get ingclass # 等效于原命令，更简洁 #查询结果如下 NAME CONTROLLER nginx k8s.io/ingress-nginx 编写 ingress.yaml\n1 vim ingress.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion: v1 kind: Ingress metadata: name: echo-hello namespace: sound annotations: nginx.ingress.kubernetes.io/rewrite-target: / # 将请求路径重写为根路径 spec: ingressClassName: nginx # 指定使用nginx类型的Ingress控制器 rules: - host: example.org # 匹配的域名 http: # HTTP路由规则 paths: - path: /echo-hello # 匹配的路径 pathType: Prefix # 路径匹配类型为前缀匹配 backend: service: name: echoserver-service # 转发到的服务名称 port: number: 8080 # 转发到的服务端口 创建 ingress 资源\n1 2 3 4 5 6 7 kubectl apply -f ingress.yaml kubectl get ingress -n sound #验证Ingress配置 curl http://example.org/echo-hello exit 四.网络策略NetworkPolicy 题目：从提供的YAML文件中选择并应用适当的NetworkPolicy。确保所选的NetworkPolicy不会过于宽松，同时允许运行在fronted和backend namespace 中的 fronted 和 backend Deployment之间的通信。\n首先，分析fronted 和 backend Deployment,了解其通信需求，以便确定需要应用的NetworkPolicy.\n接着，检查位于 ~/netpol 文件夹中的 NetworkPolicy YAML实例。\n最后，应用一个NetworkPolicy,以启用 frontend 和 backend Deployment 之间的通信，但不要使其过于宽松。\n搭建模拟环境 创建命名空间\n1 2 kubectl create ns frontend kubectl create ns backend 在 fronted 和 backend 下部署pod\n1 vim 4-fronted.aml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: apps/v1 kind: Deployment metadata: name: fronted-app namespace: frontend spec: replicas: 1 selector: matchLabels: app: frontend template: metadata: labels: app: frontend spec: containers: - name: nginx image: nginx:1.25 imagePullPolicy: IfNotPresent ports: - containerPort:80 1 kubectl apply -f 4-frontend.yaml 1 vim 4-backend.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: apps/v1 kind: Deployment metadata: name: backend-app namespace: backend spec: replicas: 1 selector: matchLabels: app: backend template: metadata: labels: app: backend spec: containers: - name: nginx image: nginx:1.25 imagePullPolicy: IfNotPresent ports: - containerPort: 80 1 kubectl apply -f 4-backend.yaml 创建backend默认拒绝策略\n1 vim networkpolicy.yaml 1 2 3 4 5 6 7 8 apiVersion: networking.k8s.io/v1 kind： NetworkPolicy metadata: name: default-deny-all spec: podSelector: {} policyTypes: - Ingress 解题 连接到指定节点\n检查fronted和backend两个namespace的标签\n1 kubectl get ns frontend backend --show-labls 检查frontend和backend两个namespace下所有pod的标签\n1 2 kubectl -n frontend get pod --show-labels kubectl -n backend get pod --show-labels 查看默认拒绝策略 检查backend namespace中是否有默认拒绝的Network Policy\n1 kubectl get networkpolicy -n backend 查看题目给定的NetworkPolicy示例，并选择最合适的策略\n1 2 3 4 5 6 cat ~/netpol/netpol1.yaml cat ~/netpol/netpol2.yaml cat ~/netpol/netpol3.yaml ## 根据题目要求选择合适的应用 kubectl apply -f *** 五.配置管理中心configmap 题目：\n名为nginx-static-test的NGINX Deployment 正在nginx-static-test namespace 中运行，它通过名为 nginx-static-config 的ConfigMap进行配置。\n更新nginx-static-config 这个configmap 以仅允许 TLSv1.3连接\n注意：您可以根据需要重新创建，启动或扩展资源\n您可以使用以下命令测试更改：\n1 curl -k --tls-max 1.2 https://web.k8snginx.local 搭建模拟环境 创建命名空间\n1 kubectl create ns nginx-static-test 资源准备：ConfigMap\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion: v1 kind: ConfigMap metadata: name: nginx-static-config namespace: nginx-static-test data: nginx.conf:{ worker_processes 1; envnts{ worker_connectionss 1024; } http{ # SSL配置加在这里，http块内，server块外 ssl_protocols TLSv1.2; ssl_ciphers \u0026#39;ECHDE-RSA-***\u0026#39; ssl_perfer_srever_ciphers on; } server{ listen 80; server_name web.k8snginx.local; localtion/{ root /usr/share/nginx/html; index index.html index.htm; } } } kubectl apply -f nginx-static-config.yaml 资源准备：Deployment\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-static-test namespace: nginx-static-test spec: replicas: 1 selector: matchlabels: app: nginx-static-test template: metadata: labels: app: nginx-static-test spec: containers: - name: nginx image: nginx:1.25 volumeMounts: - name: nginx-config mountPath: /etc/nginx/nginx.conf subPath: nginx.conf volumes: - name: nginx-config configMap: name: nginx-static-config 1 2 3 kubectl apply -f nginx-static-deployment.yaml kubectl get pods -n nginx-static-test 解题 连接到指定节点\n导出现有的nginx-static-config ConfigMap\n1 kubectl -n nginx-static-test get configmap nginx-static-config -o yaml \u0026gt; nginx-static-config.yaml 修改ConfigMap的ssl_protocols配置\n1 2 3 4 5 6 vim nginx-static-config.yaml data: nginx.conf: #其他配置... ssl_protocols TLSv1.3; # 按题目要求修改 先删除旧的ConfigMap,再创建新的configmap\n1 2 3 kubectl delete configmap nginx-static-test -n nginx-static-test kubectl apply -f nginx-static-test -n nginx-static-test 重启nginx-static-test Deployment\n1 2 3 kubectl -n nginx-static-test rollout restart deployment nginx-static-test kubectl get pods -n nginx-static-test 测试TLSv1.2连接，按照目前修改，访问是失败的说明配置正确\n1 2 3 curl -k --tls-max 1.2 https://web.k8snginx.local exit 六.存储类storageclass 题目：\n首先，为名为rancher.io/local-path的现有制备器，创建一个名为test-local-path的新Storageclass\n将卷绑定模式设置为 WaitForFirstConsumer\n接下来，将test-local-path Storageclass设置为默认的Storageclass\n请勿修改任何现有的Deployment和PVC\n解题 连接至指定节点\n创建正确的Storageclass Yaml文件\n1 2 3 4 5 6 7 8 9 10 11 12 vim sc.yaml apiVeison: storage.k8s.io/v1 kind: Storageclass metadata: name: test-local-path annotations: storageclass.kubernetes.io/is-default-class:\u0026#34;true\u0026#34; provisioner: rancher.io/local-path # 制备器的名字 volumeBindingMode: WaitForFirstConsumer # 卷绑定模式依题目要求 kubectl apply -f sc.yaml 验证test-local-path是否是默认的Storageclass\n1 2 3 4 5 6 7 kubectl get storageclass # 输出如下 NAME　PROVISIONER DEFAULT test-local-path(default) rancher.io/local-path true # 提示是true说明是对的 exit退出当前节点\n七.HPA（Pod水平自动扩缩容） 题目：\n在 autoscale namespace 中创建一个名为 nginx-server 的新 HorizontalPodAutoscaler(HPA),此HPA必须定位到 autoscale namespace 中名为 nginx-server的现有Deployment\n将HPA设置为每个Pod的CPU使用率旨在50%。将其配置为至少有一个Pod，且不超过4个Pod。此外，将缩小稳定窗口设置为30秒\n搭建模拟环境 解题 连接至指定节点\n创建hpa\n1 kubectl autoscale deployment nginx-server --cpu-percent=50 --min=1 --max=4 -n autoscale 修改HPA的稳定窗口为30s\n1 2 3 4 5 6 7 8 kubectl edit horizontalpodautoscaler nginx-server -n autoscale spec: maxReplicas: 4 minReplicas: 1 behavior: # 新增字段 scaleDown: stabilizationWindowsSeconds: 30 exit退出当前节点\n八.Pod优先级PriorityClass 题目：\n为用户工作负载创建一个名为 high-priority 的新 PriorityClass,其值比用户定义的现有最高优先级类值小一。修改在priority namespace 中运行的现有 busybox-logger Deployment，以使用 high-priority\n确保 busybox-logger Deployment 在设置了新优先级类后成功部署\n搭建模拟环境 创建namespace\n1 kubectl create ns priority 创建PriorityClass资源\n1 2 3 4 5 6 7 8 9 10 vim priority-init.yaml apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: max-priority value: 1000000000 globalDefault: false kubectl apply -f priority-init.yaml 创建busybox-logger这个deployment资源\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 vim busybox-logger.yaml apiVersion: apps/v1 kind: Deployment metadata: name: busybox-logger namespace: priority spec: replicas: 1 selector: matchLabels: app: busybox-logger template: metadata: labels: app: busybox-logger spec: containers: - name: busybox image: busybox:1.28 imagePollPolicy: IfNotPresent command: [\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;while true;do echo \u0026#39;Logging...\u0026#39;;sleep 5;done\u0026#34;] kubectl apply -f busybox-logger.yaml kubectl get pods -n priority 解题 连接到指定节点\n查看当前集群存在的PriorityClass\n1 kubectl get priorityclass 创建新的PriorityClass\n1 2 3 4 5 6 7 8 9 10 vim priority.yaml apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: high-priority value: 999999999 globalDefault: false kubectl apply -f priority.yaml 验证PriorityClass是否创建成功\n1 kubectl get priorityclass 在线修改Deployment的priorityClassName\n1 2 3 4 5 kubectl edit deployment busybox-logger -n priority # 找到spec.template.spec部分，通常在dnsPolicy:ClusterFirst上方 # 添加以下内容 priorityClassName: high-priority exit退出当前节点\n九.resource配置 cpu 和 内存请求和限制 题目：\n您管理一个 WordPress 应用程序。由于资源的请求过高，导致某些pod无法启动\nrelative-fawn namesapce 中的 WordPress 应用程序包含： 具有 3 个副本的 WordPress Deployment\n按照如下方式调整所有 pod 的资源请求 - 将节点资源平均分配给这 3 个 Pod - 为每个 Pod 分配公平的 CPU 和内存份额 - 添加足够的开销以保持节点稳定\n请确保，对容器和初始化容器使用完全相同的请求，您无需更改任何资源限制\n在更新资源请求时，可以暂时将 WordPress Deployment 缩放为 0 个副本可能会有所帮助\n更新后，请确认：\n- WordPress 保持 3 个副本\r- 所有 Pod 都在运行并准备就绪\r搭建模拟环境 创建命名空间\n1 kubectl create namespace relative-fawn 创建 WordPress Deployment\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 vim wordpress-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: wordpress namespace: relative-fawn spec: replicas: 3 selector: matchLabels: app: wordpress template: metadata: labels: app: wordpress spec: initContainers: - name: init-mysql image: busybox:1.28 imagePullPolicy: IfNotPresent command: [\u0026#39;sh\u0026#39;,\u0026#39;-c\u0026#39;,\u0026#39;sleep 6\u0026#39;] resources: limits: cpu: 1000m memory: 500Mi containers: - name: wordpress imagePullPolicy: IfNotPresnet ports: - containerPort: 80 resources: limits: cpu: 1000m memory: 500Mi kubectl apply -f wordpress -deployment.yaml kubectl get pods -n relative-fawn 解题 连接到指定节点\n将WordPress Deployment 副本缩放为0\n1 2 3 4 5 6 7 # 必须先缩为 0 再改 requests,否则新 Pod 起不来 kubectl -n relative scale deployment wordpress --replicas=0 # 检查副本数 kubectl -n relative-fawn get deployment wordpress # 如果ready为0，说明缩放成功 检查 Node 资源使用情况\n1 2 3 kubectl get nodes kubectl describe node node-name 修改 Deployment 资源\n1 2 3 4 5 6 7 kubectl -n relative-fawn edit deployment wordpress # 将 2 个 container 的 requests 部分改为： resources: requests: cpu: 100m memory: 200Mi 将 wordpress 副本数恢复为 3\n1 kubectl -n relative-fawn scale deployment wordpress --replicas=3 检查pod副本数\n1 kubectl -n relative-fawn get pod exit退出当前节点\n十.自定义资源CRD 题目：\n验证已经部署到集群的 cert-manager 应用程序。\n使用 kubectl 将 cert-manager 命名空间所有自定义的资源（CRD）的列表，保存到~/resources.yaml\n使用 kubectl,提取定制资源 Certificate 的 subject 规范字段的文档，并将其保存到~/subject.yaml\n解题 连接至指定节点\n检查 cert-manager 资源\n1 kubectl get pods -n cert-manager 获取 cert-manager 的 CRD 资源列表\n1 2 3 4 kubectl get crd | grep cert-manager # 保存结果到文件 kubectl get crd | grep cert-manager \u0026gt; ~resource.yaml 获取 Certificate 的 subject 规范字段\n1 2 3 4 kubectl explain certificate.spec.subject # 保存结果到文件 kubectl explain certificate.spec.subject \u0026gt; ~/subject.yaml exit退出当前节点\n十一.Gateway网关 题目：\n将现有 Web 应用程序从 Ingress 迁移到 Gateway API ，您必须维护 HTTPS 访问权限。\n注意：集群中安装了一个名为 nginx 的 Gatewayclass。\n首先，创建一个名为 web-local-gatewayn的 Gateway，主机名为 gateway.web.k8s.local ,并保持现有名为 web 的 Ingress 资源的现有 TLS和侦听器配置。\n接下来，创建一个名为 web-route 的 HTTPRoute, 主机名为gateway.web.k8s.local ,并保持现有名为 web 的 Ingress 资源的现有路由规则。\n您可以使用以下命令测试 Gateway API 配置：\ncurl -Lk https://gateway.web.k8s.local:31443\n最后，删除名为 web 的现有 Ingress 资源。\n搭建模拟环境 创建 deployment 和 service\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 apiVersion: apps/v1 kind: Deployment metadata: name: web spec: replicas: 1 selector: matchLabels: app: web template: metadata: labels: app: web spec: containers: - name: web image: nginx:1.25 imagePullPolicy: IfNotPresent ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: web spec: selector: app: web ports: - port: 80 targetPort: 80 kubectl apply -f web-deploy.yaml kubectl get pods 准备TLS Secret\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 # 用openssl生成key和crt openssl req -x509 -nodes -days 365 -newkey rsa:2048 \\ -subj \u0026#34;/CN=gateway.web.k8s.local\u0026#34;\\ -keyout tls.key -out tls.crt # 创建sercret kubectl create secret tls web-cert --cert=tls.crt --key=tls.key kubectl get secert web-cert -oyaml # 创建ingress,后面要迁移到gateway vim web-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: web spec: ingressClassName: nginx tls: - hosts: - gateway.web.k8s.local secretName: web-cert rules: **- host: gateway.web.k8s.local** http: paths: - path: / pathType: Prefix backend: service: name: web port: number: 80 kubectl apply -f web-ingress.yaml kubectl get ingress 解题 连接至指定节点\n检查现有的 ingress ,提取重要信息\n1 kubectl get ingress web -o yaml 创建gateway\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 vim gateway.yaml apiVersion: gateway.networking.k8s.io/v1 kind: Gateway metadata: name: web-local-gateway spec： gatewayClassName: nginx # 依题目修改 listeners: - name: https protocol: HTTPS # 看上面ingress是什么 port: 443 hostname: gateway.web.k8s.local tls: mode: Terminate certificateRefs: - name: web-cert # 填写ingress里的tls secretname kubectl apply -f gateway.yaml kubectl get gateway 创建 httproute\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 vim httproute.yaml apiVersion: gateway.networking.k8s.io/v1 kind: HTTPRoute metadata: name: web-route spec: parentRefs: - name: web-local-gateway hostnames: - \u0026#34;gateway.web.k8s.local\u0026#34; rules: - matches: - path: type: PathPrefix value: / # ingress里path路径 backendRefs: - name: web # ingress里service name port: 80 # ingress里service port kubectl apply -f httproute.yaml kubectl get httproute 测试gateway api 配置\n1 curl -Lk https://gateway.web.k8s.local:31443 删除web ingress\n1 kubectl delete ingress web exit退出当前节点\n十二.sidecar边车容器 题目：\n为了将传统应用程序集成到K8s的日志架构（如kubectl logs）中，通常的做法是添加一个用于日志流式传输的sidecar容器。\n更新现有的sync-leverager Deployment,将使用busybox:stable镜像，且名为sidecar的并置容器，添加到现有的pod,新的并置容器必须运行以下命令： /bin/sh -c \u0026ldquo;tail -n+1 -f /var/log/sync-leverager.log\u0026rdquo;\n使用挂载在 /var/log 的Volume，使日志文件sync-leverager.log可供并置容器使用\n除了添加所需的卷挂载之外，请勿修改现有容器的规范\n搭建模拟环境 创建deployment资源 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 vim sidecar-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: sync-leverager labels: app: sync-leverager spec: replicas: 1 selector: matchLabels: app: sync-leverager template: metadata: labels: app: sync-leverager spec: containers: - name: sync-leverager image: nginx:1.25 imagePullPolicy: IfNotPresent command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;while true;do echo \\\u0026#34;$(date) INFO log line\\\u0026#34; \u0026gt;\u0026gt; /var/log/sync-leverager.log;sleep 5;done\u0026#34;] kubectl apply -f sidecar-deploy.yaml 解题 连接到指定节点\n导出yaml文件\n1 kubectl get deployment sync-leverager -o yaml \u0026gt; sidecar.yaml 编辑yaml文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 vim sidecar.yaml # 新增内容如下 spec.template.spec: volumes: - name: varlog emptyDir: {} containers: ... - command: ... volumeMounts: - name: varlog mountPath: /var/log - name: sidecar image: busybox:stable imagePullPolicy: IfNotPresent args: [\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;tail -n+1 -f /var/log/sync-leverager.log\u0026#34;] volumeMounts: - name: varlog mountPath: /var/log 1 kubectl apply -f sidecar.yaml 检查deployment,pod,sidecar容器日志\n1 2 3 4 5 kubectl get deployment sync-leverager kubectl get pod | grep sync-leverager kubectl logs sync-leverager-** -c sidecar exit退出当前节点\n十三.calico网络插件 题目：\n文档地址\nFlannel Manifest： https://github.com/flannel-io/flannel/releases/download/v0.26.1/kube-flannel.yml\nCalico Manifest： https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/tigera-operator.yaml\nContext： 集群的 CNI 未通过安全审核，已被移除。您必须安装一个可以实施网络策略的新 CNI。\nTask:\n安装并设置满足以下要求的容器网络接口（CNI）：\n选择并安装以下CNI选项之一：\nFlannel版本 0.26.1 Calico版本 3.27.0 选择的CNI必须：\n让Pod相互通信 支持Network Policy实施 从清单文件安装（请勿使用helm） 解题 连接到指定节点\n下载calico tigera-operator.yaml 文件并创建（Flannel不支持NetworkPolicy,只能选Calico）\n1 2 3 wget https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/tigera-operator.yaml kubectl create -f tigera-operater.yaml # 必须用create，apply可能导致CRD冲突 查看 PodCIDR\n1 2 3 4 5 kubectl cluster-info dump | grep -i cluster-cidr # 假设输出为： \u0026#34;clusterCIDR\u0026#34;: \u0026#34;10.244.0.0/16\u0026#34; # 记住这个地址：10.244.0.0/16 下载 Calico 自定义资源配置 custom-resources.yaml\n1 2 3 4 wget https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/custom resources.yaml # 备注：这个网址考试没给，可以在tigera-operator.yaml这个下载地址基础上记个单词custom-resources.yaml 即可。 编辑 custom-resources.yaml 文件，修改 cidr 字段\n1 2 3 4 5 6 7 8 9 10 vim custom-resources.yaml spec: calicoNetwork: ipPools: - blockSize: 26 cidr: 10.244.0.0/16 # 按上步骤实际查询的pod网段写 encapsulation: VXLANCrossSubnet natOutgoing: Enabled nodeSelector: all() 创建 Calico 自定义资源\n1 kubectl create -f custom-resources.yaml 检查Calico组件运行状态\n1 kubectl -n calico-system get pod exit退出当前节点\n十四.argocd 十五.etcd修复 题目：\nkubeadm 配置的集群已迁移到新机器，他需要更改配置才能成功运行。\n修复在机器迁移过程中损坏的单节点集群\n首先，确定损坏的集群组件，并调查导致其损坏的原因 接下来，修复所有损坏的集群组件的配置。 最后，确保集群运行正常，确保每个节点和 Pod 都处于Ready状态\n注意：\n已停用的集群使用外部 etcd 服务器 确保重新启动所有必要的服务和组件，以使更改生效。否则可能导致分数降低。 解题 连接到指定节点\n修复kube-apiserver配置（etcd地址）\n1 2 3 4 5 6 # 编辑kube-apiserver的静态Pod清单文件 vim /etc/kubernetes/manifests/kube-apiserver.yaml # 找到--etcd-server参数 # 改为： --etcd-server=https://127.0.0.1:2379 #指向本地etcd服务 重启kubelet服务\n1 2 systemctl daemon-reload systemctl restart kubelet 检查节点和Pod状态（此时可能还是有问题）\n1 2 3 kubectl get nodes kubectl get pod -n kube-system 修复 kube-scheduler-master01 配置\n1 2 3 4 5 6 vim /etc/kubernetes/manifests/kube-scheduler.yaml # 找到resources 配置 resources: requests: cpu: 100m 等待kubelet自动重启kube-scheduler,一两分钟后再次检查集群状态\n1 2 3 kubectl get nodes kubectl get pod -n kube-system exit退出当前节点\n十六.cri-dockerd容器运行时 题目：\nContext：您的任务是为 Kubernetes 准备一个 Linux 系统。 Docker 已被安装，但您需要为 kubeadm 配置它。\n完成以下任务，为 Kubernetes 准备系统：\n设置 cri-dockerd ：\n1.安装 Debian 软件包 ~/cri-dockerd_0.3.6.3-0.ubuntu-jammy_amd64.deb Debian 软件包 使用 dpkg 安装。\n2.启用并启动 cri-docker 服务\n配置以下系统参数:\nnet.bridge.bridge-nf-call-iptables 设 置 为 1\nnet.ipv6.conf.all.forwarding 设 置 为 1\nnet.ipv4.ip_forward 设 置 为 1\nnet.netfilter.nf_conntrack_max 设置为 131072\n确保这些系统参数在系统重启后仍然存在，并应用于正在运行的系统。\n解题 连接到指定节点\n安装cri-dockerd并启动\n1 2 3 4 sudo dpkg -i ~/cri-dockerd_0.3.6.3-0ubuntu-jammy_amd64.deb sudo systemctl enable cri-docker sudo systemctl start cri-docker 配置系统参数\n1 2 3 4 5 6 7 8 9 sudo modprobe br_netfilter sudo vim /etc/sysctl.conf #添加以下内容到文件最后 net.bridge.bridge-nf-call-iptables = 1 net.ipv6.conf.all.forwarding = 1 net.ipv4.ip_forward = 1 net.netfilter.nf_conntrack_max = 131072 让配置生效\n1 sudo sysctl -p exit退出当前节点\n","date":"2025-07-29T10:28:19+08:00","permalink":"https://Logic0528.github.io/p/cka%E8%AE%A4%E8%AF%81/","title":"CKA认证"},{"content":"1.创建存储配置PV与PVC mysql-pvc.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi # 如果是AWS/GKE等云环境，可以指定对应的存储类 # storageClassName: gp2 mysql-pv.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: PersistentVolume metadata: name: mysql-pv # PV名称，需唯一 spec: capacity: storage: 10Gi # 存储容量，需与PVC请求一致 accessModes: - ReadWriteOnce # 访问模式，需与PVC匹配 persistentVolumeReclaimPolicy: Retain # 回收策略（Retain/Delete/Recycle） hostPath: path: /data/mysql # 节点上的实际存储路径（需提前创建） 2.创建配置文件configmap 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # mysql-config.yaml apiVersion: v1 kind: ConfigMap metadata: name: mysql-config data: my.cnf: | [mysqld] character-set-server=utf8mb4 collation-server=utf8mb4_unicode_ci default-storage-engine=InnoDB max_connections=500 [client] default-character-set=utf8mb4 3.创建secret存储密码 1 2 3 kubectl create secret generic mysql-secret \\ --from-literal=root-password=\u0026#34;your-root-password\u0026#34; \\ --from-literal=password=\u0026#34;your-user-password\u0026#34; 4.创建deployment部署MySQL实例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 # mysql-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: mysql spec: selector: matchLabels: app: mysql strategy: type: Recreate template: metadata: labels: app: mysql spec: containers: - name: mysql image: mysql:8.0 ports: - containerPort: 3306 name: mysql env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-secret key: root-password - name: MYSQL_DATABASE value: mydatabase - name: MYSQL_USER value: myuser - name: MYSQL_PASSWORD valueFrom: secretKeyRef: name: mysql-secret key: password volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql - name: config-volume mountPath: /etc/mysql/my.cnf subPath: my.cnf volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-pvc - name: config-volume configMap: name: mysql-config 5.创建service 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: Service metadata: name: mysql spec: ports: - port: 3306 selector: app: mysql clusterIP: None # 使用无头服务，如果需要直接访问Pod # 如果需要从集群外部访问，可以使用NodePort或LoadBalancer类型 # type: NodePort # ports: # - port: 3306 # nodePort: 30006 6.部署命令 1 2 3 4 kubectl apply -f mysql-pvc.yaml kubectl apply -f mysql-config.yaml kubectl apply -f mysql-deployment.yaml kubectl apply -f mysql-service.yaml 7.在本机上部署和在docker上部署和在k8s集群上部署有什么不同呢 在本机上部署无疑环境不隔离，缺乏扩展性，在docker上部署和在K8s集群上部署区别体现出k8s为何被称为容器编排管理器，k8s通过yaml配置文件定义deployment，service，pvc等资源，由k8s集群自动调度到节点，完全声明式管理。支持通过deployment的replicas字段一键扩容多实例，service负载均衡实现流量分发，扩展能力极强，pv实现数据持久化\n补充 MySQL作为有状态应用理应使用statefulset而不是deployment，扩展为多节点集群，主从复制MySQL cluster时必须使用statefulset，以确保数据一致性和服务稳定性，此处单节点测试\n","date":"2025-07-02T15:47:16+08:00","permalink":"https://Logic0528.github.io/p/%E5%9C%A8k8s%E9%9B%86%E7%BE%A4%E4%B8%8A%E9%83%A8%E7%BD%B2mysql/","title":"在k8s集群上部署mysql"},{"content":"Ingress 随着时间的积累原来不懂的东西现在逐渐看清了\nIngress：网关入口\n集群中Service的统一网管入口\n当Ingress安装完成并启动后，会为其创建一个Service，并设置一个NodePort，因为Ingress要处理所有的外部请求，故必须将Ingress暴露出去，此时访问集群人一台机器的指定端口就可以映射到Ingress上，然后通过在Ingress配置相应的规则，就可以实现对Service的统一网关入口\n","date":"2025-07-02T10:37:07+08:00","permalink":"https://Logic0528.github.io/p/ingress/","title":"Ingress"},{"content":"自己动手，丰衣足食，经过多次的搭建k8s集群现在可以说是基本明白了，记录一下过程不用每次都再搜索别人的教程了\n准备工作 1 2 3 4 5 6 7 8 9 10 11 12 #关闭防火墙 systemctl stop firewalld systemctl disable firewalld #关闭swap分区 swapoff -a 永久关闭需修改/etc/fstab sudo vim /etc/fstab #关闭selinux setenforce 0 sed -i \u0026#39;s/SELINUX=enforcing/SELINUX=disabled/g\u0026#39; /etc/selinux/config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # 添加配置文件并加载ipvs模块（kube-proxy使用ipvs模式） cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/ipvs.conf ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh nf_conntrack EOF # 执行命令加载模块 systemctl restart systemd-modules-load.service modprobe -- ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh nf_conntrack # 添加内核参数配置文件，参数在重新启动后保持不变 cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 EOF # 执行命令使参数生效 sysctl --system 安装前检查 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 执行命令检查selinux状态，输出应为SELinux status：disabled sestatus # 检查swap是否已经关闭，输出应为空 cat /proc/swaps # 检查ipvs模块是否已经加载 lsmod |grep ip_vs # 检查防火墙是否已经关闭 systemctl status firewalld # 检查内核参数是否已经设置，值应该为 1 cat /proc/sys/net/ipv4/ip_forward 安装docker,容器运行时 Ubuntu系统：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # step 1: 安装必要的一些系统工具 sudo apt-get update sudo apt-get install ca-certificates curl gnupg # step 2: 信任 Docker 的 GPG 公钥 sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg # Step 3: 写入软件源信息 echo \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://mirrors.aliyun.com/docker-ce/linux/ubuntu \\ \u0026#34;$(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;)\u0026#34; stable\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null # Step 4: 安装Docker sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin # 安装指定版本的Docker-CE: # Step 1: 查找Docker-CE的版本: # apt-cache madison docker-ce # docker-ce | 17.03.1~ce-0~ubuntu-xenial | https://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages # docker-ce | 17.03.0~ce-0~ubuntu-xenial | https://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages # Step 2: 安装指定版本的Docker-CE: (VERSION例如上面的17.03.1~ce-0~ubuntu-xenial) # sudo apt-get -y install docker-ce=[VERSION] centos系统：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # step 1: 安装必要的一些系统工具 sudo yum install -y yum-utils # Step 2: 添加软件源信息 yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # Step 3: 安装Docker sudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin # Step 4: 开启Docker服务 sudo service docker start # 注意： # 官方软件源默认启用了最新的软件，您可以通过编辑软件源的方式获取各个版本的软件包。例如官方并没有将测试版本的软件源置为可用，您可以通过以下方式开启。同理可以开启各种测试版本等。 # vim /etc/yum.repos.d/docker-ce.repo # 将[docker-ce-test]下方的enabled=0修改为enabled=1 # # 安装指定版本的Docker-CE: # Step 1: 查找Docker-CE的版本: # yum list docker-ce.x86_64 --showduplicates | sort -r # Loading mirror speeds from cached hostfile # Loaded plugins: branch, fastestmirror, langpacks # docker-ce.x86_64 17.03.1.ce-1.el7.centos docker-ce-stable # docker-ce.x86_64 17.03.1.ce-1.el7.centos @docker-ce-stable # docker-ce.x86_64 17.03.0.ce-1.el7.centos docker-ce-stable # Available Packages # Step2: 安装指定版本的Docker-CE: (VERSION例如上面的17.03.0.ce.1-1.el7.centos) # sudo yum -y install docker-ce-[VERSION] 接下来这一点是之前几次失败的原因\n1 2 # 生成默认配置 containerd config default | sudo tee /etc/containerd/config.toml 配置文件\n1 2 3 4 5 6 7 8 # 修改/etc/containerd/config.toml配置 disabled_plugins = [] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] SystemdCgroup = true # 这里改为true [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;] # 这里修改镜像地址 sandbox_image = \u0026#34;crpi-u3f530xi0zgtvmy0.cn-beijing.personal.cr.aliyuncs.com/image-infra/pause:3.10\u0026#34; 注意plugins.\u0026ldquo;io.containerd.grpc.v1.cri\u0026quot;里面有个systemd_cgroup=true/false,之前出错就是因为有这个，和下面的SystemdCgroup冲突了，有的话注释掉\n1 2 3 4 5 6 7 8 9 # 设置开机启动 systemctl enable containerd # 启动containerd systemctl start containerd # 检查状态 systemctl status containerd 安装kubelet,kubeadm,kubectl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 配置kubernetes 源 cat \u0026lt;\u0026lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/ enabled=1 gpgcheck=1 gpgkey=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/repodata/repomd.xml.key exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni EOF # 安装kubelet，kubeadm，kubectl yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes # 启动kubelet，启动完成查看kubelet的状态可能是异常的，不用惊慌 systemctl enable --now kubelet 集群初始化 1 2 3 4 5 sudo kubeadm init \\ --pod-network-cidr=10.244.0.0/16 \\ --apiserver-advertise-address=192.168.189.100 \\ --control-plane-endpoint=192.168.189.100 \\ --upload-certs 安装flannel网络插件 1 wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 补充一点，关于权限问题，在master节点上执行下面的命令，配置kubectl的权限\n1 2 3 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config ","date":"2025-06-30T14:59:31+08:00","permalink":"https://Logic0528.github.io/p/k8s%E9%9B%86%E7%BE%A4%E5%88%9D%E5%A7%8B%E5%8C%96/","title":"K8s集群初始化"},{"content":"笑死了他妈的四个实习生来的最早被锁在门外面，明天不来这么早了\n","date":"2025-06-30T14:59:09+08:00","permalink":"https://Logic0528.github.io/p/%E5%8C%97%E6%BC%82%E6%97%A5%E8%AE%B004/","title":"北漂日记04"},{"content":"实习第一天，报道领电脑，感觉还不错，没人给我上压力，领导还挺和气的中午带我去吃饭，下班时找我聊聊天问候两句，今天像上了一天自习，虽然没学太多东西只是把电脑装点软件但是感觉动力十足，晚上回来后把linux基础作业完成了，配置了桥接模式，需要和主机一个网段，下载了nginx，看了访问日志，主机需要防火墙放行80端口才能访问，现在已经九点半了，接下来洗个澡就十点了，但是今天还没有学新东西，看看阿里的acp什么的，如果去打英雄联盟呢好像一天没休息了也还行，但是会不会又是明日复明日明日何其多呢，感觉早睡也睡不着，今天一百八到手的还挺轻松，算了不打了，到点来局大乱斗睡觉吧，把明天的docker作业看一下，看会acp，一把大乱斗然后睡觉，嗯，完美\n","date":"2025-06-23T21:35:42+08:00","permalink":"https://Logic0528.github.io/p/%E5%8C%97%E6%BC%82%E6%97%A5%E8%AE%B003/","title":"北漂日记03"},{"content":"来到北京的第二天，上午从民宿起床后吃了饭往公寓搬东西入住，两天从看房到入住感觉有些太快了，处处谨慎的问东问西透露出我是个刚出社会的新兵蛋子，房主显然有些不耐烦了，下午坐地铁来的时候突然想听听“北京欢迎你”，也不知道北京欢不欢迎我\n坐地铁时有个空位，我和另一个大叔看上了这个座位但双方都很谦让，他说你做吧，本来我是想坐的毕竟我行李一大堆，但是我看到大叔地中海的发型快要顶不住秃掉觉得有些可怜就说你做吧我还有几站就下车了，哈哈\n来北京两天我已经有好几个兄弟了，他们一半是房产中介，一半是销售\n","date":"2025-06-20T18:27:47+08:00","permalink":"https://Logic0528.github.io/p/%E5%8C%97%E6%BC%82%E6%97%A5%E8%AE%B002/","title":"北漂日记02"},{"content":"“最后一次来你家店吃饭了，离开北京了”\n这个男人快步走进这家面馆，人还没到柜台时我听到了这句话，今天是我第一次来北京。\n他应该是这家店的常客了，老板停顿了一下\n“去哪？”\n“回老家，西安” 深呼吸，吐气\n几秒的沉默，吃面的我也放慢了速度，我意识到这或许是第一段难忘的记忆\n“……” “回去干啥？”\n“一个月三四千，服务员什么的……”\n“还行，在那边三四千比这边六千都舒服，吃点啥”\n“来份小炒肉盖饭吧”\n“再来瓶啤酒”\n每天都有人疲惫地离开，每天都有人迷茫地涌进来，一座城市像一个巨大的有机生物体，细胞们通过一条条血管般的轨道运输到各处，这个生物体里有最令人作呕的腐烂，也有最生机勃勃的希望\n今天坐出租车时在下雨，雨刷器规律的摆动着，有雨水时就刷刷两下，没有雨时就安静的躺在那里，我想如果此时雨刷器在玻璃上跳起拉丁舞，那些一切杂乱的动作组合排列在一起，或许就是生命的意义\n我在心中暗自对自己说，等我离开北京那一天，我的最后一餐要是这家店的小炒肉盖饭，或许可以再来瓶啤酒\n","date":"2025-06-19T22:16:51+08:00","permalink":"https://Logic0528.github.io/p/%E5%8C%97%E6%BC%82%E6%97%A5%E8%AE%B001/","title":"北漂日记01"},{"content":"五月份进行了两次面试，面试不同于闭门造车,问的许多问题需要进行规范的回答类似于八股文之类的，虽然我很不喜欢八股文代表的那种死板的精神，但不得不承认在对于基础知识的掌握有许多不足，在面试过程中很多概念熟悉了解但是语言组织不够流畅，对面试中没能回答上来或者回答不流畅或者以后面试可能会遇到的额问题进行整理。 屡战屡败，屡败屡战，面试中不会的问题记下来解决也是一种收获\n1.k8s架构和基本组件 k8s是一个开源的容器编排平台，用于自动化部署，扩展和管理容器化应用。其架构采用主从架构（master-node），核心组件分为控制平面control plane和工作节点worker node。\n控制平面组件：\nkube-apiserver:API网关和集群控制中心，处理所有管理请求如创建/删除pod，部署应用等 etcd: 分布式键值存储，保存集群配置,如pod,service,namespace\nkube-scheduler: 负责pod调度到合适的节点\nkube-controller-manager: 运行控制器进程，包括node controller,deployment controller,service controller\n节点组件：\nkubelet: 管理节点上的pod\nkube-proxy: 实现网络规则和负载均衡，维护节点上的网络配置\ncontainer runtime: 负责运行容器（如docker，containerd）\n附加组件： coreDNS：集群DNS服务，提供域名解析功能，将服务名解析为IP地址。 IngressContorller：负责外部流量访问，支持负载均衡和路由规则。HTTP路由 NetworkPluginL: calico，flannel等\n2.pod,deployment,service pod： 是k8s中最小的调度单位，封装一个或多个容器，共享网络和存储\ndeployment： 基于ReplicaSet的更高层抽象，声明式管理Pod的期望状态，如副本数，镜像版本，更新策略 作用： 确保pod副本数始终等于期望值，自动重启/创建失败的pod，支持滚动更新，回滚版本\nservice：抽象的网络层，为一组pod提供固定访问入口 ip+端口 作用： 内部服务发现：同意集群内其他pod可通过service域名访问 外部暴露： 通过nodeport等类型将服务暴露到集群外 负载均衡： 将流量分发到后端多个pod副本\ndeployment是管理pod的控制器，service是抽象网络层，三者通过标签匹配。\n3.nginx与负载均衡 nginx是一款高性能的web服务器，反向代理服务器，负载均衡器 web服务器： 处理静态资源的请求，直接返回文件内容，性能远超传统服务器\n反向代理服务器： 将客户端请求转发到后端多个应用服务器如tomcat，隐藏真实服务器地址\n负载均衡器： 分发客户端请求到多个后端服务器，避免单点过载\n负载均衡： 将网络流量均发到多个计算资源如服务器，容器，避免单一节点过载，提高系统的可用性，性能和容错能力\nnginx负载均衡方式： 轮询；默认策略，按顺序分发请求 weight: 根据权重分配 ip_hash： 基于客户端IP哈希 等\u0026hellip;.深入了解后补充\nnginx反向代理的作用与好处：\n作用：将用户请求转发到后端服务器，隐藏后端服务器 好处： 1.节省公网IP，域名是解析到反向代理服务器，后端服务器不需要配置ip 2.提高安全性，反向代理对用户不可见，降低直接攻击后端服务器的风险 3.统一访问入口，反向代理可以代理多个服务器，也就是多个项目，通过域名，将不同的项目转化到不同的后台服务器 4.提高访问速度，反向代理服务器是在后端的前面，所以他可以将一些静态内容缓存到本地，下一次再有请求的时候直接从本地读取，不会像后端发起请求，加快了响应速度\n4.k8s中如何将一个服务暴露到外部，有什么区别 1.nodeport: 在节点上开放一个固定端口，外部流量通过任一节点的该端口转发到该服务，适用于测试环境，内部系统临时暴露 缺点：端口管理复杂，需要避开暴露的端口，仅支持4层负载均衡，安全性差 2.ingress： 基于ingress controller实现7层负载均衡 作用： 提供基于域名的路由规则，实现七层负载均衡，支持路径规则，TLS 加密，基于域名的虚拟主机等功能\n区别： 1.层级不同：nodeport在集群每个节点上暴露端口，ingress在集群外部 2.负载均衡方式：nodeport基于IPVS实现四层负载均衡，ingress基于Nginx实现七层负载均衡 3.功能复杂性：nodeport仅支持基本的负载均衡，ingress支持更丰富的功能，如路由规则，TLS 加密，基于域名的虚拟主机等\n5.dockerfile常用变量，基于nginx镜像做个性化配置 1 2 3 4 5 6 7 8 9 10 11 12 FROM 镜像 USER 设置运行后续命令的用户 WORKDIR 设置工作目录 ENV 设置环境变量 RUN 执行命令，安装软件或配置环境 COPY 将本地文件复制到镜像中 EXPOSE 暴露端口 ENV 定义环境变量，可在后续命令中使用 VOLUME 定义匿名卷，用于数据持久化 CMD 设置容器启动时执行的命令 ENDPOINT 配置容器启动时执行的命令 基于nginx镜像做个性化设置： 在同目录下创建nginx.conf 修改nginx配置，docker build -t mynginx:v1\n6.linux常用命令 1 2 3 4 5 6 7 8 9 10 11 12 13 14 whoami 输出当前用户名 id echo $USER uptime 查看系统负载和运行时间 free 查看内存使用情况 df -h 查看磁盘空间使用情况 ps aux 查看进程信息 netstat -tlnp 查看网络连接情况 lsblk 查看磁盘总容量及使用情况 dd 测试读写速率 iostat 磁盘整体读写速率 lscpu 查看有多少个cpu top 实时监控内存及进程占用 ","date":"2025-06-07T14:51:28+08:00","permalink":"https://Logic0528.github.io/p/%E9%9D%A2%E8%AF%95/","title":"面试"},{"content":"项目目录结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 k8s-deploy-platform/ ├── helm-charts/ # Helm Chart模板 │ └── app-template/ │ ├── Chart.yaml # Chart基本信息 │ ├── values.yaml # 默认配置值 │ └── templates/ # K8s资源模板 │ ├── deployment.yaml # 部署配置 │ ├── service.yaml # 服务配置 │ └── ingress.yaml # 入口配置 │ ├── manifests/ # K8s资源清单 │ ├── applications/ # 应用相关配置 │ │ └── app-template.yaml │ ├── logging/ # 日志系统配置 │ │ └── efk-values.yaml │ └── monitoring/ # 监控系统配置 │ ├── prometheus-rules.yaml # 监控规则 │ ├── prometheus-values.yaml # Prometheus配置 │ ├── pv.yaml # 持久化卷 │ └── storageclass.yaml # 存储类 │ └── scripts/ # 自动化脚本 ├── install.sh # 安装脚本 └── deploy.sh # 部署脚本 helm-charts/app-template Chart.yaml 1 2 3 4 5 apiVersion: v2 name: app-template description: 一个基础的应用部署模板 version: 0.1.0 type: application values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # 应用配置 name: myapp replicas: 2 image: nginx:latest # 资源配置 resources: requests: memory: \u0026#34;64Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; # 健康检查配置 healthCheck: enabled: true path: /health port: 8080 initialDelay: 30 period: 10 # 服务配置 service: type: ClusterIP port: 80 targetPort: 8080 # Ingress配置 ingress: enabled: false className: nginx host: myapp.example.com path: / pathType: Prefix value.yaml中service类型为ClusterIP,clusterip仅在集群内部可以访问，与之对应的是nodeport，nodeport在所有节点上开放指定端口（30000-32767）\nclusterIP让外部访问的方式：\n可以通过配置ingress\n1 2 3 4 5 graph LR A[外部用户] --\u0026gt; B[Ingress] B --\u0026gt; C[ClusterIP Service] C --\u0026gt; D[Pod1] C --\u0026gt; E[Pod2] 也可以使用这条命令临时端口转发\n1 kubectl port-forward -n monitoring svc/monitoring-grafana 3000:80 1 2 3 4 graph LR A[外部用户] --\u0026gt; B[临时端口转发] B --\u0026gt; C[ClusterIP Service] C --\u0026gt; D[Pod1] templates/deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion: apps/v1 kind: Deployment metadata: name: {{ .Values.name }} spec: replicas: {{ .Values.replicas }} selector: matchLabels: app: {{ .Values.name }} template: metadata: labels: app: {{ .Values.name }} spec: containers: - name: {{ .Values.name }} image: {{ .Values.image }} resources: {{ toYaml .Values.resources | nindent 12 }} {{- if .Values.healthCheck.enabled }} livenessProbe: httpGet: path: {{ .Values.healthCheck.path }} port: {{ .Values.healthCheck.port }} initialDelaySeconds: {{ .Values.healthCheck.initialDelay }} periodSeconds: {{ .Values.healthCheck.period }} {{- end }} deployment.yaml这样配置可以根据values.yaml中的配置动态生成deployment资源，相当于values.yaml定义变量，方便修改\ntemplates/service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: Service metadata: name: {{ .Values.name }} spec: type: {{ .Values.service.type }} ports: - port: {{ .Values.service.port }} targetPort: {{ .Values.service.targetPort }} protocol: TCP selector: app: {{ .Values.name }} templates/ingress.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 {{- if .Values.ingress.enabled }} apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: {{ .Values.name }} spec: ingressClassName: {{ .Values.ingress.className }} rules: - host: {{ .Values.ingress.host }} http: paths: - path: {{ .Values.ingress.path }} pathType: {{ .Values.ingress.pathType }} backend: service: name: {{ .Values.name }} port: number: {{ .Values.service.port }} {{- end }} manifests applications/app-template.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion: apps/v1 kind: Deployment metadata: name: {{ .Values.name }} spec: replicas: {{ .Values.replicas }} selector: matchLabels: app: {{ .Values.name }} template: metadata: labels: app: {{ .Values.name }} spec: containers: - name: {{ .Values.name }} image: {{ .Values.image }} resources: {{ toYaml .Values.resources | nindent 12 }} {{- if .Values.healthCheck.enabled }} livenessProbe: httpGet: path: {{ .Values.healthCheck.path }} port: {{ .Values.healthCheck.port }} initialDelaySeconds: {{ .Values.healthCheck.initialDelay }} periodSeconds: {{ .Values.healthCheck.period }} {{- end }} logging/efk-values.yaml 暂时还不会\nmonitoring/prometheus-rules.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: app-alerts namespace: monitoring spec: groups: - name: app rules: - alert: HighCPUUsage expr: container_cpu_usage_seconds_total \u0026gt; 80 for: 5m labels: severity: warning annotations: description: \u0026#34;容器 CPU 使用率超过 80%\u0026#34; - alert: HighMemoryUsage expr: container_memory_usage_bytes \u0026gt; 80 for: 5m labels: severity: warning annotations: description: \u0026#34;容器内存使用率超过 80%\u0026#34; monitoring/prometheus-values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 prometheus: enabled: true serverFiles: prometheus.yml: global: scrape_interval: 15s scrape_configs: - job_name: \u0026#39;kubernetes-pods\u0026#39; kubernetes_sd_configs: - role: pod - job_name: \u0026#39;kubernetes-nodes\u0026#39; kubernetes_sd_configs: - role: node persistence: enabled: true size: 10Gi grafana: enabled: true adminPassword: admin persistence: enabled: true size: 5Gi type: pvc storageClassName: local-storage grafana实现持久化存储时，需要配置storageClassName，这个storageClassName需要在k8s集群中已经存在，这里使用的是local-storage\nmonitoring/storageclass.yaml 1 2 3 4 5 6 7 class.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: local-storage provisioner: kubernetes.io/no-provisioner volumeBindingMode: Immediate monitoring/pv.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: v1 kind: PersistentVolume metadata: name: grafana-pv spec: capacity: storage: 5Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: local-storage local: path: /mnt/data nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: #- master # 替换为你的节点名称 - work-node1 注意：\n这里的path需要创建好，否则会报错 nodeAffinity是为了保证grafana的数据不会被其他节点的pod所使用，就一个工作节点所以没写，master节点上一般会有污点，最好调度到工作节点上 scripts install.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # filepath: scripts/setup-cluster.sh #!/bin/bash # 启动本地集群 start_cluster() { echo \u0026#34;Starting minikube cluster...\u0026#34; minikube start --driver=docker \\ --memory=4096 \\ --cpus=2 \\ --kubernetes-version=v1.24.0 } # 安装必要的组件 install_components() { # 添加 Helm 仓库 helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo add elastic https://helm.elastic.co helm repo update } # 创建必要的命名空间 create_namespaces() { kubectl create namespace monitoring kubectl create namespace logging kubectl create namespace applications } main() { start_cluster install_components create_namespaces } main \u0026#34;$@\u0026#34; deploy.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 #!/bin/bash # 设置变量 APP_NAME=$1 IMAGE=$2 NAMESPACE=${3:-applications} REPLICAS=${4:-1} # 验证输入 if [ -z \u0026#34;$APP_NAME\u0026#34; ] || [ -z \u0026#34;$IMAGE\u0026#34; ]; then echo \u0026#34;Usage: $0 \u0026lt;app-name\u0026gt; \u0026lt;image\u0026gt; [namespace] [replicas]\u0026#34; exit 1 fi # 部署应用 deploy_application() { echo \u0026#34;Deploying $APP_NAME...\u0026#34; helm upgrade --install $APP_NAME ./helm-charts/app-template \\ --namespace $NAMESPACE \\ --set name=$APP_NAME \\ --set image=$IMAGE \\ --set replicas=$REPLICAS } main() { deploy_application } main \u0026#34;$@\u0026#34; 项目暂时完成，grafana全是英文暂时不熟练，后续再完善\n","date":"2025-05-20T10:38:28+08:00","permalink":"https://Logic0528.github.io/p/%E5%9F%BA%E4%BA%8Ek8s%E7%9A%84%E5%BA%94%E7%94%A8%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2%E5%B9%B3%E5%8F%B0/","title":"基于K8s的应用自动化部署平台"},{"content":"污点（Taint） effect: NoSchedule：不允许调度，已经存在的Pod不受影响\nPreferNoSchedule：尽量不调度，已经存在的Pod不受影响\nNoExecute：不允许调度，并且驱逐已经存在的Pod\n容忍（Toleration） operator: Equal：需要key和value完全匹配\nExists：只需要key匹配，不需要value匹配\nvalue: 污点的值\n示例代码：\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: Pod metadata: name: pod-with-tolerations spec: tolerations: - key: \u0026#34;key1\u0026#34; # 污点的键 operator: \u0026#34;Equal\u0026#34; # 操作符：Equal或Exists value: \u0026#34;value1\u0026#34; # 污点的值 effect: \u0026#34;NoSchedule\u0026#34; # 效果 tolerationSeconds: 3600 # 容忍时间（仅用于NoExecute） 亲和力（Affinity） NodeAffinity 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: v1 kind: Pod metadata: name: with-node-affinity spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: # 硬性要求 nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux preferredDuringSchedulingIgnoredDuringExecution: # 软性偏好 - weight: 1 preference: matchExpressions: - key: disk-type operator: In values: - ssd PodAffinity 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: Pod metadata: name: with-pod-affinity spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - web-store topologyKey: kubernetes.io/hostname PodAntiAffinity(节点反亲和力) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: v1 kind: Pod metadata: name: with-pod-antiaffinity spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - cache topologyKey: kubernetes.io/hostname 没什么好说的，类似于标签，慢工出细活~\n","date":"2025-05-15T15:52:46+08:00","permalink":"https://Logic0528.github.io/p/k8s-%E6%B1%A1%E7%82%B9%E4%B8%8E%E5%AE%B9%E5%BF%8D/","title":"K8s-污点与容忍"},{"content":"PV和PVC PV和PVC是Kubernetes中用于管理持久化存储的两个重要概念。\nPV（Persistent Volume） 是集群中的存储资源 由管理员事先创建和配置 独立与pod的生命周期 可以对接各种存储后端，如nfs、ceph、云存储等 定义了存储的容量、访问模式、存储类等信息 PVC（Persistent Volume Claim） 是pod对存储资源的请求 由用户/开发者创建 类似于pod消耗Node资源，PVC消耗PV资源 声明需要的存储容量、访问模式、存储类等信息 不需要关心存储的具体实现细节 工作流程 管理员创建PV，定义存储资源的属性 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: v1 kind: PersistentVolume #描述资源类型为PV类型 metadata: name: pv0001 spec: capacity: #容量配置 storage: 5Gi volumeMode: Filesystem #存储类型为文件系统 accessModes: #访问模式：ReadWriteOnce WeadWriteMany ReadOnlyMany - ReadWriteMany #可被单节点读写 persistentVolumeReclaimPolicy: Retain #回收策略 storageClassName: slow #创建PV的存储类名，需要与pvc的相同 mountOptions: #加载配置 - hard - nfsvers=4.1 nfs: #连接到nfs path: /home/nfs/rw/pv-test #存储路径 server: 192.168.189.130 #nfs服务地址 用户/开发者创建PVC，声明需要的存储资源 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: PersistentVolumeClaim #资源类型为pvc metadata: name: nfs-pvc spec: accessModes: - ReadWriteMany volumeMode: Filesystem resources: requests: storage: 5Gi storageClassName: slow #名字要与对应的pv相同 # selector: #使用选择器选择对应的pv Kubernetes调度器根据PVC的要求，选择合适的PV进行绑定 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: v1 kind: Pod metadata: name: pvc-test-pod spec: containers: - image: nginx name: nginx-volume volumeMounts: - mountPath: /usr/share/nginx/html #挂载到容器的哪个目录 name: test-volume #挂载哪个volume volumes: - name: test-volume persistentVolumeClaim: claimName: nfs-pvc tolerations: - key: \u0026#34;node-role.kubernetes.io/control-plane\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; 动态制备PV 被污点给整破防了，暂时先不写了。（后半句由ai补充），天哪你连我心里想的什么都知道快点拯救全人类吧，待我学完污点归来\n目前遗留问题，-n kube-system po nfs-server-provisioner ImagePullBackOff,kube-apiserver pending状态，selfLink怎么处理，其他的部分理解深入许多\n好的，了解到主节点上默认会有污点，目的是为了保护主节点资源，防止普通pod调\n动态制备pv 创建storageclass 定义存储类型 指定Provisioner 设置存储参数 1 2 3 4 5 6 7 8 9 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: managed-nfs-storage provisioner: fuseim.pri/ifs #外部制备器提供者，编写为提供者的名称 parameters: archiveOnDelete: \u0026#34;false\u0026#34; #是否存档 reclaimPolicy: Retain #回收策略 volumeBindingMode: Immediate #默认为Immediate，表示创建PVC立即进行绑定 用户创建pvc 指定存储大小 指定访问模式 引用storageclass 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 cat nfs-sc-statefulset.yaml --- apiVersion: v1 kind: Service metadata: name: nginx-sc labels: app: nginx-sc spec: type: NodePort ports: - name: web port: 80 protocol: TCP selector: app: nginx-sc --- apiVersion: apps/v1 kind: StatefulSet metadata: name: nginx-sc spec: replicas: 1 #serverName: \u0026#34;nginx-sc\u0026#34; selector: matchLabels: app: nginx-sc template: metadata: labels: app: nginx-sc spec: containers: - image: nginx name: nginx-sc imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /usr/share/nginx/html name: nginx-sc-test-pvc volumeClaimTemplates: - metadata: name: nginx-sc-test-pvc spec: storageClassName: managed-nfs-storage accessModes: - ReadWriteMany resources: requests: storage: 1Gi provisioner创建pv 检测到新建pvc 根据storageclass找到对应的provisioner provisoner创建pv 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 cat nfs-provisioner.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nfs-client-provisioner namespace: kube-system labels: app: nfs-client-provisioner spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:v4.0.0 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: 192.168.189.130 - name: NFS_PATH value: /home/data/nfs/rw volumes: - name: nfs-client-root nfs: server: 192.168.189.130 path: /home/data/nfs/rw 基本过程如上，上次出现污点问题是因为nfs服务器地址写为master节点导致pod部署到master节点，有污点\n另补充一点，provisioner需要特定权限才能创建和管理pv，需要用到rbac.yaml\n","date":"2025-05-11T14:49:19+08:00","permalink":"https://Logic0528.github.io/p/k8s-%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8-pv%E5%92%8Cpvc/","title":"K8s-持久化存储-PV和PVC"},{"content":"超级帅哥悄悄路过\n","date":"2025-05-08T23:05:44+08:00","permalink":"https://Logic0528.github.io/p/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/","title":"第一篇博客"}]