[{"content":"一、k8s组件篇 Master节点:\nkube-apiserver (API服务器)\netcd (分布式存储)\nkube-scheduler (调度器)\nkube-controller-manager (控制器)\nWorker节点：\nkubelet (节点代理)\nkube-proxy (网络代理)\nContainer Runtime (容器运行时)\n1、kube-apiserver kube-apiserver的核心作用，为什么说它是k8s的中枢\nkube-apiserver是k8s所有组件的统一接口，负责接收，验证和处理所有客户端请求（如kubectl命令，其他组件通信），并将集群状态持久化到etcd\n之所以称为中枢，是因为所有组件均通过apiserver交互，而非直接通信，它是集群数据的唯一入口\nkube-apiserver如何处理客户端请求（如kubectl命令）？是否支持高可用部署？如何实现 多实例 + 负载均衡？\n接收请求\u0026ndash;\u0026gt;依次经过认证(Authentication,验证请求者身份，token，证书)\u0026ndash;\u0026gt;授权(Authorization,验证请求者权限，如RBAC规则)\u0026ndash;\u0026gt;准入控制(Admission Control,修改或拒绝请求，如资源限制检查)\u0026ndash;\u0026gt;验证通过后，将数据写入etcd,并返回结果\n支持高可用部署：通过多实例 + 负载均衡实现（如HAProxy），多个apiserver实例共享etcd集群，负载均衡器分发请求，避免单点故障\nkube-apiserver如何保证API调用的安全性？\n认证：支持客户端证书，用户名密码等方式，验证请求者身份\n授权：通过RBAC（基于角色的访问控制）等机制，检查请求者是否有权限执行操作；\nAPI Server的API版本有哪些？不同版本的区别是什么？如何应对高并发请求？\nAPI版本：分为alpha(测试版，可能删除)，beta(测试版，功能基本稳定)，stable(稳定版，如v1,apps/v1)，不同版本对应资源的功能成熟度。\n高并发优化：通过缓存如etcd缓存，减少重复查询，支持请求限流防止过载，可水平扩展apiserver实例分担压力\n2、etcd etcd在k8s中存储什么数据？为什么选择etcd作为存储后端？\n存储k8s集群的所有核心机制，包括Pod,Service,Deployment等资源的定义和状态，以及集群配置（如RBAC规则）\n选择etcd原因：分布式一致性，基于Raft协议，保持数据多副本一致，强一致性读写，高可用（集群部署），支持事务和监听机制（适合实时同步集群状态）\netcd是单节点还是分布式的？如何部署etcd集群保证高可用？\netcd是分布式存储，需部署集群\nRaft协议作用：通过领导者选举，保证集群有一个主节点Leader处理写请求，从节点同步数据，若Leader故障，重新选举新Leader，确保数据不丢失且一致\n如何备份和恢复etcd数据？数据增长过快怎么办？\n备份：使用etcdctl snapshot save \u0026lt;备份文件\u0026gt;需指定etcd地址和证书 恢复：etcdctl snapshot restore \u0026lt;备份文件\u0026gt;需指定新的集群名称和数据目录。 数据增长过快：启用自动压缩，定期处理过期数据，使用SSD提升读写性能 etcd的读写性能对k8s集群有什么影响？如何优化？\netcd性能直接影响集群响应速度，如Pod创建延迟\n优化方式：存储介质使用SSD，集群规模控制在3-5节点，减少Raft协议开销\n3、kube-scheduler kube-scheduler的核心作用是什么？它要解决的核心问题是什么？ 核心作用：为新创建的Pod选择最合适的Node节点 解决的核心问题：在满足Pod资源需求和调度策略的前提下，优化集群资源使用率 详细描述Pod的调度过程，过滤阶段和打分阶段分别做什么。 过滤：从所有Node中排除不满足Pod要求的节点，输出可行节点列表 打分：对可行节点按优先级评分，得分最高的节点被选中 绑定：将Pod与选中的Node绑定，通过apiserver更新Pod的spec.nodeName字段 调度策略 节点亲和性nodeAffinity：Pod通过规则指定偏好或必须部署的Node Pod亲和性podAffinity：Pod偏好与特定Pod部署在同一拓扑域（如数据库Pod在同一机房） 污点taint与容忍toleration：node通过污点排斥Pod，Pod通过容忍突破排斥 4、kube-controller-manager kube-controller-manager作用是什么？它包含哪些核心控制器？ 作用是通过控制器监测集群状态，确保实际状态与期望状态一致 核心控制器包括DeploymentController,ReplicaSetController,NodeController,JobController,ServiceController 控制器的调谐循环是如何工作的？ 监测：通过apiserver监听目标资源的状态变化，如Deployment的副本数 对比：将实际状态与期望状态对比 调谐：执行操作使实际状态接近期望状态 常见控制器的作用 DeploymentController: 管理Replicaset,通过控制ReplicaSet的数量和版本，保证Deployment的期望副本数和滚动更新 NodeController: 监测Node健康状态，若Node失联 5、kubelet kubelet核心作用是什么？它与Master节点的哪个组件直接交互 核心作用是在Node上运行Pod，并保证Pod按期望状态运行，处理Master下发到本节点的任务 直接与kube-apiserver交互，定期从apiserver获取Node上的Pod定义，上报Pod和Node状态 kubelet如何保证Pod按照期望状态运行？ 监听：通过apiserver的Watch机制，实时获取分配到当前Node的pod信息 创建：根据Pod定义，通过容器运行时创建容器 监控：持续检查Pod和容器状态，若状态异常，尝试重启 上报：将Pod状态通过apiserver写入etcd kubelet如何执行Pod的存活探针和就绪探针？探针失败会触发什么操作？ 存活探针livenessprobe: 检测容器是否存活，失败则触发容器重启 就绪探针readnessprobe: 检测容器是否就绪，失败则将Pod从Service的端点列表中移除 kubelet定期执行探针，配置periodseconds,根据结果执行对应操作 什么是静态Pod?kubelet如何管理静态Pod? 静态Pod是由kubelet直接管理的Pod,不通过apiserver,配置文件通常放在/etc/kubernetes/manifests kubelet定期扫描目录，若配置文件更新，自动重建Pod,静态Pod在apiserver中可见，但无法通过kubectl删除 用途：部署Master组件，如apiserver,etcd，确保组件随Node启动而启动 7、Container Runnertime 二、简述题 1.简述Kubernetes创建一个Pod的主要流程 Kubernetes中创建一个Pod涉及多个组件之间联动，主要流程如下：\n客户端提交Pod的配置信息（可以是yaml文件定义的信息）到kube-apiserver。 Apiserver收到指令后，通知给controller-manager创建一个资源对象。 Controller-manager通过api-server将Pod的配置信息存储到etcd数据中心中。 Kube-scheduler检测到Pod信息会开始调度预选，会先过滤掉不符合Pod资源配置要求的节点，然后开始调度调优，主要是挑选出更适合运行Pod的节点，然后将Pod的资源配置单发送到Node节点上的kubelet组件上。 Kubelet根据scheduler发来的资源配置单运行Pod，运行成功后，将Pod的运行信息返回给scheduler，scheduler将返回的Pod运行状况的信息存储到etcd数据中心。 2.简述Kubernetes自动扩容机制 Kubernetes使用Horizontal Pod Autoscaler（HPA）的控制器实现基于CPU使用率进行自动Pod扩缩容的功能。HPA控制器周期性地监测目标Pod的资源性能指标，并与HPA资源对象中的扩缩容条件进行对比，在满足条件时对Pod副本数量进行调整。\nKubernetes中的某个Metrics Server（Heapster或自定义Metrics Server）持续采集所有Pod副本的指标数据。HPA控制器通过Metrics Server的API（Heapster的API或聚合API）获取这些数据，基于用户定义的扩缩容规则进行计算，得到目标Pod副本数量。\n当目标Pod副本数量与当前副本数量不同时，HPA控制器就向Pod的副本控制器（Deployment、RC或ReplicaSet）发起scale操作，调整Pod的副本数量，完成扩缩容操作\n3.简述Kubernetes RC的机制 Replication Controller用来管理Pod的副本，保证集群中存在指定数量的Pod副本。当定义了RC并提交至Kubernetes集群中之后，Master节点上的Controller Manager组件获悉，并同时巡检系统中当前存活的目标Pod，并确保目标Pod实例的数量刚好等于此RC的期望值，若存在过多的Pod副本在运行，系统会停止一些Pod，反之则自动创建一些Pod\n4.简述Kubernetes Replica Set和Replication Controller之间有什么区别 Replica Set和Replication Controller类似，都是确保在任何给定时间运行指定数量的Pod副本。不同之处在于RS使用基于集合的选择器，而Replication Controller使用基于权限的选择器。\n5.简述Kubernetes中什么是静态Pod 静态Pod是由kubelet进行管理的仅存在于特定Node的Pod上，他们不能通过API Server进行管理，无法与ReplicationController、Deployment或者DaemonSet进行关联，并且kubelet无法对他们进行健康检查。静态Pod总是由kubelet进行创建，并且总是在kubelet所在的Node上运行。\n6.简述Kubernetes中Pod的健康检查方式 对Pod的健康检查可以通过两类探针来检查：LivenessProbe和ReadinessProbe。\nLivenessProbe探针：用于判断容器是否存活（running状态），如果LivenessProbe探针探测到容器不健康，则kubelet将杀掉该容器，并根据容器的重启策略做相应处理。若一个容器不包含LivenessProbe探针，kubelet认为该容器的LivenessProbe探针返回值用于是“Success”。 ReadineeProbe探针：用于判断容器是否启动完成（ready状态）。如果ReadinessProbe探针探测到失败，则Pod的状态将被修改。Endpoint Controller将从Service的Endpoint中删除包含该容器所在Pod的Eenpoint。 startupProbe探针：启动检查机制，应用一些启动缓慢的业务，避免业务长时间启动而被上面两类探针kill掉 7.简述Kubernetes各模块如何与API Server通信 Kubernetes API Server作为集群的核心，负责集群各功能模块之间的通信。集群内的各个功能模块通过API Server将信息存入etcd，当需要获取和操作这些数据时，则通过API Server提供的REST接口（用GET、LIST或WATCH方法）来实现，从而实现各模块之间的信息交互。\n如kubelet进程与API Server的交互：每个Node上的kubelet每隔一个时间周期，就会调用一次API Server的REST接口报告自身状态，API Server在接收到这些信息后，会将节点状态信息更新到etcd中。\n如kube-controller-manager进程与API Server的交互：kube-controller-manager中的Node Controller模块通过API Server提供的Watch接口实时监控Node的信息，并做相应处理。\n如kube-scheduler进程与API Server的交互：Scheduler通过API Server的Watch接口监听到新建Pod副本的信息后，会检索所有符合该Pod要求的Node列表，开始执行Pod调度逻辑，在调度成功后将Pod绑定到目标节点上。\n简述Kubernetes kubelet监控Worker节点资源是使用什么组件来实现的\nkubelet使用cAdvisor对worker节点资源进行监控。在Kubernetes系统中，cAdvisor已被默认集成到kubelet组件内，当kubelet服务启动时，它会自动启动cAdvisor服务，然后cAdvisor会实时采集所在节点的性能指标及在节点上运行的容器的性能指标。\n三、网络篇 1.简述Kubernetes网络模型 Kubernetes网络模型中每个Pod都拥有一个独立的IP地址，并假定所有Pod都在一个可以直接连通的、扁平的网络空间中。所以不管它们是否运行在同一个Node（宿主机）中，都要求它们可以直接通过对方的IP进行访问。设计这个原则的原因是，用户不需要额外考虑如何建立Pod之间的连接，也不需要考虑如何将容器端口映射到主机端口等问题。\n同时为每个Pod都设置一个IP地址的模型使得同一个Pod内的不同容器会共享同一个网络命名空间，也就是同一个Linux网络协议栈。这就意味着同一个Pod内的容器可以通过localhost来连接对方的端口。\n在Kubernetes的集群里，IP是以Pod为单位进行分配的。一个Pod内部的所有容器共享一个网络堆栈（相当于一个网络命名空间，它们的IP地址、网络设备、配置等都是共享的）。\n2.简述Kubernetes CNI模型 CNI提供了一种应用容器的插件化网络解决方案，定义对容器网络进行操作和配置的规范，通过插件的形式对CNI接口进行实现。CNI仅关注在创建容器时分配网络资源，和在销毁容器时删除网络资源。在CNI模型中只涉及两个概念：容器和网络。\n容器（Container）：是拥有独立Linux网络命名空间的环境，例如使用Docker或rkt创建的容器。容器需要拥有自己的Linux网络命名空间，这是加入网络的必要条件。 网络（Network）：表示可以互连的一组实体，这些实体拥有各自独立、唯一的IP地址，可以是容器、物理机或者其他网络设备（比如路由器）等。 对容器网络的设置和操作都通过插件（Plugin）进行具体实现，CNI插件包括两种类型：CNI Plugin和IPAM（IP Address Management）Plugin。CNI Plugin负责为容器配置网络资源，IPAM Plugin负责对容器的IP地址进行分配和管理。IPAM Plugin作为CNI Plugin的一部分，与CNI Plugin协同工作。\n3.简述Kubernetes网络策略 为实现细粒度的容器间网络访问隔离策略，Kubernetes引入Network Policy。\nNetwork Policy的主要功能是对Pod间的网络通信进行限制和准入控制，设置允许访问或禁止访问的客户端Pod列表。Network Policy定义网络策略，配合策略控制器（Policy Controller）进行策略的实现。\n4.简述Kubernetes网络策略原理 Network Policy的工作原理主要为：policy controller需要实现一个API Listener，监听用户设置的Network Policy定义，并将网络访问规则通过各Node的Agent进行实际设置（Agent则需要通过CNI网络插件实现）。\n5.简述Kubernetes中flannel的作用 Flannel可以用于Kubernetes底层网络的实现，主要作用有：\n它能协助Kubernetes，给每一个Node上的Docker容器都分配互相不冲突的IP地址。 它能在这些IP地址之间建立一个覆盖网络（Overlay Network），通过这个覆盖网络，将数据包原封不动地传递到目标容器内。 6.简述Kubernetes Calico网络组件实现原理 Calico是一个基于BGP的纯三层的网络方案，与OpenStack、Kubernetes、AWS、GCE等云平台都能够良好地集成。\nCalico在每个计算节点都利用Linux Kernel实现了一个高效的vRouter来负责数据转发。每个vRouter都通过BGP协议把在本节点上运行的容器的路由信息向整个Calico网络广播，并自动设置到达其他节点的路由转发规则。\nCalico保证所有容器之间的数据流量都是通过IP路由的方式完成互联互通的。Calico节点组网时可以直接利用数据中心的网络结构（L2或者L3），不需要额外的NAT、隧道或者Overlay Network，没有额外的封包解包，能够节约CPU运算，提高网络效率。\n7.简述kube-proxy的作用 kube-proxy运行在所有节点上，它监听apiserver中service和endpoint的变化情况，创建路由规则以提供服务IP和负载均衡功能。简单理解此进程是Service的透明代理兼负载均衡器，其核心功能是将到某个Service的访问请求转发到后端的多个Pod实例上。\n8.简述Kubernetes Service类型 通过创建Service，可以为一组具有相同功能的容器应用提供一个统一的入口地址，并且将请求负载分发到后端的各个容器应用上。其主要类型有：\nClusterIP：虚拟的服务IP地址，该地址用于Kubernetes集群内部的Pod访问，在Node上kube-proxy通过设置的iptables规则进行转发； NodePort：使用宿主机的端口，使能够访问各Node的外部客户端通过Node的IP地址和端口号就能访问服务； LoadBalancer：使用外接负载均衡器完成到服务的负载分发，需要在spec.status.loadBalancer字段指定外部负载均衡器的IP地址，通常用于公有云。 9.简述Kubernetes Service分发后端的策略 Service负载分发的策略有：RoundRobin和SessionAffinity\nRoundRobin：默认为轮询模式，即轮询将请求转发到后端的各个Pod上。 SessionAffinity：基于客户端IP地址进行会话保持的模式，即第1次将某个客户端发起的请求转发到后端的某个Pod上，之后从相同的客户端发起的请求都将被转发到后端相同的Pod上。 10.简述Kubernetes外部如何访问集群内的服务 对于Kubernetes，集群外的客户端默认情况，无法通过Pod的IP地址或者Service的虚拟IP地址：虚拟端口号进行访问。通常可以通过以下方式进行访问Kubernetes集群内的服务：\n映射Pod到物理机：将Pod端口号映射到宿主机，即在Pod中采用hostPort方式，以使客户端应用能够通过物理机访问容器应用。 映射Service到物理机：将Service端口号映射到宿主机，即在Service中采用nodePort方式，以使客户端应用能够通过物理机访问容器应用。 映射Sercie到LoadBalancer：通过设置LoadBalancer映射到云服务商提供的LoadBalancer地址。这种用法仅用于在公有云服务提供商的云平台上设置Service的场景。 11.简述Kubernetes ingress Kubernetes的Ingress资源对象，用于将不同URL的访问请求转发到后端不同的Service，以实现HTTP层的业务路由机制。\nKubernetes使用了Ingress策略和Ingress Controller，两者结合并实现了一个完整的Ingress负载均衡器。使用Ingress进行负载分发时，Ingress Controller基于Ingress规则将客户端请求直接转发到Service对应的后端Endpoint（Pod）上，从而跳过kube-proxy的转发功能，kube-proxy不再起作用，全过程为：ingress controller + ingress 规则 \u0026mdash;-\u0026gt; services。\n同时当Ingress Controller提供的是对外服务，则实际上实现的是边缘路由器的功能。\n","date":"2025-08-12T15:00:19+08:00","permalink":"https://Logic0528.github.io/p/kubernetes%E7%A7%8B%E6%8B%9B%E9%9D%A2%E8%AF%95%E9%A2%98%E5%87%86%E5%A4%87/","title":"kubernetes秋招面试题准备"},{"content":"项目亮点: 云原生架构：全服务容器化部署，通过 K8s 实现资源动态调度、故障自愈，符合云原生 “弹性、可扩展” 特性 自动化流程：CI/CD 流水线将部署周期从小时级缩短至分钟级，减少人工操作错误 全链路监控：覆盖节点硬件、服务性能、业务指标，可快速定位故障与性能瓶颈 ^_^ 一.服务器角色分配 服务器IP 角色 硬件建议 说明 10.11.27.171 Master节点 2 核 4G 集群控制节点 10.11.27.172 Worker节点 4 核 8G 运行应用容器，Gitlab服务器 10.11.27.173 Worker节点 4 核 8G 运行网关，认证服务 10.11.27.174 Worker节点 4 核 8G 运行网关，权限服务 10.11.27.175 Worker节点 4 核 8G 运行业务服务，前端，Harbor镜像仓库 二.前期准备工作 在五台服务器上安装docker,k8s,calico等,并在171服务器上初始化k8s集群，详细步骤见另一篇博客，这里不再赘述。\n三.配置集群基础功能 部署Metrics Server(提供资源监控) Metrics Server 用于收集Pod/节点的CPU，内存使用数据，是HPA的依赖\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 下载适配k8s 1.33的配置文件 kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.6.4/components.yaml # 修改配置（解决本地集群证书问题） kubectl edit deployment metrics-server -n kube-system # 在spec.template.spec.containers.args中添加： # - --kubelet-insecure-tls # 验证Metrics Server状态 kubectl get pods -n kube-system | grep metrics-server # 查看资源使用情况 kubectl top nodes kubectl top pods 四.部署存储解决方案（持久化数据） 微服务需要持久化存储，本地集群推荐使用NFS\n选择10.11.27.175作为NFS服务器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # 安装NFS服务 yum install -y nfs-utils rpcbind # 创建共享目录 mkdir -p /data/nfs/k8s chmod 777 /data/nfs/k8s # 配置NFS共享（允许集群所有节点访问） cat \u0026gt;\u0026gt; /etc/exports \u0026lt;\u0026lt; EOF /data/nfs/k8s 10.11.27.0/24(rw,no_root_squash,sync) EOF # 启动NFS服务 systemctl start rpcbind systemctl start nfs-server systemctl enable rpcbind systemctl enable nfs-server # 验证共享 exportfs -r # 刷新配置 exportfs # 应显示共享目录和允许的网段 在所有节点安装NFS客户端\n1 2 3 4 5 yum install -y nfs-utils # 验证NFS服务器连接，在任意节点执行即可 showmount -e 10.11.27.175 # 预期输出：Export list for 10.11.27.175: /data/nfs/k8s 10.11.27.0/24 在k8s中部署NFS存储类(StorageClass)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 vim nfs-storageclass.yaml apiVersion: v1 kind: ServiceAccount metadata: name: nfs-provisioner namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: nfs-provisioner-runner rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;persistentvolumes\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;delete\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;persistentvolumeclaims\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;update\u0026#34;] - apiGroups: [\u0026#34;storage.k8s.io\u0026#34;] resources: [\u0026#34;storageclasses\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;events\u0026#34;] verbs: [\u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: run-nfs-provisioner subjects: - kind: ServiceAccount name: nfs-provisioner namespace: kube-system roleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io --- apiVersion: apps/v1 kind: Deployment metadata: name: nfs-provisioner namespace: kube-system spec: replicas: 1 selector: matchLabels: app: nfs-provisioner template: metadata: labels: app: nfs-provisioner spec: serviceAccountName: nfs-provisioner containers: - name: nfs-provisioner image: registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/nfs-subdir-external-provisioner:v4.0.2 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: nfs-provisioner # 存储类名称，后续会引用 - name: NFS_SERVER value: 10.11.27.175 # NFS服务器IP - name: NFS_PATH value: /data/nfs/k8s # NFS共享目录 volumes: - name: nfs-client-root nfs: server: 10.11.27.175 path: /data/nfs/k8s --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-storageclass # 存储类名称，创建PVC时需指定 provisioner: nfs-provisioner # 与Deployment中的PROVISIONER_NAME一致 parameters: archiveOnDelete: \u0026#34;false\u0026#34; # 删除PVC时自动删除数据 部署存储类\n1 2 3 4 5 kubectl apply -f nfs-storageclass.yaml # 验证存储类部署 kubectl get pods -n kube-system | grep nfs-provisioner kubectl get sc 五.部署私有镜像仓库 在10.11.27.175上部署Harbor作为私有仓库存储pig项目镜像\n安装Docker Compose\n1 2 3 4 # 在175节点执行 curl -L \u0026#34;https://github.com/docker/compose/releases/download/v2.20.3/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose docker-compose --version 部署Harbor\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 在175节点下载Harbor wget https://github.com/goharbor/harbor/releases/download/v2.8.4/harbor-offline-installer-v2.8.4.tgz tar -zxvf harbor-offline-installer-v2.8.4.tgz cd harbor # 复制配置文件并修改 cp harbor.yml.tmpl harbor.yml vi harbor.yml hostname: 10.11.27.175（改为175的IP） # 注释掉HTTPS相关配置（测试环境无需HTTPS） # 可选：修改默认密码（harbor_admin_password） # 安装Harbor ./install.sh # 启动Harbor docker-compose up -d 配置信任私有仓库,需在所有节点执行\n1 2 3 4 5 6 7 8 9 10 11 12 cat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt; EOF { \u0026#34;insecure-registries\u0026#34;: [\u0026#34;10.11.27.175:80\u0026#34;] } EOF # 重启Docker和containerd systemctl restart docker systemctl restart containerd # 登录Harbor（默认账号admin，密码见harbor.yml配置） docker login 10.11.27.175:80 六.构建pig项目镜像并推送至Harbor 安装构建依赖\n1 2 # 175节点上执行 yum install -y git maven 克隆代码并修改配置\n1 2 3 4 5 6 7 8 9 10 11 12 git clone https://github.com/pig-mesh/pig.git cd pig # 修改配置文件，适配k8s环境： # 1. 所有服务的Nacos地址改为k8s内部服务名（后续会创建nacos-service） find . -name \u0026#34;application.yml\u0026#34; -exec sed -i \u0026#39;s/server-addr: .*/server-addr: nacos-service:8848/g\u0026#39; {} \\; # 2. 数据库地址改为k8s内部服务名（后续会创建mysql-service） find . -name \u0026#34;application.yml\u0026#34; -exec sed -i \u0026#39;s/url: jdbc:mysql:\\/\\/.*/url: jdbc:mysql:\\/\\/mysql-service:3306\\/pig?useUnicode=true\u0026amp;characterEncoding=utf8\u0026amp;serverTimezone=Asia\\/Shanghai/g\u0026#39; {} \\; # 3. Redis地址改为k8s内部服务名（后续会创建redis-service） find . -name \u0026#34;application.yml\u0026#34; -exec sed -i \u0026#39;s/host: .*/host: redis-service/g\u0026#39; {} \\; 构建Docker镜像并推送\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # 构建基础镜像（项目根目录下） mvn clean package -DskipTests # 构建并推送Nacos镜像（pig-register） cd pig-register docker build -t 10.11.27.175:80/pig/pig-register:latest . docker push 10.11.27.175:80/pig/pig-register:latest # 构建并推送网关镜像（pig-gateway） cd ../pig-gateway docker build -t 10.11.27.175:80/pig/pig-gateway:latest . docker push 10.11.27.175:80/pig/pig-gateway:latest # 构建并推送认证服务（pig-auth） cd ../pig-auth docker build -t 10.11.27.175:80/pig/pig-auth:latest . docker push 10.11.27.175:80/pig/pig-auth:latest # 构建并推送权限服务（pig-upms-biz） cd ../pig-upms/pig-upms-biz docker build -t 10.11.27.175:80/pig/pig-upms-biz:latest . docker push 10.11.27.175:80/pig/pig-upms-biz:latest # 构建并推送监控服务（pig-monitor） cd /root/pig/pig-visual/pig-monitor docker build -t 10.11.27.175:80/pig/pig-monitor:latest . docker push 10.11.27.175:80/pig/pig-monitor:latest # 构建并推送代码生成服务（pig-codegen） cd /root/pig/pig-visual/pig-codegen docker build -t 10.11.27.175:80/pig/pig-codegen:latest . docker push 10.11.27.175:80/pig/pig-codegen:latest # 构建并推送定时任务服务（pig-quartz） cd /root/pig/pig-visual/pig-quartz docker build -t 10.11.27.175:80/pig/pig-quartz:latest . docker push 10.11.27.175:80/pig/pig-quartz:latest 验证镜像推送结果 浏览器访问http://10.11.27.175:80,在pig项目下能看到推送的镜像\n七.部署pig项目依赖服务 部署MySQL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 apiVersion: v1 kind: Service metadata: name: mysql labels: app: mysql spec: ports: - port: 3306 name: mysql clusterIP: None # 无头服务，与StatefulSet配合 selector: app: mysql --- apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql spec: serviceName: \u0026#34;mysql\u0026#34; # 关联上面定义的无头服务 replicas: 1 # 可根据需要扩展为多节点 selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: initContainers: - name: init-mysql image: mysql:8.0 command: - bash - \u0026#34;-c\u0026#34; - | set -ex # 生成MySQL配置文件 cat \u0026gt; /mnt/conf.d/my.cnf \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; [mysqld] skip-host-cache skip-name-resolve datadir = /var/lib/mysql socket = /var/lib/mysql/mysql.sock pid-file = /var/run/mysqld/mysqld.pid EOF # 为不同实例生成标识文件 if [ -z \u0026#34;$(ls -A /var/lib/mysql)\u0026#34; ]; then echo \u0026#34;server-id=1\u0026#34; \u0026gt;\u0026gt; /mnt/conf.d/my.cnf fi volumeMounts: - name: conf mountPath: /mnt/conf.d - name: data mountPath: /var/lib/mysql subPath: mysql containers: - name: mysql image: mysql:8.0 ports: - containerPort: 3306 name: mysql env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-secret key: root-password - name: MYSQL_DATABASE value: \u0026#34;pig\u0026#34; volumeMounts: - name: conf mountPath: /etc/mysql/conf.d - name: data mountPath: /var/lib/mysql subPath: mysql - name: init-sql mountPath: /docker-entrypoint-initdb.d livenessProbe: exec: command: [\u0026#34;mysqladmin\u0026#34;, \u0026#34;ping\u0026#34;, \u0026#34;-u\u0026#34;, \u0026#34;root\u0026#34;, \u0026#34;-p$(MYSQL_ROOT_PASSWORD)\u0026#34;] initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 readinessProbe: exec: command: [\u0026#34;mysql\u0026#34;, \u0026#34;-h\u0026#34;, \u0026#34;127.0.0.1\u0026#34;, \u0026#34;-u\u0026#34;, \u0026#34;root\u0026#34;, \u0026#34;-p$(MYSQL_ROOT_PASSWORD)\u0026#34;, \u0026#34;-e\u0026#34;, \u0026#34;SELECT 1\u0026#34;] initialDelaySeconds: 5 periodSeconds: 2 timeoutSeconds: 1 volumes: - name: conf emptyDir: {} - name: init-sql hostPath: path: /root/pig/db type: DirectoryOrCreate # 定义PVC模板，StatefulSet会为每个实例创建独立的PVC volumeClaimTemplates: - metadata: name: data spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] storageClassName: \u0026#34;nfs-storageclass\u0026#34; resources: requests: storage: 10Gi --- # 存储数据库密码的Secret apiVersion: v1 kind: Secret metadata: name: mysql-secret type: Opaque data: root-password: cm9vdA== # 这里是\u0026#34;root\u0026#34;的base64编码，实际使用应更换为强密码 部署MySQL\n1 2 3 4 5 # 确保pig项目的db目录在部署节点存在 kubectl apply -f mysql-deploy.yaml # 验证MySQL状态 kubectl get pods | grep mysql 部署redis\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 apiVersion: v1 kind: Service metadata: name: redis labels: app: redis spec: ports: - port: 6379 name: redis clusterIP: None # 无头服务 selector: app: redis --- apiVersion: apps/v1 kind: StatefulSet metadata: name: redis spec: serviceName: \u0026#34;redis\u0026#34; # 关联无头服务 replicas: 1 selector: matchLabels: app: redis template: metadata: labels: app: redis spec: containers: - name: redis image: redis:7.0 ports: - containerPort: 6379 name: redis command: [\u0026#34;redis-server\u0026#34;, \u0026#34;--requirepass\u0026#34;, \u0026#34;$(REDIS_PASSWORD)\u0026#34;] env: - name: REDIS_PASSWORD valueFrom: secretKeyRef: name: redis-secret key: password volumeMounts: - name: data mountPath: /data livenessProbe: exec: command: [\u0026#34;redis-cli\u0026#34;, \u0026#34;-a\u0026#34;, \u0026#34;$(REDIS_PASSWORD)\u0026#34;, \u0026#34;ping\u0026#34;] initialDelaySeconds: 30 periodSeconds: 10 readinessProbe: exec: command: [\u0026#34;redis-cli\u0026#34;, \u0026#34;-a\u0026#34;, \u0026#34;$(REDIS_PASSWORD)\u0026#34;, \u0026#34;ping\u0026#34;] initialDelaySeconds: 5 periodSeconds: 5 volumeClaimTemplates: - metadata: name: data spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] storageClassName: \u0026#34;nfs-storageclass\u0026#34; resources: requests: storage: 5Gi --- apiVersion: v1 kind: Secret metadata: name: redis-secret type: Opaque data: password: cmVkaXM= # \u0026#34;redis\u0026#34;的base64编码 部署redis\n1 2 kubectl apply -f redis-deploy.yaml kubectl get pods | grep redis 部署Nacos集群\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 vim nacos-deploy.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nacos-pvc-0 spec: accessModes: - ReadWriteOnce storageClassName: nfs-storageclass resources: requests: storage: 5Gi --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nacos-pvc-1 spec: accessModes: - ReadWriteOnce storageClassName: nfs-storageclass resources: requests: storage: 5Gi --- apiVersion: v1 kind: Service metadata: name: nacos-service spec: ports: - port: 8848 name: server clusterIP: None selector: app: nacos --- apiVersion: apps/v1 kind: StatefulSet metadata: name: nacos spec: serviceName: nacos-service replicas: 2 selector: matchLabels: app: nacos template: metadata: labels: app: nacos spec: containers: - name: nacos image: 10.11.27.175:80/pig/pig-register:latest # 私有仓库镜像 ports: - containerPort: 8848 env: - name: MODE value: \u0026#34;cluster\u0026#34; - name: NACOS_SERVERS value: \u0026#34;nacos-0.nacos-service.default.svc.cluster.local:8848,nacos-1.nacos-service.default.svc.cluster.local:8848\u0026#34; - name: SPRING_DATASOURCE_PLATFORM value: \u0026#34;mysql\u0026#34; - name: MYSQL_SERVICE_HOST value: \u0026#34;mysql-service\u0026#34; - name: MYSQL_SERVICE_DB_NAME value: \u0026#34;pig_config\u0026#34; - name: MYSQL_SERVICE_USER value: \u0026#34;root\u0026#34; - name: MYSQL_SERVICE_PASSWORD value: \u0026#34;root\u0026#34; volumeMounts: - name: nacos-data mountPath: /home/nacos/data volumeClaimTemplates: - metadata: name: nacos-data spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] storageClassName: \u0026#34;nfs-storageclass\u0026#34; resources: requests: storage: 5Gi 部署nacos\n1 2 kubectl apply -f nacos-deploy.yaml kubectl get pods | grep nacos # 2个实例均为Running 八.部署pig核心微服务 部署网关 pig-gateway\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 vim gateway-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: pig-gateway spec: replicas: 2 # 2个实例实现负载均衡 selector: matchLabels: app: pig-gateway template: metadata: labels: app: pig-gateway annotation: prometheus.io/scrape: \u0026#34;true\u0026#34; # 允许Prometheus采集 prometheus.io/path: \u0026#34;/actuator/prometheus\u0026#34; # 指标暴露路径 prometheus.io/port: \u0026#34;9999\u0026#34; # 指标暴露端口（网关的端口） spec: containers: - name: pig-gateway image: 10.11.27.175:80/pig/pig-gateway:latest ports: - containerPort: 9999 livenessProbe: # 健康检查 httpGet: path: /actuator/health port: 9999 initialDelaySeconds: 60 periodSeconds: 10 --- apiVersion: v1 kind: Service metadata: name: pig-gateway-service spec: selector: app: pig-gateway ports: - port: 80 targetPort: 9999 部署网关\n1 2 kubectl apply -f gateway-deploy.yaml kubectl get pods | grep pig-gateway 部署认证服务 pig-auth\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 vim auth-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: pig-auth spec: replicas: 1 selector: matchLabels: app: pig-auth template: metadata: labels: app: pig-auth annotations: prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/path: \u0026#34;/actuator/prometheus\u0026#34; prometheus.io/port: \u0026#34;3000\u0026#34; # 认证服务的端口 spec: containers: - name: pig-auth image: 10.11.27.175:80/pig/pig-auth:latest ports: - containerPort: 3000 --- apiVersion: v1 kind: Service metadata: name: pig-auth-service spec: selector: app: pig-auth ports: - port: 3000 targetPort: 3000 部署认证服务\n1 2 kubectl apply -f auth-deploy.yaml kubectl get pods | grep pig-auth 部署权限服务 pig-upms-biz\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 vim upms-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: pig-upms-biz spec: replicas: 1 selector: matchLabels: app: pig-upms-biz template: metadata: labels: app: pig-upms-biz annotations: prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/path: \u0026#34;/actuator/prometheus\u0026#34; prometheus.io/port: \u0026#34;4000\u0026#34; # 权限服务的端口 spec: containers: - name: pig-upms-biz image: 10.11.27.175:80/pig/pig-upms-biz:latest ports: - containerPort: 4000 env: - name: SPRING_PROFILES_ACTIVE value: \u0026#34;prod\u0026#34; --- apiVersion: v1 kind: Service metadata: name: pig-upms-biz-service spec: selector: app: pig-upms-biz ports: - port: 4000 targetPort: 4000 部署权限服务\n1 2 kubectl apply -f upms-deploy.yaml kubectl get pods | grep pig-upms-biz 部署监控服务 pig-monitor\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 vim monitor-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: pig-monitor spec: replicas: 1 selector: matchLabels: app: pig-monitor template: metadata: labels: app: pig-monitor annotations: prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/path: \u0026#34;/actuator/prometheus\u0026#34; prometheus.io/port: \u0026#34;5001\u0026#34; # 权限服务的端口 spec: containers: - name: pig-monitor image: 10.11.27.175:80/pig/pig-monitor:latest ports: - containerPort: 5001 --- apiVersion: v1 kind: Service metadata: name: pig-monitor-service spec: selector: app: pig-monitor ports: - port: 5001 targetPort: 5001 部署监控服务\n1 kubectl apply -f monitor-deploy.yaml 部署代码生成服务 pig-codegen\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 vim codegen-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: pig-codegen spec: replicas: 1 selector: matchLabels: app: pig-codegen template: metadata: labels: app: pig-codegen annotations: prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/path: \u0026#34;/actuator/prometheus\u0026#34; prometheus.io/port: \u0026#34;5003\u0026#34; # 权限服务的端口 spec: containers: - name: pig-codegen image: 10.11.27.175:80/pig/pig-codegen:latest ports: - containerPort: 5003 --- apiVersion: v1 kind: Service metadata: name: pig-codegen-service spec: selector: app: pig-codegen ports: - port: 5003 targetPort: 5003 部署代码生成服务\n1 kubectl apply -f codegen-deploy.yaml 部署定时任务服务 pig-quartz\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 vim quartz-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: pig-quartz spec: replicas: 1 selector: matchLabels: app: pig-quartz template: metadata: labels: app: pig-quartz annotations: prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/path: \u0026#34;/actuator/prometheus\u0026#34; prometheus.io/port: \u0026#34;5006\u0026#34; # 权限服务的端口 spec: containers: - name: pig-quartz image: 10.11.27.175:80/pig/pig-quartz:latest ports: - containerPort: 5006 --- apiVersion: v1 kind: Service metadata: name: pig-quartz-service spec: selector: app: pig-quartz ports: - port: 5006 targetPort: 5006 部署\n1 kubectl apply -f quartz-deploy.yaml 九.部署Ingress控制器(外部访问入口) 部署Ingress-NGINX,实现外部流量路由到网关服务\n部署Ingress控制器\n1 2 3 4 kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.9.4/deploy/static/provider/baremetal/deploy.yaml # 验证部署 kubectl get pods -n ingress-nginx 配置Ingress路由规则\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 vim pig-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: pig-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: ingressClassName: nginx rules: - host: pig.local # 本地测试域名 http: paths: - path: / pathType: Prefix backend: service: name: pig-gateway-service port: number: 80 部署Ingress\n1 2 3 4 kubectl apply -f pig-ingress.yaml # 查看Ingress状态 kubectl get ingress 十、验证应用访问 查看Ingress控制器暴露的端口\n1 kubectl get svc -n ingress-nginx 配置本地hosts\n1 10.11.27.172 pig.local # 指向任意worker节点IP 访问应用\n浏览器访问http://pig.local:30080/admin（使用上一步的 NodePort），预期能看到pig项目的登录页面（默认账号admin，密码123456）\n十一、部署前端 构建并推送前端镜像\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # 克隆前端代码（在175节点） git clone https://gitee.com/log4j/pig-ui.git cd pig-ui # 安装依赖并打包（需Node.js环境） yum install -y nodejs npm npm install npm run build # 生成dist目录 # 构建Docker镜像 cat \u0026gt; Dockerfile \u0026lt;\u0026lt; EOF FROM nginx:alpine COPY dist/ /usr/share/nginx/html/ COPY nginx.conf /etc/nginx/conf.d/default.conf # 可选：自定义Nginx配置 EOF # 自定义Nginx配置（解决跨域问题） cat \u0026gt; nginx.conf \u0026lt;\u0026lt; EOF server { listen 80; root /usr/share/nginx/html; index index.html; # 转发API请求到网关 location /api/ { proxy_pass http://pig-gateway-service:80/; } } EOF # 构建并推送镜像 docker build -t 10.11.27.175:80/pig/pig-ui:latest . docker push 10.11.27.175:80/pig/pig-ui:latest 在k8s中部署前端\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 vim ui-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: pig-ui spec: replicas: 1 selector: matchLabels: app: pig-ui template: metadata: labels: app: pig-ui spec: containers: - name: pig-ui image: 10.11.27.175:80/pig/pig-ui:latest ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: pig-ui-service spec: selector: app: pig-ui ports: - port: 80 targetPort: 80 部署\n1 kubectl apply -f ui-deploy.yaml 配置Ingress路由前端\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # 修改pig-ingress.yaml,添加前端路由： apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: pig-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: ingressClassName: nginx rules: - host: pig.local http: paths: - path: /admin # 前端路径 pathType: Prefix backend: service: name: pig-ui-service port: number: 80 - path: / # API路径 pathType: Prefix backend: service: name: pig-gateway-service port: number: 80 更新Ingress\n1 kubectl apply -f pig-ingress.yaml 访问验证：浏览器打开http://pig.local:30080/admin（30080 为 Ingress 的 NodePort）\n十二、配置HPA自动扩缩容 创建HPA配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 vim gate-hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: pig-gateway-hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: pig-gateway # 关联网关的Deployment minReplicas: 2 # 最小实例数 maxReplicas: 5 # 最大实例数 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 # CPU使用率超过70%时扩容 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 # 内存使用率超过80%时扩容 部署HPA\n1 2 3 4 5 kubectl apply -f gateway-hpa.yaml # 验证HPA kubectl get hpa # 输出应显示当前实例数、目标CPU/内存使用率 十三、Kubernetes原生部署监控系统 Prometheus + Grafana 通过 K8s 部署时，需要定义以下资源：\nNamespace: 独立命令空间，隔离监控组件 ConfigMap: 存储Prometheus配置，如监控目标，采集规则 Deployment: 部署Grafana(无状态应用) StatufalSet: 部署Prometheus(有状态应用，需要持久化存储指标数据) Service: 暴露Prometheus和Grafana的访问入口 PersistentVolumeClaim: 为Prometheus提供持久化存储 创建命名空间\n1 2 3 4 5 6 7 8 vim monitoring-namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring kubectl apply -f monitoring-namespace.yaml 部署Prometheus\n创建Prometheus ConfigMap配置文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 # prometheus-config.yaml apiVersion: v1 kind: ConfigMap metadata: name: prometheus-config namespace: monitoring data: prometheus.yml: | global: scrape_interval: 15s # 默认采集间隔 # 监控目标配置 scrape_configs: # 1. 监控 K8s 节点（通过 node-exporter） - job_name: \u0026#39;node-exporter\u0026#39; kubernetes_sd_configs: - role: node # 自动发现所有节点 relabel_configs: - source_labels: [__address__] action: replace regex: \u0026#39;(.*):10250\u0026#39; replacement: \u0026#39;${1}:9100\u0026#39; # 指向 node-exporter 的 9100 端口 - action: labelmap regex: __meta_kubernetes_node_label_(.+) # 2. 监控 K8s 集群组件（如 apiserver） - job_name: \u0026#39;kubernetes-apiservers\u0026#39; kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https # 3. 监控所有 Pod（自动发现） - job_name: \u0026#39;kubernetes-pods\u0026#39; kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true # 只采集带 annotation: prometheus.io/scrape: \u0026#34;true\u0026#34; 的 Pod 部署node-expoter,通过DaemonSet在所有节点部署node-expoter,采集节点硬件指标\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 # node-exporter.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: node-exporter namespace: monitoring spec: selector: matchLabels: app: node-exporter template: metadata: labels: app: node-exporter spec: containers: - name: node-exporter image: prom/node-exporter:v1.5.0 ports: - containerPort: 9100 hostPort: 9100 # 暴露到节点端口，便于 Prometheus 访问 volumeMounts: - name: proc mountPath: /host/proc readOnly: true - name: sys mountPath: /host/sys readOnly: true volumes: - name: proc hostPath: path: /proc - name: sys hostPath: path: /sys --- # 暴露 node-exporter 服务（可选，用于调试） apiVersion: v1 kind: Service metadata: name: node-exporter namespace: monitoring spec: selector: app: node-exporter ports: - port: 9100 targetPort: 9100 type: ClusterIP 部署Prometheus StatefulSet+ PVC\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 # prometheus-deploy.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: prometheus-data namespace: monitoring spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi # 按需调整存储大小 storageClassName: --- apiVersion: apps/v1 kind: StatefulSet metadata: name: prometheus namespace: monitoring spec: serviceName: prometheus replicas: 1 selector: matchLabels: app: prometheus template: metadata: labels: app: prometheus spec: containers: - name: prometheus image: prom/prometheus:v2.45.0 args: - \u0026#34;--config.file=/etc/prometheus/prometheus.yml\u0026#34; - \u0026#34;--storage.tsdb.path=/prometheus\u0026#34; - \u0026#34;--web.enable-lifecycle\u0026#34; # 支持热重载配置 ports: - containerPort: 9090 volumeMounts: - name: config-volume mountPath: /etc/prometheus - name: prometheus-data mountPath: /prometheus volumes: - name: config-volume configMap: name: prometheus-config volumeClaimTemplates: - metadata: name: prometheus-data spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] resources: requests: storage: 10Gi storageClassName: nfs-storageclass --- # 暴露 Prometheus 服务 apiVersion: v1 kind: Service metadata: name: prometheus namespace: monitoring spec: selector: app: prometheus ports: - port: 9090 targetPort: 9090 type: NodePort # 暴露到节点端口，便于外部访问 在pom.xml文件中添加Prometheus指标暴露依赖\n1 2 3 4 5 6 7 8 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.micrometer\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;micrometer-registry-prometheus\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 在 application.yml中开启端点\n1 2 3 4 5 6 7 8 9 management: endpoints: web: exposure: include: prometheus,health # 暴露 prometheus 端点 metrics: export: prometheus: enabled: true 应用配置文件与验证状态\n1 2 3 4 5 6 kubectl apply -f prometheus-config.yaml kubectl apply -f node-exporter.yaml kubectl apply -f prometheus-deploy.yaml kubectl get pods -n monitoring | grep prometheus kubectl get svc -n monitoring prometheus # 查看 NodePort 部署Grafana\n部署Grafana Deployment + Service\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 # grafana-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: grafana namespace: monitoring spec: replicas: 1 selector: matchLabels: app: grafana template: metadata: labels: app: grafana spec: containers: - name: grafana image: grafana/grafana:9.5.2 ports: - containerPort: 3000 env: - name: GF_SECURITY_ADMIN_PASSWORD value: \u0026#34;grafana123\u0026#34; # 初始密码 volumeMounts: - name: grafana-data mountPath: /var/lib/grafana volumes: - name: grafana-data emptyDir: {} # 临时存储，生产环境建议用 PVC 持久化 --- # 暴露 Grafana 服务 apiVersion: v1 kind: Service metadata: name: grafana namespace: monitoring spec: selector: app: grafana ports: - port: 3000 targetPort: 3000 type: NodePort # 暴露到节点端口 应用配置文件并验证\n1 2 3 4 kubectl apply -f grafana-deploy.yaml kubectl get pods -n monitoring | grep grafana # 状态应为 Running kubectl get svc -n monitoring grafana # 查看 NodePort 十四、数据备份 创建备份脚本backup.sh\n1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash BACKUP_DIR=\u0026#34;/data/backup\u0026#34; TIMESTAMP=$(date +%Y%m%d_%H%M%S) NFS_DATA_DIR=\u0026#34;/data/nfs/k8s\u0026#34; # NFS共享目录 # 创建备份目录 mkdir -p $BACKUP_DIR # 压缩备份 tar -zcvf $BACKUP_DIR/nfs-backup-$TIMESTAMP.tar.gz $NFS_DATA_DIR # 保留最近30天的备份 find $BACKUP_DIR -name \u0026#34;nfs-backup-*.tar.gz\u0026#34; -mtime +30 -delete 配置定时任务\n1 2 3 4 5 chmod +x backup.sh crontab -e # 添加以下内容： 0 2 * * * /root/backup.sh 十五、搭建CI/CD流水线 部署GitLab服务器\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 创建数据目录（持久化GitLab数据） mkdir -p /data/gitlab/{config,logs,data} # 启动GitLab docker run -d \\ --hostname 10.11.27.172 \\ --publish 80:80 --publish 443:443 \\ --name gitlab \\ --restart always \\ --volume /data/gitlab/config:/etc/gitlab \\ --volume /data/gitlab/logs:/var/log/gitlab \\ --volume /data/gitlab/data:/var/opt/gitlab \\ gitlab/gitlab-ce:16.0.0-ce.0 初始化Gitlab\n访问http://10.11.27.172(首次需等待较长时间)\n获取初始密码\n1 docker exec -it gitlab grep \u0026#39;Password:\u0026#39; /etc/gitlab/initial_root_password 登录账号：root，输入密码后修改为自定义密码\n创建项目\n部署Gitlab Runner\n1 2 3 4 5 6 7 # 启动Runner docker run -d \\ --name gitlab-runner \\ --restart always \\ --volume /srv/gitlab-runner/config:/etc/gitlab-runner \\ --volume /var/run/docker.sock:/var/run/docker.sock \\ # 允许操作Docker gitlab/gitlab-runner:v16.0.0 注册Runner到GitLab\n在GitLab项目页面获取注册信息 进入项目-\u0026gt;Settings-\u0026gt;CI/CD-\u0026gt;Runners-\u0026gt;Expand,记下URL和Registration token\n执行注册命令\n1 docker exec -it gitlab-runner gitlab-runner register Gitlab instance URL: http://10.11.27.172\nRegistration tokne: 上述令牌\nTags: pig,k8s,172\nExecutor: docker\nDefault Docker image: maven:3.8-openjdk-11\n配置Runner权限\nRunner需要访问k8s集群，推送镜像到175节点的Harbor\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # 1. 从k8s Master节点（171）复制kubeconfig到172节点 # 在171节点执行： scp /root/.kube/config root@10.11.27.172:/root/.kube/config # 2. 在172节点创建Maven缓存目录 mkdir -p /data/maven/repository # 3. 编辑Runner配置 vi /srv/gitlab-runner/config/config.toml # 在[[runners]]下的[runners.docker]中添加volumes配置： [[runners]] ... [runners.docker] ... volumes = [ \u0026#34;/var/run/docker.sock:/var/run/docker.sock\u0026#34;, \u0026#34;/root/.kube/config:/root/.kube/config\u0026#34;, # 挂载k8s配置 \u0026#34;/data/maven/repository:/root/.m2/repository\u0026#34; # 缓存Maven依赖 ] # 4. 重启Runner docker restart gitlab-runner docker login 10.11.27.175:80 十六、编写CI/CD流水线配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 # 定义流水线阶段（按顺序执行） stages: - build # 编译代码 - test # 单元测试 - build-image # 构建Docker镜像 - push-image # 推送镜像到Harbor（175节点） - deploy # 部署到k8s集群 # 全局变量 variables: HARBOR_URL: \u0026#34;10.11.27.175:80\u0026#34; # Harbor在175节点 HARBOR_PROJECT: \u0026#34;pig\u0026#34; # Harbor中的项目名 K8S_NAMESPACE: \u0026#34;default\u0026#34; # 部署到k8s的default命名空间 MAVEN_OPTS: \u0026#34;-Dmaven.repo.local=/root/.m2/repository\u0026#34; # 复用Maven缓存 # ${CI_COMMIT_SHORT_SHA} 是 GitLab CI/CD 内置的环境变量，用于表示当前代码提交的短哈希值（Short Commit SHA），是 Git 提交记录的唯一标识符 # 阶段1：编译代码 build: stage: build tags: [pig, k8s, 172] # 匹配172节点的Runner image: maven:3.8-openjdk-11 # 用Maven镜像编译 script: - echo \u0026#34;编译代码...\u0026#34; - mvn clean package -DskipTests # 跳过测试（单独阶段执行） artifacts: paths: - \u0026#34;**/target/*.jar\u0026#34; # 保存编译产物 expire_in: 1 hour # 1小时后自动清理 # 阶段2：单元测试 test: stage: test tags: [pig, k8s, 172] image: maven:3.8-openjdk-11 script: - echo \u0026#34;运行单元测试...\u0026#34; - mvn test artifacts: paths: - \u0026#34;**/target/surefire-reports/\u0026#34; # 保存测试报告 # 阶段3-5：网关服务（pig-gateway）的构建、推送、部署 build-gateway-image: stage: build-image tags: [pig, k8s, 172] image: docker:20.10 # 用Docker镜像构建 dependencies: - build # 依赖编译阶段的产物 script: - echo \u0026#34;构建网关镜像...\u0026#34; - cd pig-gateway - docker build -t $HARBOR_URL/$HARBOR_PROJECT/pig-gateway:${CI_COMMIT_SHORT_SHA} . push-gateway-image: stage: push-image tags: [pig, k8s, 172] image: docker:20.10 script: - echo \u0026#34;登录Harbor...\u0026#34; - docker login -u admin -p $HARBOR_PASSWORD $HARBOR_URL # 密码从GitLab变量获取 - echo \u0026#34;推送网关镜像...\u0026#34; - docker push $HARBOR_URL/$HARBOR_PROJECT/pig-gateway:${CI_COMMIT_SHORT_SHA} deploy-gateway: stage: deploy tags: [pig, k8s, 172] image: bitnami/kubectl:1.28 # 带kubectl的镜像 script: - echo \u0026#34;更新k8s网关部署...\u0026#34; - kubectl set image deployment/pig-gateway pig-gateway=$HARBOR_URL/$HARBOR_PROJECT/pig-gateway:${CI_COMMIT_SHORT_SHA} -n $K8S_NAMESPACE - kubectl rollout status deployment/pig-gateway -n $K8S_NAMESPACE # 等待部署完成 # 阶段3-5：认证服务（pig-auth）的构建、推送、部署 build-auth-image: stage: build-image tags: [pig, k8s, 172] image: docker:20.10 dependencies: - build script: - echo \u0026#34;构建认证服务镜像...\u0026#34; - cd pig-auth - docker build -t $HARBOR_URL/$HARBOR_PROJECT/pig-auth:${CI_COMMIT_SHORT_SHA} . push-auth-image: stage: push-image tags: [pig, k8s, 172] image: docker:20.10 script: - docker login -u admin -p $HARBOR_PASSWORD $HARBOR_URL - docker push $HARBOR_URL/$HARBOR_PROJECT/pig-auth:${CI_COMMIT_SHORT_SHA} deploy-auth: stage: deploy tags: [pig, k8s, 172] image: bitnami/kubectl:1.28 script: - echo \u0026#34;更新k8s认证服务部署...\u0026#34; - kubectl set image deployment/pig-auth pig-auth=$HARBOR_URL/$HARBOR_PROJECT/pig-auth:${CI_COMMIT_SHORT_SHA} -n $K8S_NAMESPACE - kubectl rollout status deployment/pig-auth -n $K8S_NAMESPACE # 阶段3-5：权限服务（pig-upms-biz）的构建、推送、部署 build-upms-biz-image: stage: build-image tags: [pig, k8s, 172] image: docker:20.10 dependencies: - build script: - echo \u0026#34;构建权限服务镜像...\u0026#34; - cd pig-upms/pig-upms-biz - docker build -t $HARBOR_URL/$HARBOR_PROJECT/pig-upms-biz:${CI_COMMIT_SHORT_SHA} . push-upms-biz-image: stage: push-image tags: [pig, k8s, 172] image: docker:20.10 script: - docker login -u admin -p $HARBOR_PASSWORD $HARBOR_URL - echo \u0026#34;推送权限服务镜像...\u0026#34; - docker push $HARBOR_URL/$HARBOR_PROJECT/pig-upms-biz:${CI_COMMIT_SHORT_SHA} deploy-upms-biz: stage: deploy tags: [pig, k8s, 172] image: bitnami/kubectl:1.28 script: - echo \u0026#34;更新k8s权限服务部署...\u0026#34; - kubectl set image deployment/pig-upms-biz pig-upms-biz=$HARBOR_URL/$HARBOR_PROJECT/pig-upms-biz:${CI_COMMIT_SHORT_SHA} -n $K8S_NAMESPACE - kubectl rollout status deployment/pig-upms-biz -n $K8S_NAMESPACE # 阶段3-5：监控服务（pig-monitor）的构建、推送、部署 build-monitor-image: stage: build-image tags: [pig, k8s, 172] image: docker:20.10 dependencies: - build script: - echo \u0026#34;构建监控服务镜像...\u0026#34; - cd pig-visual/pig-monitor # 路径：visual子模块下 - docker build -t $HARBOR_URL/$HARBOR_PROJECT/pig-monitor:${CI_COMMIT_SHORT_SHA} . push-monitor-image: stage: push-image tags: [pig, k8s, 172] image: docker:20.10 script: - docker login -u admin -p $HARBOR_PASSWORD $HARBOR_URL - echo \u0026#34;推送监控服务镜像...\u0026#34; - docker push $HARBOR_URL/$HARBOR_PROJECT/pig-monitor:${CI_COMMIT_SHORT_SHA} deploy-monitor: stage: deploy tags: [pig, k8s, 172] image: bitnami/kubectl:1.28 script: - echo \u0026#34;更新k8s监控服务部署...\u0026#34; - kubectl set image deployment/pig-monitor pig-monitor=$HARBOR_URL/$HARBOR_PROJECT/pig-monitor:${CI_COMMIT_SHORT_SHA} -n $K8S_NAMESPACE - kubectl rollout status deployment/pig-monitor -n $K8S_NAMESPACE # 阶段3-5：代码生成服务（pig-codegen）的构建、推送、部署 build-codegen-image: stage: build-image tags: [pig, k8s, 172] image: docker:20.10 dependencies: - build script: - echo \u0026#34;构建代码生成服务镜像...\u0026#34; - cd pig-visual/pig-codegen - docker build -t $HARBOR_URL/$HARBOR_PROJECT/pig-codegen:${CI_COMMIT_SHORT_SHA} . push-codegen-image: stage: push-image tags: [pig, k8s, 172] image: docker:20.10 script: - docker login -u admin -p $HARBOR_PASSWORD $HARBOR_URL - echo \u0026#34;推送代码生成服务镜像...\u0026#34; - docker push $HARBOR_URL/$HARBOR_PROJECT/pig-codegen:${CI_COMMIT_SHORT_SHA} deploy-codegen: stage: deploy tags: [pig, k8s, 172] image: bitnami/kubectl:1.28 script: - echo \u0026#34;更新k8s代码生成服务部署...\u0026#34; - kubectl set image deployment/pig-codegen pig-codegen=$HARBOR_URL/$HARBOR_PROJECT/pig-codegen:${CI_COMMIT_SHORT_SHA} -n $K8S_NAMESPACE - kubectl rollout status deployment/pig-codegen -n $K8S_NAMESPACE # 阶段3-5：定时任务服务（pig-quartz）的构建、推送、部署 build-quartz-image: stage: build-image tags: [pig, k8s, 172] image: docker:20.10 dependencies: - build script: - echo \u0026#34;构建定时任务服务镜像...\u0026#34; - cd pig-visual/pig-quartz - docker build -t $HARBOR_URL/$HARBOR_PROJECT/pig-quartz:${CI_COMMIT_SHORT_SHA} . push-quartz-image: stage: push-image tags: [pig, k8s, 172] image: docker:20.10 script: - docker login -u admin -p $HARBOR_PASSWORD $HARBOR_URL - echo \u0026#34;推送定时任务服务镜像...\u0026#34; - docker push $HARBOR_URL/$HARBOR_PROJECT/pig-quartz:${CI_COMMIT_SHORT_SHA} deploy-quartz: stage: deploy tags: [pig, k8s, 172] image: bitnami/kubectl:1.28 script: - echo \u0026#34;更新k8s定时任务服务部署...\u0026#34; - kubectl set image deployment/pig-quartz pig-quartz=$HARBOR_URL/$HARBOR_PROJECT/pig-quartz:${CI_COMMIT_SHORT_SHA} -n $K8S_NAMESPACE - kubectl rollout status deployment/pig-quartz -n $K8S_NAMESPACE 十七、配置Gitlab保密变量 避免密码明文暴露的同时，也方便引用，如上面的$HARBOR_PASSWORD\n进入项目-\u0026gt;Settings-\u0026gt;CI/CD-\u0026gt;Variables-\u0026gt;Add variable 添加变量： 名称：HARBOR_PASSWORD value：HARBOR12345 勾选 \u0026ldquo;Mask variable\u0026rdquo; 十八、测试CI/CD流水线 将配置文件提交到GitLab仓库\n1 2 3 4 # 在本地项目目录执行 git add .gitlab-ci.yml git commit -m \u0026#34;add ci/cd pipeline (gitlab on 172)\u0026#34; git push origin main 查看流水线执行状态\n","date":"2025-08-07T17:28:00+08:00","permalink":"https://Logic0528.github.io/p/%E5%9F%BA%E4%BA%8E-k8s-%E7%9A%84%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%A1%B9%E7%9B%AE%E5%AE%B9%E5%99%A8%E5%8C%96%E9%83%A8%E7%BD%B2%E4%B8%8E%E9%AB%98%E5%8F%AF%E7%94%A8%E5%AE%9E%E8%B7%B5/","title":"基于 K8s 的微服务项目容器化部署与高可用实践"},{"content":"目录 开始迁移前可在旧环境创建一些带有时间标识的数据，方便迁移后验证是否迁移成功\n旧环境：\n备份安装介质 停服务 备份mysql 备份mongodb 定开备份 新环境\n初始化蓝鲸平台 迁移环境变量 部署蓝鲸平台 部署devops 验证功能正常后停服 备份新平台数据 迁移mysql 迁移MongoDB 拉起服务 恢复后处理\n旧环境 1.备份安装介质 1 2 3 4 5 6 # 中控机 1. 备份 install tar cvf installold.tar /data/install 2. 备份 src tar cvf srcold.tar /data/src 2.停服务 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # 停掉除了mysql，mongod之外的全部服务 下架官方 SaaS，访问蓝鲸 PaaS 平台，通过【开发者中心】-\u0026gt;【 S-mart 应用】下架所有官方 SaaS echo nginx paas_plugins bklog bknodeman bkmonitorv3 job bkiam bkssm usermgr cmdb paas appo appt gse license influxdb redis kafka zk es7 rabbitmq consul redis_sentinel | xargs -n 1 /data/install/bkcli stop echo nginx paas_plugins bklog bknodeman bkmonitorv3 job bkiam bkssm usermgr cmdb paas appo appt gse license influxdb redis kafka zk es7 rabbitmq consul redis_sentinel | xargs -n 1 /data/install/bkcli status pcmd -m appo systemctl stop docker systemctl stop bk-ci.target # pcmd -m appt systemctl stop docker （如有对应服务停掉） ./bkcli status all # 根据情况，可以清理一下执行记录、日志、告警等表 --- 查看各个数据库的大小 --- select table_schema as \u0026#39;数据库\u0026#39;, sum(table_rows) as \u0026#39;记录数\u0026#39;, sum(truncate(data_length/1024/1024/1024, 2)) as \u0026#39;数据容量(GB)\u0026#39;, sum(truncate(index_length/1024/1024/1024, 2)) as \u0026#39;索引容量(GB)\u0026#39; from information_schema.tables group by table_schema order by sum(data_length) desc, sum(index_length) desc; --- 查看某个数据库下面各个表大小 --- select table_schema as \u0026#39;数据库\u0026#39;, table_name as \u0026#39;表名\u0026#39;, table_rows as \u0026#39;记录数\u0026#39;, truncate(data_length/1024/1024, 2) as \u0026#39;数据容量(MB)\u0026#39;, truncate(index_length/1024/1024, 2) as \u0026#39;索引容量(MB)\u0026#39; from information_schema.tables where table_schema=\u0026#39;job_execute\u0026#39; order by data_length desc, index_length desc; 3.备份mysql 1 2 3 4 5 6 7 8 9 10 11 12 13 # 1.创建备份目录 mkdir -p /data/dbbak_$(date +%F)/mysql cd /data/dbbak_$(date +%F)/mysql # 2.备份数据库(注意：是否有拆分数据库部署场景) source /data/install/utils.fc #\u0026#34;bkdata_monitor_alert\u0026#34;监控告警库是否备份请自行评估。 mysql --login-path=default-root -e \u0026#34;show databases;\u0026#34; |grep -Ewv \u0026#34;Database|information_schema|performance_schema|mysql|sys|bksuite_common|*_bkt|test\u0026#34; \u0026gt; /tmp/databases; for name in `cat /tmp/databases` ;do echo $name ; mysqldump --login-path=default-root --skip-opt --create-options --default-character-set=utf8mb4 -R -E -q -e --single-transaction --no-autocommit --master-data=2 --max-allowed-packet=1G --hex-blob --databases $name \u0026gt; /data/dbbak_$(date +%F)/mysql/$name.sql ;done # 3.检查导出 sql 文件，确认无误后打包(建议不打包直接传输减少耗时) 4.备份mongodb 1 2 3 4 5 6 7 8 9 10 11 12 13 # 1.登陆至 MongoDB 机器，创建备份目录 source /data/install/utils.fc mkdir -p /data/dbbak_$(date +%F)/mongodb # 2.开始备份 MongoDB source /data/install/utils.fc # 备份 MongoDB 数据： # 可先登录mongodb查看有哪些数据库 mongodump --host $BK_MONGODB_IP -u $BK_MONGODB_ADMIN_USER -p $BK_MONGODB_ADMIN_PASSWORD --authenticationDatabase admin --archive=/data/dbbak_$(date +%F)/mongodb/cmdb.gz --gzip --db db_devops_ci_ctest # mongodump --host $BK_MONGODB_IP -u $BK_MONGODB_ADMIN_USER -p $BK_MONGODB_ADMIN_PASSWORD --authenticationDatabase admin --archive=/data/dbbak_$(date +%F)/mongodb/cmdb.gz --gzip --db cmdb # mongodump --host $BK_MONGODB_IP -u $BK_MONGODB_ADMIN_USER -p $BK_MONGODB_ADMIN_PASSWORD --authenticationDatabase admin --archive=/data/dbbak_$(date +%F)/mongodb/gse.gz --gzip --db gse 5.定开备份 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 备份ad登陆【如存在】 source /data/install/utils.fc cd /data/install pcmd -m paas \u0026#34;mkdir -p /data/confbak_$(date +%F)\u0026#34; pcmd -m paas \u0026#34;cp -a /data/bkee/open_paas/login/ee_login /data/confbak_$(date +%F)\u0026#34; pcmd -m paas \u0026#34;cp -a /home/bkee/open_paas{,_bak$(date +%F)}\u0026#34; #检查是否备份成功 pcmd -m paas \u0026#34;ls /data/confbak_$(date +%F)\u0026#34; # 备份微信配置【如存在】 source /data/install/utils.fc cd /data/install ssh $BK_PASS_IP cp -a /data/bkee/etc/uwsgi-open_paas-esb.ini /data/confbak_$(date +%F)/ 新环境 1.初始化蓝鲸平台 根据调研信息填写excel\n解压\n把基础平台包(cwbkee_product-xxxxx.tar.gz)全部放在安装目录（默认为/data)，包含监控、日志包(我的包名为嘉为蓝鲸4.0测试-Jackliang-20231221192656.tar.gz) 解压cwbkee_product.xxxxxxx.tar.gz到/data mv /data/src/cwbk_install-xxx.tar.gz /data/ tar xvf cwbk_install-xxx.tar.gz -C /data 上传表格到/data/cwbk_install/，删除旧表格 rm 2RP01.01_蓝鲸平台-部署前环境信息调研表-20240319.xlsx 如果表格有误，可以重新上传，并删除/data/cwbk_install/.run中的conv2txt，init_install_cfg 初始化\n1 2 3 cd /data/cwbk_install ./cwbk_install -init 2.迁移环境变量 1 2 3 4 5 6 7 8 9 10 1.初始化结束后进行install的处理，把旧环境的/data/install/bin/*拷贝到新环境 ***注意*** 此处环境变量有几处需要修改 cd /data/bin/install/bin/【01-05】 grep \u0026#34;10.11.27.***(旧环境ip地址)\u0026#34; *.env 这几个文件里面有旧环境的ip的env文件需要删除，否则会导致后面部署devops平台采用旧环境ip变量，无法访问 ***注意*** 2./root/.tag 处理，把旧环境的中控机/root/.tag/*拷贝到新环境，加锁chattr +i *，/root/.tag 是标记文件，如果存在./bkcli install bkenv就不会刷新变量 3.部署蓝鲸平台 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 中控机执行 cd /data/install # 初始化环境 ./bk_install common # 校验环境和部署的配置 ./health_check/check_bk_controller.sh # 检查/data/install/bin/【01-05】下文件的时间戳有没有变化，抽查一下环境变量有没有变化。符合预期再往下进行 # 部署 PaaS 平台 ./bk_install paas # 部署 SaaS 运行环境，正式环境及测试环境 ./bk_install app_mgr # 部署用户管理和权限中心saas ./bk_install saas-o bk_iam ./bk_install saas-o bk_user_manage 4.部署devops 准备部署文件\n部署脚本包 images/centos.gz ccheck-images.gz ctest-images.gz onlyoffice.gz bkrepo-dependency-check.gz bkrepo-trivy.gz cflow-image.gz pkgs/ software.zip db.tar.gz javadb.tar.gz dependency-check-db.tar.gz apache-pulsar-2.10.6-bin.tar.gz pulsar-io-rabbitmq-2.10.6.nar apache-doris-2.1.7-fe-bin-x64-noavx2.tgz apache-doris-2.1.7-be-bin-x64-noavx2.tgz kotlin-compiler-1.8.10-canway.zip cert.gw prd/CTest-6.0.x.tar.gz CTeam-6.0.x.tar.gz CComm-6.0.x.tar.gz 解压脚本\n1 tar -xf bkci_inst_v4.3.x.tar.gz -C /data 填写环境变量\n1 vim env.properties 执行脚本\n1 2 3 4 5 6 7 8 cd $CTRL_DIR ./bkcli install es7 ./bkcli start es7 \u0026amp;\u0026amp; ./bkcli status es7 ./bkcli install influxdb ./bkcli start influxdb \u0026amp;\u0026amp; ./bkcli status influxdb cd /data/bkci_inst ./devops_install.sh | tee `date +%Y-%m-%d_%H:%M:%S`.log 5.验证功能正常后停服 1 2 3 4 5 6 7 8 9 10 11 12 # 停服务 下架官方 SaaS，访问蓝鲸 PaaS 平台，通过【开发者中心】-\u0026gt;【 S-mart 应用】下架所有官方 SaaS echo nginx bknodeman bkmonitorv3 job bkiam bkssm usermgr cmdb paas appo appt gse license influxdb redis kafka zk es7 rabbitmq consul redis_sentinel | xargs -n 1 /data/install/bkcli stop echo nginx bknodeman bkmonitorv3 job bkiam bkssm usermgr cmdb paas appo appt gse license influxdb redis kafka zk es7 rabbitmq consul redis_sentinel | xargs -n 1 /data/install/bkcli status pcmd -m appo systemctl stop docker systemctl stop bk-ci.target # pcmd -m appt systemctl stop docker （如有对应服务停掉） ./bkcli status all 6.备份新平台数据 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # 备份 mysql mkdir -p /data/dbbak_$(date +%F)/mysql ALL_DBS=$(mysql --login-path=default-root -e \u0026#34;show databases;\u0026#34;|grep -Ewv Database) for i in ${ALL_DBS[@]}; do mysqldump --login-path=default-root --skip-opt --create-options --default-character-set=utf8mb4 -R -E -q -e --single-transaction --no-autocommit --master-data=2 --max-allowed-packet=1G --hex-blob --databases $i \u0026gt; /data/dbbak_$(date +%F)/mysql/$i.sql;done # 请检查导出是否正确 grep \u0026#39;CREATE DATABASE\u0026#39; bk_mysql_alldata.sql # 备份 mongodb # MongoDB 机器上执行 source /data/install/utils.fc ssh $BK_MONGODB_IP mkdir -p /data/dbbak_$(date +%F)/mongod source /data/install/utils.fc mongodump --host $BK_MONGODB_IP -u $BK_MONGODB_ADMIN_USER -p $BK_MONGODB_ADMIN_PASSWORD --oplog --gzip --out /data/dbbak_$(date +%F)/mongod # 备份 install # 中控机执行 cp -a -r /data/install /data/install_$(date +%Y%m%d%H%M) 7.迁移 mysql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # 创建/data/dbbak/mysqlold，将旧环境的数据传过来 mkdir -p /data/dbbak/mysqlold # 删除 mysql，注意cwlicense 等特有库 ALL_DBS=$(mysql --login-path=default-root -e \u0026#34;show databases;\u0026#34; | egrep -v \u0026#34;Database|information_schema|performance_schema|mysql|sys|bksuite_common\u0026#34;) # 如果需要装双agent则不需要导入bknodeman,bk_nodeman(可在ALL_DBS过滤) echo ${ALL_DBS[@]} for i in ${ALL_DBS[@]};do mysql --login-path=default-root -e \u0026#34;drop database $i\u0026#34;;done # 遇到cc-auto 等库名，可以用下面语句处理 for i in ${ALL_DBS[@]};do mysql --login-path=default-root -e \u0026#34;drop database \\`$i\\`\u0026#34;;done # 导入 mysql for i in /data/dbbak/mysqlold/*.sql;do mysql --login-path=default-root \u0026lt; \u0026#34;$i\u0026#34;;done # 如果客户端有超时限制，可以放到后台跑 nohup sh -c \u0026#39;for i in /data/dbbak/mysqlold/*.sql; do mysql --login-path=default-root \u0026lt; \u0026#34;$i\u0026#34;; done\u0026#39; \u0026amp; # 保持监控 while true; do date; ps -ef | grep \u0026#39;[m]ysql_old\u0026#39; | tee -a /tmp/mysql.log; sleep 60; done 8.迁移 mongodb 1 2 3 4 5 6 7 8 # 迁移数据前需要创建目录/data/dbbak/mongodbold/，将旧环境的数据传过来 mkdir -p /data/dbbak/mongodbold/ mongorestore --host $BK_MONGODB_IP -u $BK_MONGODB_ADMIN_USER -p $BK_MONGODB_ADMIN_PASSWORD --authenticationDatabase admin -d gse --drop --gzip --archive=/data/dbbak/mongodbold/db_devops_ci_ctest.gz # mongorestore --host $BK_MONGODB_IP -u $BK_MONGODB_ADMIN_USER -p $BK_MONGODB_ADMIN_PASSWORD --authenticationDatabase admin -d gse --drop --gzip --archive=/data/dbbak/mongodbold/gse.gz # 如果需要装双agent则不需要导入cmdb # mongorestore --host $BK_MONGODB_IP -u $BK_MONGODB_ADMIN_USER -p $BK_MONGODB_ADMIN_PASSWORD --authenticationDatabase admin -d cmdb --drop --gzip --archive=/data/dbbak/mongodbold/cmdb.gz 9.拉起服务 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # 数据迁移后数据库里的app_secret变为旧环境的app_secret,但新环境里是新生成的app_secret,此时前后端app_secret不匹配导致无法访问 # 更改新环境app_secret,gateway所在服务器 vim /data/bkee/ci/gateway/lua/init.lua # 修改内容如下： # cat 旧环境的/data/bkee/ci/gateway/lua/init.lua | grep -A 10 oauth 将app_secret复制粘贴过 oauth = { -- 对接蓝鲸权限中心才需要的配置 ip = \u0026#34;10.11.27.174\u0026#34;, env = \u0026#34;prod\u0026#34;, port = \u0026#34;5000\u0026#34;, host = \u0026#34;bkssm.service.consul\u0026#34;, url = \u0026#34;/api/v1/auth/access-tokens\u0026#34;, -- 接口路径 app_code = \u0026#34;bk_ci\u0026#34;, app_secret = \u0026#34;981ccb3c-63ed-4a10-b60f-0daefba5d92a\u0026#34; # 改为旧环境的app_secret }, systemctl restart bk-ci-gateway.service # 启动服务以及后续处理 echo nginx bknodeman bkmonitorv3 job bkiam bkssm usermgr cmdb paas appo appt gse license influxdb redis kafka zk es7 rabbitmq consul redis_sentinel | xargs -n 1 /data/install/bkcli start systemctl start bk-ci.service 恢复后处理 vteam未注册consul服务\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 vim /data/bkee/ci/vteam/BOOT-INF/classes/application-bklatest.yml # 修改内容如下： cloud: consul: # host: 192.168.104.81 # Consul 服务器地址（填写实际运行 Consul 的节点 IP，如 10.11.27.175） host: 10.11.27.175 # Consul 服务端口（默认 8500） port: 53 discovery: # 服务名（必须为 vteam-devops，与 DNS 解析的服务名匹配） serviceName: vteam-devops # 服务标签（必须包含 devops，与 DNS 解析的 devops. 前缀匹配） tags: devops # 启用服务注册（必须设为 true） register: true # 注册时使用服务所在节点的实际 IP（10.11.27.171） preferIpAddress: true # 服务端口（若未指定，默认使用应用启动端口，如 8080，确保端口未被占用） port: 8080 systemctl restart bk-ci-vteam.service 登录异常\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 报错 APP Secret verification failed, pelase confirm if the APP Secret and APP Code [bk_app_code=bk_paas] match # 处理方法一 grep -nri APP_SECRET /data/install/bin bkapigw.env:2:BK_APIGW_APP_SECRET=f7e2a775-c9c6-4577-9a1a-42f7487744f7 ... 更新：app_token为原来的 use open_paas; update esb_app_account set app_token=\u0026#39;cd740dec-5d34-4c60-8952-429f2f08ab1e\u0026#39; where app_code=\u0026#39;bk_paas\u0026#39;; ... # 处理方法二 恢复表 open_paas.esb_app_account 开发者中心注册的appo服务器信息不对，需要重新激活\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 方法一 # 从备份新平台备份数据找到 engine_servers 表记录数据。 mysql\u0026gt;use open_paas mysql\u0026gt;delete from engine_hosting_ships where bk_server_id\u0026gt;0; mysql\u0026gt;delete from engine_servers where id \u0026gt;0; ## 如下数据来源于备份新平台备份engine_servers表数据，可以从备份sql文件提取。 mysql\u0026gt;INSERT INTO `engine_servers` VALUES (1,\u0026#39;\u0026#39;,\u0026#39;a2f127ec19e741e48c3eb1a3f3d8138b\u0026#39;,\u0026#39;39aba83ca52b4639bf525a18b764fea6\u0026#39;,\u0026#39;10.11.24.146\u0026#39;,\u0026#39;4245\u0026#39;,\u0026#39;app\u0026#39;,\u0026#39;\u0026#39;,1,\u0026#39;2023-08-25 13:01:08.932744\u0026#39;,\u0026#39;2023-08-25 13:01:12.489231\u0026#39;,\u0026#39;8010\u0026#39;,\u0026#39;28:6E:D4:8A:17:1C\u0026#39;); # 方法二 1、从开发者中心添加新的服务器信息，生成新的服务器ID和Token 2、修改appo 机器 /data/bkee/etc/paas_agent_config.yaml ，填写新的信息 3、重启 appo，页面点击激活 激活失败\n1 2 3 4 5 6 7 8 9 # 问题 /data/bkee/logs/paasagent/agent.log提示 1305102 sid or token is not valid, please check # 处理 /data/bkee/etc/paas_agent_config.yaml 检查是否一致，删除并重新生成 、激活 没有可用./bkcli install appo pcmd -m appo \u0026#34;systemctl restart bk-paasagent\u0026#34; 若有SAAS打开异常，请尝试重新部署，部署请检查环境变量，如ITSM、日志平台的REDIS配置IP是否准确\n1 2 3 4 5 mysql\u0026gt;use open_paas mysql\u0026gt;delete from paas_app_envvars; ## 如下数据来源于备份新平台备份paas_app_envvars表数据,可以从备份sql文件提取。 mysql\u0026gt; INSERT INTO `paas_app_envvars` VALUES (3,\u0026#39;bk_nodeman\u0026#39;,\u0026#39;all\u0026#39;,\u0026#39;BKAPP_GSE_SERVER_WAN_IPLIST\u0026#39;,\u0026#39;10.11.24.146\u0026#39;,\u0026#39;Update by install script\u0026#39;), (109,\u0026#39;bk_monitorv3\u0026#39;,\u0026#39;all\u0026#39;,\u0026#39;BKAPP_ENABLE_SHARED_FS\u0026#39;,\u0026#39;b\\\u0026#39;1\\\u0026#39;\u0026#39;,\u0026#39;set by S-mart App\u0026#39;); 检查外链应用是否正确\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 开发者中心 -\u0026gt; 外链应用 -\u0026gt; 修改所有外链应用 [基本信息] 栏中的 [系统链接] 为自身环境正确的地址，保存即可 #检查域名是否与新平台一致 mysql\u0026gt;use open_paas mysql\u0026gt;select code, external_url from paas_app where code in (\u0026#39;bk_job\u0026#39;, \u0026#39;bk_cmdb\u0026#39;); +---------+--------------------------+ | code | external_url | +---------+--------------------------+ | bk_cmdb | http://cmdb.bkee4.com:80 | | bk_job | http://job.bkee4.com:80 | +---------+--------------------------+ 2 rows in set (0.00 sec) ## 如不一致，需要修改。 ## job mysql\u0026gt;update paas_app set external_url=\u0026#34;xxxxx\u0026#34; where code=\u0026#39;bk_job\u0026#39; ## cmdb mysql\u0026gt;update paas_app set external_url=\u0026#34;xxxxx\u0026#34; where code=\u0026#39;bk_cmdb\u0026#39; 如果有监控，重新部署，修改influxdb\n1 2 3 4 5 6 7 8 9 # 更新influxdb ip # bkmonitorv3_alert.metadata_influxdbhostinfo mysql\u0026gt;use bkmonitorv3_alert; mysql\u0026gt;delete from metadata_influxdbclusterinfo; mysql\u0026gt;delete from metadata_influxdbhostinfo; mysql\u0026gt;delete from metadata_clusterinfo; ## 如下数据来源于备份新平台备份metadata_influxdbhostinfo表数据,可以从备份sql文件提取。 mysql\u0026gt;INSERT INTO `metadata_influxdbhostinfo` VALUES (\u0026#39;INFLUXDB_IP0\u0026#39;,\u0026#39;10.11.24.148\u0026#39;,8086,\u0026#39;admin\u0026#39;,\u0026#39;aes_str:::LrBZ5bXwhS/f6RKHWGz3ZGpFAr12hKYoI6zJ3ejTJiQ=\u0026#39;,\u0026#39;system auto add.\u0026#39;); 两套环境rabbitmq admin密码不一样，需要修改\n1 2 3 密码 echo $BK_RABBITMQ_ADMIN_PASSWORD mysql\u0026gt;update engine_third_servers set server_info=\u0026#39;{\u0026#34;ip_address\u0026#34;: \u0026#34;rabbitmq.service.consul\u0026#34;, \u0026#34;ip_port\u0026#34;: \u0026#34;15672\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;sUu7bZuxU8S2\u0026#34;}\u0026#39;; 清理cmdb的topo数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 清理旧的topo数据 # 登录 MongoDB mongo \\ -u $BK_CMDB_MONGODB_USERNAME \\ -p $BK_CMDB_MONGODB_PASSWORD \\ mongodb://$LAN_IP:$BK_CMDB_MONGODB_PORT/cmdb \\ --authenticationDatabase cmdb # 执行清理 db.cc_ServiceTemplate.remove({\u0026#34;bk_biz_id\u0026#34;:2})\tdb.cc_ProcessTemplate.remove({\u0026#34;bk_biz_id\u0026#34;:2}) db.cc_SetTemplate.remove({\u0026#34;bk_biz_id\u0026#34;:2}) db.cc_SetBase.remove( {$and:[ {\u0026#34;bk_set_id\u0026#34;:{$gt:2}}, {\u0026#34;bk_biz_id\u0026#34;:{$eq:2}} ]}, {\u0026#34;bk_set_id\u0026#34;:1,\u0026#34;bk_set_name\u0026#34;:1,\u0026#34;bk_biz_id\u0026#34;:1,\u0026#34;bk_biz_name\u0026#34;:1} ); admin 登录不上，提示密码不对\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 修改admin密码 workon usermgr-api export DJANGO_SETTINGS_MODULE=\u0026#34;bkuser_core.config.overlays.prod\u0026#34; export BK_FILE_PATH=${INSTALL_PATH}/usermgr/cert/saas_priv.txt python manage.py shell from bkuser_core.profiles.models import Profile from django.contrib.auth.hashers import make_password admin = Profile.objects.get(username=\u0026#39;admin\u0026#39;, domain=\u0026#39;default.local\u0026#39;) admin.password = make_password(\u0026#34;blueking\u0026#34;) admin.save() # 修改默认登录域 bk_user.categories_profilecategory ","date":"2025-08-05T11:02:00+08:00","permalink":"https://Logic0528.github.io/p/631%E8%BF%81%E7%A7%BB%E6%96%B9%E6%A1%88/","title":"631迁移方案"},{"content":"一.命令类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 yum install lrzsz rz #上传文件，500M以下 #远程文件同步 rsync -avz -e ssh /path/to/source/ user@remote:/path/to/destination/ # -a 归档模式，递归并保留文件属性 # -v 详细输出模式 # -z 传输时压缩文件数据 find /path -name \u0026#34;filename\u0026#34; #vim :1,100yank #复制第一到第一百行内容 :100 #跳转至第一百行 :%d #删除全部 :5,10d sed -i \u0026#39;s/old/new/g\u0026#39; file.txt kubectl -n spline expose deployment front-end --type=NodePort -port=80 --target-port=80 --name=front-end-svc # kubectl expose 和 kubectl create service都能创建service，expose专门用于 “为已有工作负载（如 Deployment、Pod、ReplicaSet 等）自动创建 Service” 二.问题类 1 2 3 /data/bkci_inst/script/ci.env #环境变量 /data/bkee/logs/ci/ #日志 1.mongodb未安装，导致脚本后续用户创建失败认证失败等一系列问题 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # 安装 MongoDB sudo rpm -ivh /data/myhttpd/data/cw_yum_3.0/mongodb-org-server-4.4.15-1.el8.x86_64.rpm sudo rpm -ivh /data/myhttpd/data/cw_yum_3.0/mongodb-org-shell-4.4.15-1.el8.x86_64.rpm sudo rpm -ivh /data/myhttpd/data/cw_yum_3.0/mongodb-database-tools-rhel70-x86_64-100.10.0.rpm # 编辑配置文件 sudo vim /etc/mongod.conf # bindIp: 0.0.0.0 # 找到并注释掉认证配置 # security: # authorization: enabled # 注释这两行，临时关闭认证 # 重启 MongoDB 服务 sudo systemctl start mongod # 查看变量里存储的密码，用户名一般就是root,下面进行手动创建用户 echo $BK_MONGODB_ADMIN_PASSWORD # 无密码连接 MongoDB（此时关闭了认证） mongo mongodb://10.11.27.174:27017/admin // 切换到 admin 数据库（管理员用户必须创建在 admin 库） use admin // 创建 root 用户，密码使用环境变量中的 JYMFo_iY9zBj db.createUser({ user: \u0026#34;root\u0026#34;, // 与 BK_MONGODB_ADMIN_USER 一致 pwd: \u0026#34;JYMFo_iY9zBj\u0026#34;, // 与 BK_MONGODB_ADMIN_PASSWORD 一致 roles: [{ role: \u0026#34;root\u0026#34;, db: \u0026#34;admin\u0026#34; }] // 授予最高权限（确保能创建其他用户） }) // 验证用户是否创建成功 db.getUser(\u0026#34;root\u0026#34;) // 应输出用户信息，确认密码哈希值存在 // 退出终端 exit 2.yum源相关问题 yum源安装失败 1 2 3 4 cat /etc/sysconfig/myhttpd.yaml #看端口是否是8070 baseurl= ip:8070///文件路径 #修改后重启myhttpd服务 非中控机无法通过网络访问中控机yum源 1 2 cat /etc/yum.repo/cwyum.repo # 查看是否IP地址后是否有8070端口，该路径下全部repo文件都需要改 缺少yum源，可能是缺少麒麟的yum源 1 2 3 4 5 6 cd /etc/yum.repo/ ls # 有个bak文件夹里面有麒麟的yum源，mv ky.repo ../ yum clean all yum makecache 3.pulsar脚本访问java路径不存在 路径不存在或是后面的mysql等服务没起来，原因可能是文件路径与脚本不匹配导致的\n1 2 3 4 5 6 7 8 9 10 11 # 正确设置 JAVA_HOME # 获取 JAVA_HOME 的正确值 readlink -f $(which java) | sed \u0026#34;s:bin/java::\u0026#34; # 更新环境变量 sudo vi /etc/profile.d/java.sh # 根据实际输出结果修改下面的路径 export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.261.b12-1.el7_7.x86_64/ export PATH=$JAVA_HOME/bin:$PATH # 刷新环境变量 source /etc/profile.d/java.sh 该问题后续会导致link_pulsar_to_mq租户不存在\n1 2 3 4 5 6 7 8 9 10 cd /data/apache-pulsar-2.10.6 bin/pulsar-admin tenants create cpack bin/pulsar-admin namespaces create cpack/bkrepo # 查看所有租户 bin/pulsar-admin tenants list # 查看 cpack 租户下的命名空间 bin/pulsar-admin namespaces list cpack 4.cousul无法访问 缺少client_addr,默认值为127.0.0.1，即仅允许本地访问8500端口，导致应用通过10.11.26.166：8500访问时被拒绝\n1 2 3 4 5 6 7 8 # 编辑配置文件 vi /etc/consul.d/consul.json { \u0026#34;client_addr\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, // 新增此行，允许所有接口访问 HTTP 接口 \u0026#34;bind_addr\u0026#34;: \u0026#34;10.11.26.166\u0026#34;, ... } 5.mongodb,redis,mysql,zookeeper等服务启动失败 很大概率是因为目录权限问题，或者是你在重启机器后发现原来正常的mysql启动不起来了 正确属组应该是mysql:mysql而不是blueking:blueking\n1 2 3 4 5 6 7 8 9 10 # 查看当前配置文件权限 ls -l /etc/redis/sentinel-default.conf # 设置权限：允许 redis 用户读写配置文件 sudo chown redis:redis /etc/redis/sentinel-default.conf sudo chmod 644 /etc/redis/sentinel-default.conf # 确保可读可写 # 确认权限已生效 ls -l /etc/redis/sentinel-default.conf # 输出应显示：-rw-r--r-- 1 redis redis ... 6.Mongodb从节点加入不了集群 1 2 3 4 5 6 7 8 Client view of cluster state is {type=REPLICA_SET, servers=[]} \u0026gt; rs.status0 “ok” : 0, \u0026#34;errmsg\u0026#34;:\u0026#34;not running with --replSet\u0026#34;, \u0026#34;code\u0026#34; : 76, \u0026#34;codeName\u0026#34; : \u0026#34;NoReplicationEnabled\u0026#34; \u0026gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # 修改配置文件(通常为/etc/mongod.conf)： replication: replSetName：\u0026#34;你的副本集名称\u0026#34; # 例如：rs0 # 用管理员账号登录MongoDB mongo --host 10.11.27.172 -u root -p JYMFo_iY9zBj -authenticationDatabase admin # 初始化副本集（单节点配置） rs.initiate({ _id：\u0026#34;你的副本集名称\u0026#34;， members:[ ｛_id: 0,host: \u0026#34;10.11.27.172:27017\u0026#34;} ] #修改应用配置，添加replicaSet参数: spring: data: mongodb: uri：mongodb://root:JYMFo_iY9zBj@10.11.27.172:27017/你的数据库名?replicaSet=你的副本集名称 7.安装es7失败 1 2 # 删除无效插件目录 sudo rm -rf /usr/share/elasticsearch/plugins/elasticsearch-analysis-ik- 8.vteam未注册consul服务 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 vim /data/bkee/ci/vteam/BOOT-INF/classes/application-bklatest.yml # 修改内容如下： cloud: consul: # host: 192.168.104.81 # Consul 服务器地址（填写实际运行 Consul 的节点 IP，如 10.11.27.175） host: 10.11.27.175 # Consul 服务端口（默认 8500） port: 53 discovery: # 服务名（必须为 vteam-devops，与 DNS 解析的服务名匹配） serviceName: vteam-devops # 服务标签（必须包含 devops，与 DNS 解析的 devops. 前缀匹配） tags: devops # 启用服务注册（必须设为 true） register: true # 注册时使用服务所在节点的实际 IP（10.11.27.171） preferIpAddress: true # 服务端口（若未指定，默认使用应用启动端口，如 8080，确保端口未被占用） port: 8080 systemctl restart bk-ci-vteam.service 9.mongod连接超时，副本集配置不匹配 1 2 3 4 5 6 7 8 9 10 11 12 # mongod配置的一个是副本集连接，但是主节点没有副本集 # 快速解决 vim /data/bkee/etc/ci/application-cteam.yml data: mongodb: uri: mongodb://ctest:ctest@10.11.27.174:27017/db_devops_ci_ctest?maxPoolSize=50\u0026amp;slaveOk=true\u0026amp;safe=true\u0026amp;w=1\u0026amp;wtimeoutMS=2000 # uri: mongodb://ctest:ctest@10.11.27.174:27017/db_devops_ci_ctest?maxPoolSize=50\u0026amp;replicaSet=rs0\u0026amp;slaveOk=true\u0026amp;safe=true\u0026amp;w=1\u0026amp;wtimeoutMS=2000 field-naming-strategy: org.springframework.data.mapping.model.SnakeCaseFieldNamingStrategy auto-index-creation: true # 删除replicaset=rs0或者是去主节点上初始化副本集rs0 10.批量启动和停止服务 1 2 3 4 5 6 7 8 9 [Unit] # 关键配置：将服务与目标关联 PartOf=bk-ci.target # 指定服务在目标启动时自动启动 WantedBy=bk-ci.target systemctl stop bk-ci.service # 由于partof属性，相关联的服务也都停止 systemctl start bk-ci.service # 由于wantedby属性，相关联的服务也都启动 ","date":"2025-07-31T14:49:19+08:00","permalink":"https://Logic0528.github.io/p/%E4%B8%83%E6%9C%88%E5%AE%9E%E4%B9%A0%E6%80%BB%E7%BB%93/","title":"七月实习总结"},{"content":"CKA认证 一.PVC 注意：解题前需连接到对应的主机。 题目： mariadb namespace 中的MariaDB Deployment 被误删除。请恢复该deployment并确保数据持久性。\n请按照以下步骤:\n如下规格在mariadb namespace 中创建名为 mariadb 的PVC： - 访问模式为ReadWriteOnce(依题目) - 存储为250Mi 集群中现有一个PersistentVolume,您必须使用现有的PV。 编辑位于~/mariadb-deployment.yaml 的 MariaDB Deployment 文件，以使用上一步中创建的PVC。 将更新的 Deploment 文件应用到集群 确保MariaDB Deployment正在运行且稳定 搭建模拟环境 正式考试中不需要搭建模拟环境，只需按照下面的解题步骤解题即可。在自己电脑上可以搭建个模拟环境模拟考试，同时起到练习的作用\n1.创建本地供应商\n​\t动态制备 PV ,我们需要一个provisioner,一个 storageclass,在local-path-storage命名空间下。\n​\tProvisioner 是一个核心组件，负责动态创建和管理持久化存储，Provisioner允许在Pod请求存储时自动创建所需的存储卷，无需管理员预先创建，这依赖于StorageClass资源的定义\n1 kubectl create ns local-path-storage local-path-provisioner.yaml\nlocal-path-storage.yaml\n1 2.创建namespace\n1 kubectl create ns mariadb 3.mariadb-pv.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: PersistentVolume metadata: name: mariadb-pv spec: capacity: storage:250Mi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClasName: local-path hostPath: path: /mnt/data/mariadb 1 2 kubectl apply -f mariadb-pv.yaml kubectl get pv 4.mariadb-deployment.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: apps/v1 kind: Deployment metadata: name: mariadb namespace: mariadb spec: replicas: 1 selector: matchLabels: app: mariadb template: metadata: labels: app: mariadb spec: containers: - name: mariadb image: mariadb:10.5 imagePullPolicy: IfNotPresent ports: - containerPort: 3306 env: - name: MYSQL_ROOT_PASSWORD value: \u0026#34;rootpassword\u0026#34; 解题 解题即是在考试环境下需要做的操作\n切换到指定节点\n检查pv,以及pv用到的storageclass\n1 2 kubectl get pv kubectl get sc 创建pvc\n1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mariadb ##按题目要求修改 namespace: mariadb ##按题目要求修改 spec: storageClassName: local-path ##第二步查到的sc accessModes: - ReadWriteOnce ##按题目要求修改 resources: requests: storage: 250Mi ##按题目要求修改 1 2 kubectl apply -f pvc.yaml kubectl get pvc -n mariadb 配置Deployment关联PVC\n1 vim ~/mariadb-deployment.yaml 增加volumes和volumeMounts部分\n1 2 3 4 5 6 7 volumes： - name:mariadb-data persistentVolumeClaim: claimName: \u0026#34;mariadb\u0026#34; volumesMounts: - name: mariadb-data mountPath: /var/lib/mysql ##按题目要求修改 完整的deployment.yaml文件如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion: apps/v1 kind: Deployment metadata: name: mariadb namespace: mariadb # 指定 namespace spec: replicas: 1 selector: matchLabels: app: mariadb template: metadata: labels: app: mariadb spec: containers: - name: mariadb image: mariadb:10.5 # 以考试为准，无需自己指定 imagePullPolicy: IfNotPresent ports: - containerPort: 3306 env: - name: MYSQL_ROOT_PASSWORD value: \u0026#34;rootpassword\u0026#34; # 可按题目或需求设置 volumeMounts: - name: mariadb-da mountPath: /var/lib/mysql # 数据挂载路径 volumes: - name: mariadb-data persistentVolumeClaim: claimName: \u0026#34;mariadb\u0026#34; 检查Deployment和Pod运行状态\n1 2 kubectl apply -f ~/mariadb-deployment.yaml kubectl get deployment -n mariadb Pod状态应为Running\nexit退出当前节点\n二.四层代理Service 题目：\n重新配置 spline namespace 中现有的 front-end Deployment, 以公开现有容器nginx的端口80/TCP。 创建一个名为 front-end-svc 的新Service，以公开容器端口 80/TCP 配置新的Service，以通过NodePort公开各个Pod 搭建模拟环境 创建namespace\n1 kubectl create ns spline front-end Deployment\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 vim 2-svc-deployment.yaml apiVersion: v1 kind: Deployment metadata: name：front-end namespace: spline spec: replicas: 1 selector: matchLabels: app: front-end template: metadata: labels: app: front-end spec: containers: - name: nginx image: nginx:1.25 imagePullPolicy: IfNotPresent 1 kubectl apply -f 2-svc-deploy.yaml 解题 连接指定节点\n修改 front-end deployment 配置，添加端口配置\n在name: nginx下方新增端口配置\n1 2 3 4 5 kubectl -n spline edit deployment front-end ports: - containerPort: 80 protocol: TCP 注意：\n正确缩进，确保ports在name: nginx同层级下 执行 kubectl -n spline edit deployment front-end 之后若提示如下，说明修改成功:deploymnet.apps/front-end edited 暴露 Deployment，创建 NodePort 类型的Service，执行命令：\n1 kubectl -n spline expose deployment front-end --type=NodePort -port=80 --target-port=80 --name=front-end-svc 看到service/front-end-svc exposed,表示创建service成功 参数说明：\n\u0026ndash;type=NodePort : 依题目要求 \u0026ndash;port=80 ：Service监听端口 \u0026ndash;target-port : 容器内应用端口 \u0026ndash;name=front-end-svc : Service名称 查看Service\n1 2 3 4 kubectl -n spline get svc front-end-svc -o wide #测试访问，用查到的cluster-ip curl 10.96.101.208:80 exit退出当前节点\n三.七层代理资源Ingress 题目：创建新的 Ingress 资源，具体如下： 名称： echo-hello Namespace: sound\n使用 Service 端口 8080 在 http://example.org/echo-hello 上公开 echoserver-service 的 Service\n可以使用以下命令检查 echoserver-service Service 的可用性，该命令应返回 hello world\n搭建模拟环境 安装Ingress-nginx-controller\n1 2 3 4 5 #ingress-deploy.yaml kubectl apply -f ingress-deploy.yaml 创建命名空间\n1 kubectl create ns sound 创建pod\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 vim 3-ingress-deploy-pod.yaml apiVersion: v1 kind: Deployment metadata: name: echoserver namespace: sound spec: repliacs: 1 selector: matchLabels: app: echoserver template: metadata: labels: app: echoserver spec: containers: - name: echoserver image: docker.io/library/tomcat:8.5.34-jre8-alpine imagePullPolicy: IfNotPresent ports: - containerPort:8080 1 kubectl apply -f 3-ingress-deploy-pod.yaml 创建service\n1 vim 3-ingress-nsvc.yaml 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: Service metadata: name: echoserver-service namespace: sound spec: selector: app: echoserver ports: - protocol: TCP port: 8080 targetPort: 8080 1 2 3 4 5 kubectl apply -f 3-ingress-svc.yaml kubectl get svc -n sound curl ip:8080 #返回网页内容，说明可以通过svc代理pod 解题 连接到指定节点\n查询 IngressClass ,确认 class 名字\n1 2 3 4 5 6 kubectl get ingressclasses.networking.k8s.io #kubectl get ingclass # 等效于原命令，更简洁 #查询结果如下 NAME CONTROLLER nginx k8s.io/ingress-nginx 编写 ingress.yaml\n1 vim ingress.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion: v1 kind: Ingress metadata: name: echo-hello namespace: sound annotations: nginx.ingress.kubernetes.io/rewrite-target: / # 将请求路径重写为根路径 spec: ingressClassName: nginx # 指定使用nginx类型的Ingress控制器 rules: - host: example.org # 匹配的域名 http: # HTTP路由规则 paths: - path: /echo-hello # 匹配的路径 pathType: Prefix # 路径匹配类型为前缀匹配 backend: service: name: echoserver-service # 转发到的服务名称 port: number: 8080 # 转发到的服务端口 创建 ingress 资源\n1 2 3 4 5 6 7 kubectl apply -f ingress.yaml kubectl get ingress -n sound #验证Ingress配置 curl http://example.org/echo-hello exit 四.网络策略NetworkPolicy 题目：从提供的YAML文件中选择并应用适当的NetworkPolicy。确保所选的NetworkPolicy不会过于宽松，同时允许运行在fronted和backend namespace 中的 fronted 和 backend Deployment之间的通信。\n首先，分析fronted 和 backend Deployment,了解其通信需求，以便确定需要应用的NetworkPolicy.\n接着，检查位于 ~/netpol 文件夹中的 NetworkPolicy YAML实例。\n最后，应用一个NetworkPolicy,以启用 frontend 和 backend Deployment 之间的通信，但不要使其过于宽松。\n搭建模拟环境 创建命名空间\n1 2 kubectl create ns frontend kubectl create ns backend 在 fronted 和 backend 下部署pod\n1 vim 4-fronted.aml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: apps/v1 kind: Deployment metadata: name: fronted-app namespace: frontend spec: replicas: 1 selector: matchLabels: app: frontend template: metadata: labels: app: frontend spec: containers: - name: nginx image: nginx:1.25 imagePullPolicy: IfNotPresent ports: - containerPort:80 1 kubectl apply -f 4-frontend.yaml 1 vim 4-backend.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: apps/v1 kind: Deployment metadata: name: backend-app namespace: backend spec: replicas: 1 selector: matchLabels: app: backend template: metadata: labels: app: backend spec: containers: - name: nginx image: nginx:1.25 imagePullPolicy: IfNotPresent ports: - containerPort: 80 1 kubectl apply -f 4-backend.yaml 创建backend默认拒绝策略\n1 vim networkpolicy.yaml 1 2 3 4 5 6 7 8 apiVersion: networking.k8s.io/v1 kind： NetworkPolicy metadata: name: default-deny-all spec: podSelector: {} policyTypes: - Ingress 解题 连接到指定节点\n检查fronted和backend两个namespace的标签\n1 kubectl get ns frontend backend --show-labls 检查frontend和backend两个namespace下所有pod的标签\n1 2 kubectl -n frontend get pod --show-labels kubectl -n backend get pod --show-labels 查看默认拒绝策略 检查backend namespace中是否有默认拒绝的Network Policy\n1 kubectl get networkpolicy -n backend 查看题目给定的NetworkPolicy示例，并选择最合适的策略\n1 2 3 4 5 6 cat ~/netpol/netpol1.yaml cat ~/netpol/netpol2.yaml cat ~/netpol/netpol3.yaml ## 根据题目要求选择合适的应用 kubectl apply -f *** 五.配置管理中心configmap 题目：\n名为nginx-static-test的NGINX Deployment 正在nginx-static-test namespace 中运行，它通过名为 nginx-static-config 的ConfigMap进行配置。\n更新nginx-static-config 这个configmap 以仅允许 TLSv1.3连接\n注意：您可以根据需要重新创建，启动或扩展资源\n您可以使用以下命令测试更改：\n1 curl -k --tls-max 1.2 https://web.k8snginx.local 搭建模拟环境 创建命名空间\n1 kubectl create ns nginx-static-test 资源准备：ConfigMap\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion: v1 kind: ConfigMap metadata: name: nginx-static-config namespace: nginx-static-test data: nginx.conf:{ worker_processes 1; envnts{ worker_connectionss 1024; } http{ # SSL配置加在这里，http块内，server块外 ssl_protocols TLSv1.2; ssl_ciphers \u0026#39;ECHDE-RSA-***\u0026#39; ssl_perfer_srever_ciphers on; } server{ listen 80; server_name web.k8snginx.local; localtion/{ root /usr/share/nginx/html; index index.html index.htm; } } } kubectl apply -f nginx-static-config.yaml 资源准备：Deployment\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-static-test namespace: nginx-static-test spec: replicas: 1 selector: matchlabels: app: nginx-static-test template: metadata: labels: app: nginx-static-test spec: containers: - name: nginx image: nginx:1.25 volumeMounts: - name: nginx-config mountPath: /etc/nginx/nginx.conf subPath: nginx.conf volumes: - name: nginx-config configMap: name: nginx-static-config 1 2 3 kubectl apply -f nginx-static-deployment.yaml kubectl get pods -n nginx-static-test 解题 连接到指定节点\n导出现有的nginx-static-config ConfigMap\n1 kubectl -n nginx-static-test get configmap nginx-static-config -o yaml \u0026gt; nginx-static-config.yaml 修改ConfigMap的ssl_protocols配置\n1 2 3 4 5 6 vim nginx-static-config.yaml data: nginx.conf: #其他配置... ssl_protocols TLSv1.3; # 按题目要求修改 先删除旧的ConfigMap,再创建新的configmap\n1 2 3 kubectl delete configmap nginx-static-test -n nginx-static-test kubectl apply -f nginx-static-test -n nginx-static-test 重启nginx-static-test Deployment\n1 2 3 kubectl -n nginx-static-test rollout restart deployment nginx-static-test kubectl get pods -n nginx-static-test 测试TLSv1.2连接，按照目前修改，访问是失败的说明配置正确\n1 2 3 curl -k --tls-max 1.2 https://web.k8snginx.local exit 六.存储类storageclass 题目：\n首先，为名为rancher.io/local-path的现有制备器，创建一个名为test-local-path的新Storageclass\n将卷绑定模式设置为 WaitForFirstConsumer\n接下来，将test-local-path Storageclass设置为默认的Storageclass\n请勿修改任何现有的Deployment和PVC\n解题 连接至指定节点\n创建正确的Storageclass Yaml文件\n1 2 3 4 5 6 7 8 9 10 11 12 vim sc.yaml apiVeison: storage.k8s.io/v1 kind: Storageclass metadata: name: test-local-path annotations: storageclass.kubernetes.io/is-default-class:\u0026#34;true\u0026#34; provisioner: rancher.io/local-path # 制备器的名字 volumeBindingMode: WaitForFirstConsumer # 卷绑定模式依题目要求 kubectl apply -f sc.yaml 验证test-local-path是否是默认的Storageclass\n1 2 3 4 5 6 7 kubectl get storageclass # 输出如下 NAME　PROVISIONER DEFAULT test-local-path(default) rancher.io/local-path true # 提示是true说明是对的 exit退出当前节点\n七.HPA（Pod水平自动扩缩容） 题目：\n在 autoscale namespace 中创建一个名为 nginx-server 的新 HorizontalPodAutoscaler(HPA),此HPA必须定位到 autoscale namespace 中名为 nginx-server的现有Deployment\n将HPA设置为每个Pod的CPU使用率旨在50%。将其配置为至少有一个Pod，且不超过4个Pod。此外，将缩小稳定窗口设置为30秒\n搭建模拟环境 解题 连接至指定节点\n创建hpa\n1 kubectl autoscale deployment nginx-server --cpu-percent=50 --min=1 --max=4 -n autoscale 修改HPA的稳定窗口为30s\n1 2 3 4 5 6 7 8 kubectl edit horizontalpodautoscaler nginx-server -n autoscale spec: maxReplicas: 4 minReplicas: 1 behavior: # 新增字段 scaleDown: stabilizationWindowsSeconds: 30 exit退出当前节点\n八.Pod优先级PriorityClass 题目：\n为用户工作负载创建一个名为 high-priority 的新 PriorityClass,其值比用户定义的现有最高优先级类值小一。修改在priority namespace 中运行的现有 busybox-logger Deployment，以使用 high-priority\n确保 busybox-logger Deployment 在设置了新优先级类后成功部署\n搭建模拟环境 创建namespace\n1 kubectl create ns priority 创建PriorityClass资源\n1 2 3 4 5 6 7 8 9 10 vim priority-init.yaml apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: max-priority value: 1000000000 globalDefault: false kubectl apply -f priority-init.yaml 创建busybox-logger这个deployment资源\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 vim busybox-logger.yaml apiVersion: apps/v1 kind: Deployment metadata: name: busybox-logger namespace: priority spec: replicas: 1 selector: matchLabels: app: busybox-logger template: metadata: labels: app: busybox-logger spec: containers: - name: busybox image: busybox:1.28 imagePollPolicy: IfNotPresent command: [\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;while true;do echo \u0026#39;Logging...\u0026#39;;sleep 5;done\u0026#34;] kubectl apply -f busybox-logger.yaml kubectl get pods -n priority 解题 连接到指定节点\n查看当前集群存在的PriorityClass\n1 kubectl get priorityclass 创建新的PriorityClass\n1 2 3 4 5 6 7 8 9 10 vim priority.yaml apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: high-priority value: 999999999 globalDefault: false kubectl apply -f priority.yaml 验证PriorityClass是否创建成功\n1 kubectl get priorityclass 在线修改Deployment的priorityClassName\n1 2 3 4 5 kubectl edit deployment busybox-logger -n priority # 找到spec.template.spec部分，通常在dnsPolicy:ClusterFirst上方 # 添加以下内容 priorityClassName: high-priority exit退出当前节点\n九.resource配置 cpu 和 内存请求和限制 题目：\n您管理一个 WordPress 应用程序。由于资源的请求过高，导致某些pod无法启动\nrelative-fawn namesapce 中的 WordPress 应用程序包含： 具有 3 个副本的 WordPress Deployment\n按照如下方式调整所有 pod 的资源请求 - 将节点资源平均分配给这 3 个 Pod - 为每个 Pod 分配公平的 CPU 和内存份额 - 添加足够的开销以保持节点稳定\n请确保，对容器和初始化容器使用完全相同的请求，您无需更改任何资源限制\n在更新资源请求时，可以暂时将 WordPress Deployment 缩放为 0 个副本可能会有所帮助\n更新后，请确认：\n- WordPress 保持 3 个副本\r- 所有 Pod 都在运行并准备就绪\r搭建模拟环境 创建命名空间\n1 kubectl create namespace relative-fawn 创建 WordPress Deployment\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 vim wordpress-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: wordpress namespace: relative-fawn spec: replicas: 3 selector: matchLabels: app: wordpress template: metadata: labels: app: wordpress spec: initContainers: - name: init-mysql image: busybox:1.28 imagePullPolicy: IfNotPresent command: [\u0026#39;sh\u0026#39;,\u0026#39;-c\u0026#39;,\u0026#39;sleep 6\u0026#39;] resources: limits: cpu: 1000m memory: 500Mi containers: - name: wordpress imagePullPolicy: IfNotPresnet ports: - containerPort: 80 resources: limits: cpu: 1000m memory: 500Mi kubectl apply -f wordpress -deployment.yaml kubectl get pods -n relative-fawn 解题 连接到指定节点\n将WordPress Deployment 副本缩放为0\n1 2 3 4 5 6 7 # 必须先缩为 0 再改 requests,否则新 Pod 起不来 kubectl -n relative scale deployment wordpress --replicas=0 # 检查副本数 kubectl -n relative-fawn get deployment wordpress # 如果ready为0，说明缩放成功 检查 Node 资源使用情况\n1 2 3 kubectl get nodes kubectl describe node node-name 修改 Deployment 资源\n1 2 3 4 5 6 7 kubectl -n relative-fawn edit deployment wordpress # 将 2 个 container 的 requests 部分改为： resources: requests: cpu: 100m memory: 200Mi 将 wordpress 副本数恢复为 3\n1 kubectl -n relative-fawn scale deployment wordpress --replicas=3 检查pod副本数\n1 kubectl -n relative-fawn get pod exit退出当前节点\n十.自定义资源CRD 题目：\n验证已经部署到集群的 cert-manager 应用程序。\n使用 kubectl 将 cert-manager 命名空间所有自定义的资源（CRD）的列表，保存到~/resources.yaml\n使用 kubectl,提取定制资源 Certificate 的 subject 规范字段的文档，并将其保存到~/subject.yaml\n解题 连接至指定节点\n检查 cert-manager 资源\n1 kubectl get pods -n cert-manager 获取 cert-manager 的 CRD 资源列表\n1 2 3 4 kubectl get crd | grep cert-manager # 保存结果到文件 kubectl get crd | grep cert-manager \u0026gt; ~resource.yaml 获取 Certificate 的 subject 规范字段\n1 2 3 4 kubectl explain certificate.spec.subject # 保存结果到文件 kubectl explain certificate.spec.subject \u0026gt; ~/subject.yaml exit退出当前节点\n十一.Gateway网关 题目：\n将现有 Web 应用程序从 Ingress 迁移到 Gateway API ，您必须维护 HTTPS 访问权限。\n注意：集群中安装了一个名为 nginx 的 Gatewayclass。\n首先，创建一个名为 web-local-gatewayn的 Gateway，主机名为 gateway.web.k8s.local ,并保持现有名为 web 的 Ingress 资源的现有 TLS和侦听器配置。\n接下来，创建一个名为 web-route 的 HTTPRoute, 主机名为gateway.web.k8s.local ,并保持现有名为 web 的 Ingress 资源的现有路由规则。\n您可以使用以下命令测试 Gateway API 配置：\ncurl -Lk https://gateway.web.k8s.local:31443\n最后，删除名为 web 的现有 Ingress 资源。\n搭建模拟环境 创建 deployment 和 service\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 apiVersion: apps/v1 kind: Deployment metadata: name: web spec: replicas: 1 selector: matchLabels: app: web template: metadata: labels: app: web spec: containers: - name: web image: nginx:1.25 imagePullPolicy: IfNotPresent ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: web spec: selector: app: web ports: - port: 80 targetPort: 80 kubectl apply -f web-deploy.yaml kubectl get pods 准备TLS Secret\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 # 用openssl生成key和crt openssl req -x509 -nodes -days 365 -newkey rsa:2048 \\ -subj \u0026#34;/CN=gateway.web.k8s.local\u0026#34;\\ -keyout tls.key -out tls.crt # 创建sercret kubectl create secret tls web-cert --cert=tls.crt --key=tls.key kubectl get secert web-cert -oyaml # 创建ingress,后面要迁移到gateway vim web-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: web spec: ingressClassName: nginx tls: - hosts: - gateway.web.k8s.local secretName: web-cert rules: **- host: gateway.web.k8s.local** http: paths: - path: / pathType: Prefix backend: service: name: web port: number: 80 kubectl apply -f web-ingress.yaml kubectl get ingress 解题 连接至指定节点\n检查现有的 ingress ,提取重要信息\n1 kubectl get ingress web -o yaml 创建gateway\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 vim gateway.yaml apiVersion: gateway.networking.k8s.io/v1 kind: Gateway metadata: name: web-local-gateway spec： gatewayClassName: nginx # 依题目修改 listeners: - name: https protocol: HTTPS # 看上面ingress是什么 port: 443 hostname: gateway.web.k8s.local tls: mode: Terminate certificateRefs: - name: web-cert # 填写ingress里的tls secretname kubectl apply -f gateway.yaml kubectl get gateway 创建 httproute\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 vim httproute.yaml apiVersion: gateway.networking.k8s.io/v1 kind: HTTPRoute metadata: name: web-route spec: parentRefs: - name: web-local-gateway hostnames: - \u0026#34;gateway.web.k8s.local\u0026#34; rules: - matches: - path: type: PathPrefix value: / # ingress里path路径 backendRefs: - name: web # ingress里service name port: 80 # ingress里service port kubectl apply -f httproute.yaml kubectl get httproute 测试gateway api 配置\n1 curl -Lk https://gateway.web.k8s.local:31443 删除web ingress\n1 kubectl delete ingress web exit退出当前节点\n十二.sidecar边车容器 题目：\n为了将传统应用程序集成到K8s的日志架构（如kubectl logs）中，通常的做法是添加一个用于日志流式传输的sidecar容器。\n更新现有的sync-leverager Deployment,将使用busybox:stable镜像，且名为sidecar的并置容器，添加到现有的pod,新的并置容器必须运行以下命令： /bin/sh -c \u0026ldquo;tail -n+1 -f /var/log/sync-leverager.log\u0026rdquo;\n使用挂载在 /var/log 的Volume，使日志文件sync-leverager.log可供并置容器使用\n除了添加所需的卷挂载之外，请勿修改现有容器的规范\n搭建模拟环境 创建deployment资源 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 vim sidecar-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: sync-leverager labels: app: sync-leverager spec: replicas: 1 selector: matchLabels: app: sync-leverager template: metadata: labels: app: sync-leverager spec: containers: - name: sync-leverager image: nginx:1.25 imagePullPolicy: IfNotPresent command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;while true;do echo \\\u0026#34;$(date) INFO log line\\\u0026#34; \u0026gt;\u0026gt; /var/log/sync-leverager.log;sleep 5;done\u0026#34;] kubectl apply -f sidecar-deploy.yaml 解题 连接到指定节点\n导出yaml文件\n1 kubectl get deployment sync-leverager -o yaml \u0026gt; sidecar.yaml 编辑yaml文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 vim sidecar.yaml # 新增内容如下 spec.template.spec: volumes: - name: varlog emptyDir: {} containers: ... - command: ... volumeMounts: - name: varlog mountPath: /var/log - name: sidecar image: busybox:stable imagePullPolicy: IfNotPresent args: [\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;tail -n+1 -f /var/log/sync-leverager.log\u0026#34;] volumeMounts: - name: varlog mountPath: /var/log 1 kubectl apply -f sidecar.yaml 检查deployment,pod,sidecar容器日志\n1 2 3 4 5 kubectl get deployment sync-leverager kubectl get pod | grep sync-leverager kubectl logs sync-leverager-** -c sidecar exit退出当前节点\n十三.calico网络插件 题目：\n文档地址\nFlannel Manifest： https://github.com/flannel-io/flannel/releases/download/v0.26.1/kube-flannel.yml\nCalico Manifest： https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/tigera-operator.yaml\nContext： 集群的 CNI 未通过安全审核，已被移除。您必须安装一个可以实施网络策略的新 CNI。\nTask:\n安装并设置满足以下要求的容器网络接口（CNI）：\n选择并安装以下CNI选项之一：\nFlannel版本 0.26.1 Calico版本 3.27.0 选择的CNI必须：\n让Pod相互通信 支持Network Policy实施 从清单文件安装（请勿使用helm） 解题 连接到指定节点\n下载calico tigera-operator.yaml 文件并创建（Flannel不支持NetworkPolicy,只能选Calico）\n1 2 3 wget https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/tigera-operator.yaml kubectl create -f tigera-operater.yaml # 必须用create，apply可能导致CRD冲突 查看 PodCIDR\n1 2 3 4 5 kubectl cluster-info dump | grep -i cluster-cidr # 假设输出为： \u0026#34;clusterCIDR\u0026#34;: \u0026#34;10.244.0.0/16\u0026#34; # 记住这个地址：10.244.0.0/16 下载 Calico 自定义资源配置 custom-resources.yaml\n1 2 3 4 wget https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/custom resources.yaml # 备注：这个网址考试没给，可以在tigera-operator.yaml这个下载地址基础上记个单词custom-resources.yaml 即可。 编辑 custom-resources.yaml 文件，修改 cidr 字段\n1 2 3 4 5 6 7 8 9 10 vim custom-resources.yaml spec: calicoNetwork: ipPools: - blockSize: 26 cidr: 10.244.0.0/16 # 按上步骤实际查询的pod网段写 encapsulation: VXLANCrossSubnet natOutgoing: Enabled nodeSelector: all() 创建 Calico 自定义资源\n1 kubectl create -f custom-resources.yaml 检查Calico组件运行状态\n1 kubectl -n calico-system get pod exit退出当前节点\n十四.argocd 1 2 3 4 5 6 7 8 9 helm repo add argo https://argoproj.github.io/argo-helm helm search repo argo | grep argo-cd helm template argocd argo/argo-cd --version 5.5.22 --namespace argocd --set crds.install=false \u0026gt; ~/argo-helm.yaml helm install argocd argo/argo-cd --version 5.5.22 --namespace argocd --set crds.install=false 十五.etcd修复 题目：\nkubeadm 配置的集群已迁移到新机器，他需要更改配置才能成功运行。\n修复在机器迁移过程中损坏的单节点集群\n首先，确定损坏的集群组件，并调查导致其损坏的原因 接下来，修复所有损坏的集群组件的配置。 最后，确保集群运行正常，确保每个节点和 Pod 都处于Ready状态\n注意：\n已停用的集群使用外部 etcd 服务器 确保重新启动所有必要的服务和组件，以使更改生效。否则可能导致分数降低。 解题 连接到指定节点\n修复kube-apiserver配置（etcd地址）\n1 2 3 4 5 6 # 编辑kube-apiserver的静态Pod清单文件 vim /etc/kubernetes/manifests/kube-apiserver.yaml # 找到--etcd-server参数 # 改为： --etcd-server=https://127.0.0.1:2379 #指向本地etcd服务 重启kubelet服务\n1 2 systemctl daemon-reload systemctl restart kubelet 检查节点和Pod状态（此时可能还是有问题）\n1 2 3 kubectl get nodes kubectl get pod -n kube-system 修复 kube-scheduler-master01 配置\n1 2 3 4 5 6 vim /etc/kubernetes/manifests/kube-scheduler.yaml # 找到resources 配置 resources: requests: cpu: 100m 等待kubelet自动重启kube-scheduler,一两分钟后再次检查集群状态\n1 2 3 kubectl get nodes kubectl get pod -n kube-system exit退出当前节点\n十六.cri-dockerd容器运行时 题目：\nContext：您的任务是为 Kubernetes 准备一个 Linux 系统。 Docker 已被安装，但您需要为 kubeadm 配置它。\n完成以下任务，为 Kubernetes 准备系统：\n设置 cri-dockerd ：\n1.安装 Debian 软件包 ~/cri-dockerd_0.3.6.3-0.ubuntu-jammy_amd64.deb Debian 软件包 使用 dpkg 安装。\n2.启用并启动 cri-docker 服务\n配置以下系统参数:\nnet.bridge.bridge-nf-call-iptables 设 置 为 1\nnet.ipv6.conf.all.forwarding 设 置 为 1\nnet.ipv4.ip_forward 设 置 为 1\nnet.netfilter.nf_conntrack_max 设置为 131072\n确保这些系统参数在系统重启后仍然存在，并应用于正在运行的系统。\n解题 连接到指定节点\n安装cri-dockerd并启动\n1 2 3 4 sudo dpkg -i ~/cri-dockerd_0.3.6.3-0ubuntu-jammy_amd64.deb sudo systemctl enable cri-docker sudo systemctl start cri-docker 配置系统参数\n1 2 3 4 5 6 7 8 9 sudo modprobe br_netfilter sudo vim /etc/sysctl.conf #添加以下内容到文件最后 net.bridge.bridge-nf-call-iptables = 1 net.ipv6.conf.all.forwarding = 1 net.ipv4.ip_forward = 1 net.netfilter.nf_conntrack_max = 131072 让配置生效\n1 sudo sysctl -p exit退出当前节点\n","date":"2025-07-29T10:28:19+08:00","permalink":"https://Logic0528.github.io/p/cka%E8%AE%A4%E8%AF%81/","title":"CKA认证"},{"content":"1.创建存储配置PV与PVC mysql-pvc.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi # 如果是AWS/GKE等云环境，可以指定对应的存储类 # storageClassName: gp2 mysql-pv.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: PersistentVolume metadata: name: mysql-pv # PV名称，需唯一 spec: capacity: storage: 10Gi # 存储容量，需与PVC请求一致 accessModes: - ReadWriteOnce # 访问模式，需与PVC匹配 persistentVolumeReclaimPolicy: Retain # 回收策略（Retain/Delete/Recycle） hostPath: path: /data/mysql # 节点上的实际存储路径（需提前创建） 2.创建配置文件configmap 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # mysql-config.yaml apiVersion: v1 kind: ConfigMap metadata: name: mysql-config data: my.cnf: | [mysqld] character-set-server=utf8mb4 collation-server=utf8mb4_unicode_ci default-storage-engine=InnoDB max_connections=500 [client] default-character-set=utf8mb4 3.创建secret存储密码 1 2 3 kubectl create secret generic mysql-secret \\ --from-literal=root-password=\u0026#34;your-root-password\u0026#34; \\ --from-literal=password=\u0026#34;your-user-password\u0026#34; 4.创建deployment部署MySQL实例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 # mysql-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: mysql spec: selector: matchLabels: app: mysql strategy: type: Recreate template: metadata: labels: app: mysql spec: containers: - name: mysql image: mysql:8.0 ports: - containerPort: 3306 name: mysql env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-secret key: root-password - name: MYSQL_DATABASE value: mydatabase - name: MYSQL_USER value: myuser - name: MYSQL_PASSWORD valueFrom: secretKeyRef: name: mysql-secret key: password volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql - name: config-volume mountPath: /etc/mysql/my.cnf subPath: my.cnf volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-pvc - name: config-volume configMap: name: mysql-config 5.创建service 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: Service metadata: name: mysql spec: ports: - port: 3306 selector: app: mysql clusterIP: None # 使用无头服务，如果需要直接访问Pod # 如果需要从集群外部访问，可以使用NodePort或LoadBalancer类型 # type: NodePort # ports: # - port: 3306 # nodePort: 30006 6.部署命令 1 2 3 4 kubectl apply -f mysql-pvc.yaml kubectl apply -f mysql-config.yaml kubectl apply -f mysql-deployment.yaml kubectl apply -f mysql-service.yaml 7.在本机上部署和在docker上部署和在k8s集群上部署有什么不同呢 在本机上部署无疑环境不隔离，缺乏扩展性，在docker上部署和在K8s集群上部署区别体现出k8s为何被称为容器编排管理器，k8s通过yaml配置文件定义deployment，service，pvc等资源，由k8s集群自动调度到节点，完全声明式管理。支持通过deployment的replicas字段一键扩容多实例，service负载均衡实现流量分发，扩展能力极强，pv实现数据持久化\n补充 MySQL作为有状态应用理应使用statefulset而不是deployment，扩展为多节点集群，主从复制MySQL cluster时必须使用statefulset，以确保数据一致性和服务稳定性，此处单节点测试\n","date":"2025-07-02T15:47:16+08:00","permalink":"https://Logic0528.github.io/p/%E5%9C%A8k8s%E9%9B%86%E7%BE%A4%E4%B8%8A%E9%83%A8%E7%BD%B2mysql/","title":"在k8s集群上部署mysql"},{"content":"Ingress 随着时间的积累原来不懂的东西现在逐渐看清了\nIngress：网关入口\n集群中Service的统一网管入口\n当Ingress安装完成并启动后，会为其创建一个Service，并设置一个NodePort，因为Ingress要处理所有的外部请求，故必须将Ingress暴露出去，此时访问集群人一台机器的指定端口就可以映射到Ingress上，然后通过在Ingress配置相应的规则，就可以实现对Service的统一网关入口\n","date":"2025-07-02T10:37:07+08:00","permalink":"https://Logic0528.github.io/p/ingress/","title":"Ingress"},{"content":"自己动手，丰衣足食，经过多次的搭建k8s集群现在可以说是基本明白了，记录一下过程不用每次都再搜索别人的教程了\n准备工作 1 2 3 4 5 6 7 8 9 10 11 12 #关闭防火墙 systemctl stop firewalld systemctl disable firewalld #关闭swap分区 swapoff -a 永久关闭需修改/etc/fstab sudo vim /etc/fstab #关闭selinux setenforce 0 sed -i \u0026#39;s/SELINUX=enforcing/SELINUX=disabled/g\u0026#39; /etc/selinux/config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # 添加配置文件并加载ipvs模块（kube-proxy使用ipvs模式） cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/ipvs.conf ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh nf_conntrack EOF # 执行命令加载模块 systemctl restart systemd-modules-load.service modprobe -- ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh nf_conntrack # 添加内核参数配置文件，参数在重新启动后保持不变 cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 EOF # 执行命令使参数生效 sysctl --system 安装前检查 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 执行命令检查selinux状态，输出应为SELinux status：disabled sestatus # 检查swap是否已经关闭，输出应为空 cat /proc/swaps # 检查ipvs模块是否已经加载 lsmod |grep ip_vs # 检查防火墙是否已经关闭 systemctl status firewalld # 检查内核参数是否已经设置，值应该为 1 cat /proc/sys/net/ipv4/ip_forward 安装docker,容器运行时 Ubuntu系统：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # step 1: 安装必要的一些系统工具 sudo apt-get update sudo apt-get install ca-certificates curl gnupg # step 2: 信任 Docker 的 GPG 公钥 sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg # Step 3: 写入软件源信息 echo \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://mirrors.aliyun.com/docker-ce/linux/ubuntu \\ \u0026#34;$(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;)\u0026#34; stable\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null # Step 4: 安装Docker sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin # 安装指定版本的Docker-CE: # Step 1: 查找Docker-CE的版本: # apt-cache madison docker-ce # docker-ce | 17.03.1~ce-0~ubuntu-xenial | https://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages # docker-ce | 17.03.0~ce-0~ubuntu-xenial | https://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages # Step 2: 安装指定版本的Docker-CE: (VERSION例如上面的17.03.1~ce-0~ubuntu-xenial) # sudo apt-get -y install docker-ce=[VERSION] centos系统：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # step 1: 安装必要的一些系统工具 sudo yum install -y yum-utils # Step 2: 添加软件源信息 yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # Step 3: 安装Docker sudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin # Step 4: 开启Docker服务 sudo service docker start # 注意： # 官方软件源默认启用了最新的软件，您可以通过编辑软件源的方式获取各个版本的软件包。例如官方并没有将测试版本的软件源置为可用，您可以通过以下方式开启。同理可以开启各种测试版本等。 # vim /etc/yum.repos.d/docker-ce.repo # 将[docker-ce-test]下方的enabled=0修改为enabled=1 # # 安装指定版本的Docker-CE: # Step 1: 查找Docker-CE的版本: # yum list docker-ce.x86_64 --showduplicates | sort -r # Loading mirror speeds from cached hostfile # Loaded plugins: branch, fastestmirror, langpacks # docker-ce.x86_64 17.03.1.ce-1.el7.centos docker-ce-stable # docker-ce.x86_64 17.03.1.ce-1.el7.centos @docker-ce-stable # docker-ce.x86_64 17.03.0.ce-1.el7.centos docker-ce-stable # Available Packages # Step2: 安装指定版本的Docker-CE: (VERSION例如上面的17.03.0.ce.1-1.el7.centos) # sudo yum -y install docker-ce-[VERSION] 接下来这一点是之前几次失败的原因\n1 2 # 生成默认配置 containerd config default | sudo tee /etc/containerd/config.toml 配置文件\n1 2 3 4 5 6 7 8 # 修改/etc/containerd/config.toml配置 disabled_plugins = [] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] SystemdCgroup = true # 这里改为true [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;] # 这里修改镜像地址 sandbox_image = \u0026#34;crpi-u3f530xi0zgtvmy0.cn-beijing.personal.cr.aliyuncs.com/image-infra/pause:3.10\u0026#34; 注意plugins.\u0026ldquo;io.containerd.grpc.v1.cri\u0026quot;里面有个systemd_cgroup=true/false,之前出错就是因为有这个，和下面的SystemdCgroup冲突了，有的话注释掉\n1 2 3 4 5 6 7 8 9 # 设置开机启动 systemctl enable containerd # 启动containerd systemctl start containerd # 检查状态 systemctl status containerd 安装kubelet,kubeadm,kubectl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 配置kubernetes 源 cat \u0026lt;\u0026lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/ enabled=1 gpgcheck=1 gpgkey=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/repodata/repomd.xml.key exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni EOF # 安装kubelet，kubeadm，kubectl yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes # 启动kubelet，启动完成查看kubelet的状态可能是异常的，不用惊慌 systemctl enable --now kubelet 集群初始化 1 2 3 4 5 sudo kubeadm init \\ --pod-network-cidr=10.244.0.0/16 \\ --apiserver-advertise-address=192.168.189.100 \\ --control-plane-endpoint=192.168.189.100 \\ --upload-certs 安装flannel网络插件 1 wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 补充一点，关于权限问题，在master节点上执行下面的命令，配置kubectl的权限\n1 2 3 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config ","date":"2025-06-30T14:59:31+08:00","permalink":"https://Logic0528.github.io/p/k8s%E9%9B%86%E7%BE%A4%E5%88%9D%E5%A7%8B%E5%8C%96/","title":"K8s集群初始化"},{"content":"笑死了他妈的四个实习生来的最早被锁在门外面，明天不来这么早了\n","date":"2025-06-30T14:59:09+08:00","permalink":"https://Logic0528.github.io/p/%E5%8C%97%E6%BC%82%E6%97%A5%E8%AE%B004/","title":"北漂日记04"},{"content":"实习第一天，报道领电脑，感觉还不错，没人给我上压力，领导还挺和气的中午带我去吃饭，下班时找我聊聊天问候两句，今天像上了一天自习，虽然没学太多东西只是把电脑装点软件但是感觉动力十足，晚上回来后把linux基础作业完成了，配置了桥接模式，需要和主机一个网段，下载了nginx，看了访问日志，主机需要防火墙放行80端口才能访问，现在已经九点半了，接下来洗个澡就十点了，但是今天还没有学新东西，看看阿里的acp什么的，如果去打英雄联盟呢好像一天没休息了也还行，但是会不会又是明日复明日明日何其多呢，感觉早睡也睡不着，今天一百八到手的还挺轻松，算了不打了，到点来局大乱斗睡觉吧，把明天的docker作业看一下，看会acp，一把大乱斗然后睡觉，嗯，完美\n","date":"2025-06-23T21:35:42+08:00","permalink":"https://Logic0528.github.io/p/%E5%8C%97%E6%BC%82%E6%97%A5%E8%AE%B003/","title":"北漂日记03"},{"content":"来到北京的第二天，上午从民宿起床后吃了饭往公寓搬东西入住，两天从看房到入住感觉有些太快了，处处谨慎的问东问西透露出我是个刚出社会的新兵蛋子，房主显然有些不耐烦了，下午坐地铁来的时候突然想听听“北京欢迎你”，也不知道北京欢不欢迎我\n坐地铁时有个空位，我和另一个大叔看上了这个座位但双方都很谦让，他说你做吧，本来我是想坐的毕竟我行李一大堆，但是我看到大叔地中海的发型快要顶不住秃掉觉得有些可怜就说你做吧我还有几站就下车了，哈哈\n来北京两天我已经有好几个兄弟了，他们一半是房产中介，一半是销售\n","date":"2025-06-20T18:27:47+08:00","permalink":"https://Logic0528.github.io/p/%E5%8C%97%E6%BC%82%E6%97%A5%E8%AE%B002/","title":"北漂日记02"},{"content":"“最后一次来你家店吃饭了，离开北京了”\n这个男人快步走进这家面馆，人还没到柜台时我听到了这句话，今天是我第一次来北京。\n他应该是这家店的常客了，老板停顿了一下\n“去哪？”\n“回老家，西安” 深呼吸，吐气\n几秒的沉默，吃面的我也放慢了速度，我意识到这或许是第一段难忘的记忆\n“……” “回去干啥？”\n“一个月三四千，服务员什么的……”\n“还行，在那边三四千比这边六千都舒服，吃点啥”\n“来份小炒肉盖饭吧”\n“再来瓶啤酒”\n每天都有人疲惫地离开，每天都有人迷茫地涌进来，一座城市像一个巨大的有机生物体，细胞们通过一条条血管般的轨道运输到各处，这个生物体里有最令人作呕的腐烂，也有最生机勃勃的希望\n今天坐出租车时在下雨，雨刷器规律的摆动着，有雨水时就刷刷两下，没有雨时就安静的躺在那里，我想如果此时雨刷器在玻璃上跳起拉丁舞，那些一切杂乱的动作组合排列在一起，或许就是生命的意义\n我在心中暗自对自己说，等我离开北京那一天，我的最后一餐要是这家店的小炒肉盖饭，或许可以再来瓶啤酒\n","date":"2025-06-19T22:16:51+08:00","permalink":"https://Logic0528.github.io/p/%E5%8C%97%E6%BC%82%E6%97%A5%E8%AE%B001/","title":"北漂日记01"},{"content":"五月份进行了两次面试，面试不同于闭门造车,问的许多问题需要进行规范的回答类似于八股文之类的，虽然我很不喜欢八股文代表的那种死板的精神，但不得不承认在对于基础知识的掌握有许多不足，在面试过程中很多概念熟悉了解但是语言组织不够流畅，对面试中没能回答上来或者回答不流畅或者以后面试可能会遇到的额问题进行整理。 屡战屡败，屡败屡战，面试中不会的问题记下来解决也是一种收获\n1.k8s架构和基本组件 k8s是一个开源的容器编排平台，用于自动化部署，扩展和管理容器化应用。其架构采用主从架构（master-node），核心组件分为控制平面control plane和工作节点worker node。\n控制平面组件：\nkube-apiserver:API网关和集群控制中心，处理所有管理请求如创建/删除pod，部署应用等 etcd: 分布式键值存储，保存集群配置,如pod,service,namespace\nkube-scheduler: 负责pod调度到合适的节点\nkube-controller-manager: 运行控制器进程，包括node controller,deployment controller,service controller\n节点组件：\nkubelet: 管理节点上的pod\nkube-proxy: 实现网络规则和负载均衡，维护节点上的网络配置\ncontainer runtime: 负责运行容器（如docker，containerd）\n附加组件： coreDNS：集群DNS服务，提供域名解析功能，将服务名解析为IP地址。 IngressContorller：负责外部流量访问，支持负载均衡和路由规则。HTTP路由 NetworkPluginL: calico，flannel等\n2.pod,deployment,service pod： 是k8s中最小的调度单位，封装一个或多个容器，共享网络和存储\ndeployment： 基于ReplicaSet的更高层抽象，声明式管理Pod的期望状态，如副本数，镜像版本，更新策略 作用： 确保pod副本数始终等于期望值，自动重启/创建失败的pod，支持滚动更新，回滚版本\nservice：抽象的网络层，为一组pod提供固定访问入口 ip+端口 作用： 内部服务发现：同意集群内其他pod可通过service域名访问 外部暴露： 通过nodeport等类型将服务暴露到集群外 负载均衡： 将流量分发到后端多个pod副本\ndeployment是管理pod的控制器，service是抽象网络层，三者通过标签匹配。\n3.nginx与负载均衡 nginx是一款高性能的web服务器，反向代理服务器，负载均衡器 web服务器： 处理静态资源的请求，直接返回文件内容，性能远超传统服务器\n反向代理服务器： 将客户端请求转发到后端多个应用服务器如tomcat，隐藏真实服务器地址\n负载均衡器： 分发客户端请求到多个后端服务器，避免单点过载\n负载均衡： 将网络流量均发到多个计算资源如服务器，容器，避免单一节点过载，提高系统的可用性，性能和容错能力\nnginx负载均衡方式： 轮询；默认策略，按顺序分发请求 weight: 根据权重分配 ip_hash： 基于客户端IP哈希 等\u0026hellip;.深入了解后补充\nnginx反向代理的作用与好处：\n作用：将用户请求转发到后端服务器，隐藏后端服务器 好处： 1.节省公网IP，域名是解析到反向代理服务器，后端服务器不需要配置ip 2.提高安全性，反向代理对用户不可见，降低直接攻击后端服务器的风险 3.统一访问入口，反向代理可以代理多个服务器，也就是多个项目，通过域名，将不同的项目转化到不同的后台服务器 4.提高访问速度，反向代理服务器是在后端的前面，所以他可以将一些静态内容缓存到本地，下一次再有请求的时候直接从本地读取，不会像后端发起请求，加快了响应速度\n4.k8s中如何将一个服务暴露到外部，有什么区别 1.nodeport: 在节点上开放一个固定端口，外部流量通过任一节点的该端口转发到该服务，适用于测试环境，内部系统临时暴露 缺点：端口管理复杂，需要避开暴露的端口，仅支持4层负载均衡，安全性差 2.ingress： 基于ingress controller实现7层负载均衡 作用： 提供基于域名的路由规则，实现七层负载均衡，支持路径规则，TLS 加密，基于域名的虚拟主机等功能\n区别： 1.层级不同：nodeport在集群每个节点上暴露端口，ingress在集群外部 2.负载均衡方式：nodeport基于IPVS实现四层负载均衡，ingress基于Nginx实现七层负载均衡 3.功能复杂性：nodeport仅支持基本的负载均衡，ingress支持更丰富的功能，如路由规则，TLS 加密，基于域名的虚拟主机等\n5.dockerfile常用变量，基于nginx镜像做个性化配置 1 2 3 4 5 6 7 8 9 10 11 12 FROM 镜像 USER 设置运行后续命令的用户 WORKDIR 设置工作目录 ENV 设置环境变量 RUN 执行命令，安装软件或配置环境 COPY 将本地文件复制到镜像中 EXPOSE 暴露端口 ENV 定义环境变量，可在后续命令中使用 VOLUME 定义匿名卷，用于数据持久化 CMD 设置容器启动时执行的命令 ENDPOINT 配置容器启动时执行的命令 基于nginx镜像做个性化设置： 在同目录下创建nginx.conf 修改nginx配置，docker build -t mynginx:v1\n6.linux常用命令 1 2 3 4 5 6 7 8 9 10 11 12 13 14 whoami 输出当前用户名 id echo $USER uptime 查看系统负载和运行时间 free 查看内存使用情况 df -h 查看磁盘空间使用情况 ps aux 查看进程信息 netstat -tlnp 查看网络连接情况 lsblk 查看磁盘总容量及使用情况 dd 测试读写速率 iostat 磁盘整体读写速率 lscpu 查看有多少个cpu top 实时监控内存及进程占用 ","date":"2025-06-07T14:51:28+08:00","permalink":"https://Logic0528.github.io/p/%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%E5%A4%8D%E7%9B%98/","title":"实习面试复盘"},{"content":"项目目录结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 k8s-deploy-platform/ ├── helm-charts/ # Helm Chart模板 │ └── app-template/ │ ├── Chart.yaml # Chart基本信息 │ ├── values.yaml # 默认配置值 │ └── templates/ # K8s资源模板 │ ├── deployment.yaml # 部署配置 │ ├── service.yaml # 服务配置 │ └── ingress.yaml # 入口配置 │ ├── manifests/ # K8s资源清单 │ ├── applications/ # 应用相关配置 │ │ └── app-template.yaml │ ├── logging/ # 日志系统配置 │ │ └── efk-values.yaml │ └── monitoring/ # 监控系统配置 │ ├── prometheus-rules.yaml # 监控规则 │ ├── prometheus-values.yaml # Prometheus配置 │ ├── pv.yaml # 持久化卷 │ └── storageclass.yaml # 存储类 │ └── scripts/ # 自动化脚本 ├── install.sh # 安装脚本 └── deploy.sh # 部署脚本 helm-charts/app-template Chart.yaml 1 2 3 4 5 apiVersion: v2 name: app-template description: 一个基础的应用部署模板 version: 0.1.0 type: application values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # 应用配置 name: myapp replicas: 2 image: nginx:latest # 资源配置 resources: requests: memory: \u0026#34;64Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; # 健康检查配置 healthCheck: enabled: true path: /health port: 8080 initialDelay: 30 period: 10 # 服务配置 service: type: ClusterIP port: 80 targetPort: 8080 # Ingress配置 ingress: enabled: false className: nginx host: myapp.example.com path: / pathType: Prefix value.yaml中service类型为ClusterIP,clusterip仅在集群内部可以访问，与之对应的是nodeport，nodeport在所有节点上开放指定端口（30000-32767）\nclusterIP让外部访问的方式：\n可以通过配置ingress\n1 2 3 4 5 graph LR A[外部用户] --\u0026gt; B[Ingress] B --\u0026gt; C[ClusterIP Service] C --\u0026gt; D[Pod1] C --\u0026gt; E[Pod2] 也可以使用这条命令临时端口转发\n1 kubectl port-forward -n monitoring svc/monitoring-grafana 3000:80 1 2 3 4 graph LR A[外部用户] --\u0026gt; B[临时端口转发] B --\u0026gt; C[ClusterIP Service] C --\u0026gt; D[Pod1] templates/deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion: apps/v1 kind: Deployment metadata: name: {{ .Values.name }} spec: replicas: {{ .Values.replicas }} selector: matchLabels: app: {{ .Values.name }} template: metadata: labels: app: {{ .Values.name }} spec: containers: - name: {{ .Values.name }} image: {{ .Values.image }} resources: {{ toYaml .Values.resources | nindent 12 }} {{- if .Values.healthCheck.enabled }} livenessProbe: httpGet: path: {{ .Values.healthCheck.path }} port: {{ .Values.healthCheck.port }} initialDelaySeconds: {{ .Values.healthCheck.initialDelay }} periodSeconds: {{ .Values.healthCheck.period }} {{- end }} deployment.yaml这样配置可以根据values.yaml中的配置动态生成deployment资源，相当于values.yaml定义变量，方便修改\ntemplates/service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: Service metadata: name: {{ .Values.name }} spec: type: {{ .Values.service.type }} ports: - port: {{ .Values.service.port }} targetPort: {{ .Values.service.targetPort }} protocol: TCP selector: app: {{ .Values.name }} templates/ingress.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 {{- if .Values.ingress.enabled }} apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: {{ .Values.name }} spec: ingressClassName: {{ .Values.ingress.className }} rules: - host: {{ .Values.ingress.host }} http: paths: - path: {{ .Values.ingress.path }} pathType: {{ .Values.ingress.pathType }} backend: service: name: {{ .Values.name }} port: number: {{ .Values.service.port }} {{- end }} manifests applications/app-template.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion: apps/v1 kind: Deployment metadata: name: {{ .Values.name }} spec: replicas: {{ .Values.replicas }} selector: matchLabels: app: {{ .Values.name }} template: metadata: labels: app: {{ .Values.name }} spec: containers: - name: {{ .Values.name }} image: {{ .Values.image }} resources: {{ toYaml .Values.resources | nindent 12 }} {{- if .Values.healthCheck.enabled }} livenessProbe: httpGet: path: {{ .Values.healthCheck.path }} port: {{ .Values.healthCheck.port }} initialDelaySeconds: {{ .Values.healthCheck.initialDelay }} periodSeconds: {{ .Values.healthCheck.period }} {{- end }} logging/efk-values.yaml 暂时还不会\nmonitoring/prometheus-rules.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: app-alerts namespace: monitoring spec: groups: - name: app rules: - alert: HighCPUUsage expr: container_cpu_usage_seconds_total \u0026gt; 80 for: 5m labels: severity: warning annotations: description: \u0026#34;容器 CPU 使用率超过 80%\u0026#34; - alert: HighMemoryUsage expr: container_memory_usage_bytes \u0026gt; 80 for: 5m labels: severity: warning annotations: description: \u0026#34;容器内存使用率超过 80%\u0026#34; monitoring/prometheus-values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 prometheus: enabled: true serverFiles: prometheus.yml: global: scrape_interval: 15s scrape_configs: - job_name: \u0026#39;kubernetes-pods\u0026#39; kubernetes_sd_configs: - role: pod - job_name: \u0026#39;kubernetes-nodes\u0026#39; kubernetes_sd_configs: - role: node persistence: enabled: true size: 10Gi grafana: enabled: true adminPassword: admin persistence: enabled: true size: 5Gi type: pvc storageClassName: local-storage grafana实现持久化存储时，需要配置storageClassName，这个storageClassName需要在k8s集群中已经存在，这里使用的是local-storage\nmonitoring/storageclass.yaml 1 2 3 4 5 6 7 class.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: local-storage provisioner: kubernetes.io/no-provisioner volumeBindingMode: Immediate monitoring/pv.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: v1 kind: PersistentVolume metadata: name: grafana-pv spec: capacity: storage: 5Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: local-storage local: path: /mnt/data nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: #- master # 替换为你的节点名称 - work-node1 注意：\n这里的path需要创建好，否则会报错 nodeAffinity是为了保证grafana的数据不会被其他节点的pod所使用，就一个工作节点所以没写，master节点上一般会有污点，最好调度到工作节点上 scripts install.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # filepath: scripts/setup-cluster.sh #!/bin/bash # 启动本地集群 start_cluster() { echo \u0026#34;Starting minikube cluster...\u0026#34; minikube start --driver=docker \\ --memory=4096 \\ --cpus=2 \\ --kubernetes-version=v1.24.0 } # 安装必要的组件 install_components() { # 添加 Helm 仓库 helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo add elastic https://helm.elastic.co helm repo update } # 创建必要的命名空间 create_namespaces() { kubectl create namespace monitoring kubectl create namespace logging kubectl create namespace applications } main() { start_cluster install_components create_namespaces } main \u0026#34;$@\u0026#34; deploy.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 #!/bin/bash # 设置变量 APP_NAME=$1 IMAGE=$2 NAMESPACE=${3:-applications} REPLICAS=${4:-1} # 验证输入 if [ -z \u0026#34;$APP_NAME\u0026#34; ] || [ -z \u0026#34;$IMAGE\u0026#34; ]; then echo \u0026#34;Usage: $0 \u0026lt;app-name\u0026gt; \u0026lt;image\u0026gt; [namespace] [replicas]\u0026#34; exit 1 fi # 部署应用 deploy_application() { echo \u0026#34;Deploying $APP_NAME...\u0026#34; helm upgrade --install $APP_NAME ./helm-charts/app-template \\ --namespace $NAMESPACE \\ --set name=$APP_NAME \\ --set image=$IMAGE \\ --set replicas=$REPLICAS } main() { deploy_application } main \u0026#34;$@\u0026#34; 项目暂时完成，grafana全是英文暂时不熟练，后续再完善\n","date":"2025-05-20T10:38:28+08:00","permalink":"https://Logic0528.github.io/p/%E5%9F%BA%E4%BA%8Ek8s%E7%9A%84%E5%BA%94%E7%94%A8%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2%E5%B9%B3%E5%8F%B0/","title":"基于K8s的应用自动化部署平台"},{"content":"污点（Taint） effect: NoSchedule：不允许调度，已经存在的Pod不受影响\nPreferNoSchedule：尽量不调度，已经存在的Pod不受影响\nNoExecute：不允许调度，并且驱逐已经存在的Pod\n容忍（Toleration） operator: Equal：需要key和value完全匹配\nExists：只需要key匹配，不需要value匹配\nvalue: 污点的值\n示例代码：\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: Pod metadata: name: pod-with-tolerations spec: tolerations: - key: \u0026#34;key1\u0026#34; # 污点的键 operator: \u0026#34;Equal\u0026#34; # 操作符：Equal或Exists value: \u0026#34;value1\u0026#34; # 污点的值 effect: \u0026#34;NoSchedule\u0026#34; # 效果 tolerationSeconds: 3600 # 容忍时间（仅用于NoExecute） 亲和力（Affinity） NodeAffinity 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: v1 kind: Pod metadata: name: with-node-affinity spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: # 硬性要求 nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux preferredDuringSchedulingIgnoredDuringExecution: # 软性偏好 - weight: 1 preference: matchExpressions: - key: disk-type operator: In values: - ssd PodAffinity 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: Pod metadata: name: with-pod-affinity spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - web-store topologyKey: kubernetes.io/hostname PodAntiAffinity(节点反亲和力) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: v1 kind: Pod metadata: name: with-pod-antiaffinity spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - cache topologyKey: kubernetes.io/hostname 没什么好说的，类似于标签，慢工出细活~\n","date":"2025-05-15T15:52:46+08:00","permalink":"https://Logic0528.github.io/p/k8s-%E6%B1%A1%E7%82%B9%E4%B8%8E%E5%AE%B9%E5%BF%8D/","title":"K8s-污点与容忍"},{"content":"PV和PVC PV和PVC是Kubernetes中用于管理持久化存储的两个重要概念。\nPV（Persistent Volume） 是集群中的存储资源 由管理员事先创建和配置 独立与pod的生命周期 可以对接各种存储后端，如nfs、ceph、云存储等 定义了存储的容量、访问模式、存储类等信息 PVC（Persistent Volume Claim） 是pod对存储资源的请求 由用户/开发者创建 类似于pod消耗Node资源，PVC消耗PV资源 声明需要的存储容量、访问模式、存储类等信息 不需要关心存储的具体实现细节 工作流程 管理员创建PV，定义存储资源的属性 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: v1 kind: PersistentVolume #描述资源类型为PV类型 metadata: name: pv0001 spec: capacity: #容量配置 storage: 5Gi volumeMode: Filesystem #存储类型为文件系统 accessModes: #访问模式：ReadWriteOnce WeadWriteMany ReadOnlyMany - ReadWriteMany #可被单节点读写 persistentVolumeReclaimPolicy: Retain #回收策略 storageClassName: slow #创建PV的存储类名，需要与pvc的相同 mountOptions: #加载配置 - hard - nfsvers=4.1 nfs: #连接到nfs path: /home/nfs/rw/pv-test #存储路径 server: 192.168.189.130 #nfs服务地址 用户/开发者创建PVC，声明需要的存储资源 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: PersistentVolumeClaim #资源类型为pvc metadata: name: nfs-pvc spec: accessModes: - ReadWriteMany volumeMode: Filesystem resources: requests: storage: 5Gi storageClassName: slow #名字要与对应的pv相同 # selector: #使用选择器选择对应的pv Kubernetes调度器根据PVC的要求，选择合适的PV进行绑定 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: v1 kind: Pod metadata: name: pvc-test-pod spec: containers: - image: nginx name: nginx-volume volumeMounts: - mountPath: /usr/share/nginx/html #挂载到容器的哪个目录 name: test-volume #挂载哪个volume volumes: - name: test-volume persistentVolumeClaim: claimName: nfs-pvc tolerations: - key: \u0026#34;node-role.kubernetes.io/control-plane\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; 动态制备PV 被污点给整破防了，暂时先不写了。（后半句由ai补充），天哪你连我心里想的什么都知道快点拯救全人类吧，待我学完污点归来\n目前遗留问题，-n kube-system po nfs-server-provisioner ImagePullBackOff,kube-apiserver pending状态，selfLink怎么处理，其他的部分理解深入许多\n好的，了解到主节点上默认会有污点，目的是为了保护主节点资源，防止普通pod调\n动态制备pv 创建storageclass 定义存储类型 指定Provisioner 设置存储参数 1 2 3 4 5 6 7 8 9 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: managed-nfs-storage provisioner: fuseim.pri/ifs #外部制备器提供者，编写为提供者的名称 parameters: archiveOnDelete: \u0026#34;false\u0026#34; #是否存档 reclaimPolicy: Retain #回收策略 volumeBindingMode: Immediate #默认为Immediate，表示创建PVC立即进行绑定 用户创建pvc 指定存储大小 指定访问模式 引用storageclass 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 cat nfs-sc-statefulset.yaml --- apiVersion: v1 kind: Service metadata: name: nginx-sc labels: app: nginx-sc spec: type: NodePort ports: - name: web port: 80 protocol: TCP selector: app: nginx-sc --- apiVersion: apps/v1 kind: StatefulSet metadata: name: nginx-sc spec: replicas: 1 #serverName: \u0026#34;nginx-sc\u0026#34; selector: matchLabels: app: nginx-sc template: metadata: labels: app: nginx-sc spec: containers: - image: nginx name: nginx-sc imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /usr/share/nginx/html name: nginx-sc-test-pvc volumeClaimTemplates: - metadata: name: nginx-sc-test-pvc spec: storageClassName: managed-nfs-storage accessModes: - ReadWriteMany resources: requests: storage: 1Gi provisioner创建pv 检测到新建pvc 根据storageclass找到对应的provisioner provisoner创建pv 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 cat nfs-provisioner.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nfs-client-provisioner namespace: kube-system labels: app: nfs-client-provisioner spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:v4.0.0 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: 192.168.189.130 - name: NFS_PATH value: /home/data/nfs/rw volumes: - name: nfs-client-root nfs: server: 192.168.189.130 path: /home/data/nfs/rw 基本过程如上，上次出现污点问题是因为nfs服务器地址写为master节点导致pod部署到master节点，有污点\n另补充一点，provisioner需要特定权限才能创建和管理pv，需要用到rbac.yaml\n","date":"2025-05-11T14:49:19+08:00","permalink":"https://Logic0528.github.io/p/k8s-%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8-pv%E5%92%8Cpvc/","title":"K8s-持久化存储-PV和PVC"},{"content":"超级帅哥悄悄路过\n","date":"2025-05-08T23:05:44+08:00","permalink":"https://Logic0528.github.io/p/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/","title":"第一篇博客"}]